{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6489c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import subprocess\n",
    "import multiprocessing as mp\n",
    "from collections import Counter, defaultdict\n",
    "import psutil\n",
    "import sys\n",
    "import tempfile\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "# First check if BLAST+ is installed\n",
    "def check_blast_installation():\n",
    "    try:\n",
    "        # Check if blastn is available\n",
    "        result = subprocess.run([\"which\", \"blastn\"], capture_output=True, text=True)\n",
    "        if result.returncode != 0:\n",
    "            print(\"ERROR: BLAST+ tools (blastn, makeblastdb) not found in PATH\")\n",
    "            print(\"\\nPlease install NCBI BLAST+ tools:\")\n",
    "            print(\"For Conda: conda install -c bioconda blast\")\n",
    "            print(\"For Ubuntu/Debian: sudo apt-get install ncbi-blast+\")\n",
    "            return False\n",
    "        \n",
    "        blastn_path = result.stdout.strip()\n",
    "        print(f\"Found blastn at: {blastn_path}\")\n",
    "        \n",
    "        # Check makeblastdb\n",
    "        result = subprocess.run([\"which\", \"makeblastdb\"], capture_output=True, text=True)\n",
    "        if result.returncode != 0:\n",
    "            print(\"ERROR: makeblastdb tool not found in PATH\")\n",
    "            return False\n",
    "            \n",
    "        makeblastdb_path = result.stdout.strip()\n",
    "        print(f\"Found makeblastdb at: {makeblastdb_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking BLAST installation: {e}\")\n",
    "        return False\n",
    "\n",
    "# Performance tracking\n",
    "start_time = time.time()\n",
    "memory_usage = []\n",
    "\n",
    "def track_memory():\n",
    "    \"\"\"Track current memory usage\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_usage.append(process.memory_info().rss / 1024 / 1024)  # MB\n",
    "    return memory_usage[-1]\n",
    "\n",
    "print(f\"Starting with memory: {track_memory()} MB\")\n",
    "\n",
    "# ==================== Taxonomy Functions ====================\n",
    "\n",
    "def generate_taxonomic_tree():\n",
    "    \"\"\"Generate a taxonomic tree for the 5 reference genomes\"\"\"\n",
    "    taxonomy = {\n",
    "        'E. coli': ['Bacteria', 'Proteobacteria', 'Gammaproteobacteria', 'Enterobacterales', 'Enterobacteriaceae', 'Escherichia', 'E. coli'],\n",
    "        'B. subtilis': ['Bacteria', 'Firmicutes', 'Bacilli', 'Bacillales', 'Bacillaceae', 'Bacillus', 'B. subtilis'],\n",
    "        'P. aeruginosa': ['Bacteria', 'Proteobacteria', 'Gammaproteobacteria', 'Pseudomonadales', 'Pseudomonadaceae', 'Pseudomonas', 'P. aeruginosa'],\n",
    "        'S. aureus': ['Bacteria', 'Firmicutes', 'Bacilli', 'Bacillales', 'Staphylococcaceae', 'Staphylococcus', 'S. aureus'],\n",
    "        'M. tuberculosis': ['Bacteria', 'Actinobacteria', 'Actinomycetia', 'Mycobacteriales', 'Mycobacteriaceae', 'Mycobacterium', 'M. tuberculosis'],\n",
    "        # Add variations of names with different separators\n",
    "        'E_coli': ['Bacteria', 'Proteobacteria', 'Gammaproteobacteria', 'Enterobacterales', 'Enterobacteriaceae', 'Escherichia', 'E_coli'],\n",
    "        'B_subtilis': ['Bacteria', 'Firmicutes', 'Bacilli', 'Bacillales', 'Bacillaceae', 'Bacillus', 'B_subtilis'],\n",
    "        'P_aeruginosa': ['Bacteria', 'Proteobacteria', 'Gammaproteobacteria', 'Pseudomonadales', 'Pseudomonadaceae', 'Pseudomonas', 'P_aeruginosa'],\n",
    "        'S_aureus': ['Bacteria', 'Firmicutes', 'Bacilli', 'Bacillales', 'Staphylococcaceae', 'Staphylococcus', 'S_aureus'],\n",
    "        'M_tuberculosis': ['Bacteria', 'Actinobacteria', 'Actinomycetia', 'Mycobacteriales', 'Mycobacteriaceae', 'Mycobacterium', 'M_tuberculosis'],\n",
    "        # Add double underscore versions that might be created by regex\n",
    "        'E__coli': ['Bacteria', 'Proteobacteria', 'Gammaproteobacteria', 'Enterobacterales', 'Enterobacteriaceae', 'Escherichia', 'E__coli'],\n",
    "        'B__subtilis': ['Bacteria', 'Firmicutes', 'Bacilli', 'Bacillales', 'Bacillaceae', 'Bacillus', 'B__subtilis'],\n",
    "        'P__aeruginosa': ['Bacteria', 'Proteobacteria', 'Gammaproteobacteria', 'Pseudomonadales', 'Pseudomonadaceae', 'Pseudomonas', 'P__aeruginosa'],\n",
    "        'S__aureus': ['Bacteria', 'Firmicutes', 'Bacilli', 'Bacillales', 'Staphylococcaceae', 'Staphylococcus', 'S__aureus'],\n",
    "        'M__tuberculosis': ['Bacteria', 'Actinobacteria', 'Actinomycetia', 'Mycobacteriales', 'Mycobacteriaceae', 'Mycobacterium', 'M__tuberculosis']\n",
    "    }\n",
    "    return taxonomy\n",
    "\n",
    "def find_lca(taxonomy, species_list):\n",
    "    \"\"\"Find the lowest common ancestor of a list of species\"\"\"\n",
    "    if not species_list:\n",
    "        return \"Unknown\"\n",
    "    if len(species_list) == 1:\n",
    "        return species_list[0]  # Return the species if only one match\n",
    "    \n",
    "    # Get taxonomic paths for each species\n",
    "    paths = [taxonomy[species] for species in species_list]\n",
    "    \n",
    "    # Find the common prefix of all paths\n",
    "    common_path = []\n",
    "    for i in range(min(len(path) for path in paths)):\n",
    "        if len(set(path[i] for path in paths)) == 1:\n",
    "            common_path.append(paths[0][i])\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return common_path[-1] if common_path else \"Bacteria\"\n",
    "\n",
    "# ==================== BLAST Database Creation ====================\n",
    "\n",
    "def create_blast_db(fasta_files, temp_dir):\n",
    "    \"\"\"Create BLAST databases for each genome\"\"\"\n",
    "    db_paths = {}\n",
    "    \n",
    "    print(\"Creating BLAST databases...\")\n",
    "    for fasta_file in fasta_files:\n",
    "        base_name = os.path.basename(fasta_file)\n",
    "        species = base_name.split('.')[0]\n",
    "        \n",
    "        # Use prettier names for the species but without spaces\n",
    "        if species == \"e_coli\":\n",
    "            species_display = \"E. coli\"\n",
    "            species_id = \"E_coli\"\n",
    "        elif species == \"b_subtilis\":\n",
    "            species_display = \"B. subtilis\"\n",
    "            species_id = \"B_subtilis\"\n",
    "        elif species == \"p_aeruginosa\":\n",
    "            species_display = \"P. aeruginosa\"\n",
    "            species_id = \"P_aeruginosa\"\n",
    "        elif species == \"s_aureus\":\n",
    "            species_display = \"S. aureus\"\n",
    "            species_id = \"S_aureus\"\n",
    "        elif species == \"m_tuberculosis\":\n",
    "            species_display = \"M. tuberculosis\"\n",
    "            species_id = \"M_tuberculosis\"\n",
    "        else:\n",
    "            species_display = species\n",
    "            species_id = species\n",
    "        \n",
    "        # Create a copy of the FASTA file in the temp directory (avoid spaces in filenames)\n",
    "        db_path = os.path.join(temp_dir, species_id)\n",
    "        shutil.copy(fasta_file, f\"{db_path}.fna\")\n",
    "        \n",
    "        # Create the BLAST database\n",
    "        print(f\"Creating BLAST database for {species_display}...\")\n",
    "        cmd = [\"makeblastdb\", \"-in\", f\"{db_path}.fna\", \"-dbtype\", \"nucl\", \"-out\", db_path, \"-title\", species_id]\n",
    "        \n",
    "        try:\n",
    "            subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            print(f\"Successfully created database for {species_display}\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error creating BLAST database for {species_display}:\")\n",
    "            print(f\"Command: {' '.join(cmd)}\")\n",
    "            print(f\"Error: {e}\")\n",
    "            print(f\"stdout: {e.stdout.decode() if e.stdout else 'None'}\")\n",
    "            print(f\"stderr: {e.stderr.decode() if e.stderr else 'None'}\")\n",
    "            raise e\n",
    "        \n",
    "        db_paths[species_display] = db_path\n",
    "    \n",
    "    return db_paths\n",
    "\n",
    "# ==================== FASTQ Parsing Functions ====================\n",
    "\n",
    "def read_fastq_entries(fastq_file, max_reads=None):\n",
    "    \"\"\"Read entries from a FASTQ file and return a list of (header, sequence) tuples\"\"\"\n",
    "    reads = []\n",
    "    count = 0\n",
    "    \n",
    "    with open(fastq_file, 'r') as f:\n",
    "        while True:\n",
    "            if max_reads and count >= max_reads:\n",
    "                break\n",
    "                \n",
    "            # Read FASTQ entry\n",
    "            header = f.readline().strip()\n",
    "            if not header:\n",
    "                break  # End of file\n",
    "                \n",
    "            sequence = f.readline().strip()\n",
    "            f.readline()  # Skip '+' line\n",
    "            quality = f.readline().strip()\n",
    "            \n",
    "            if header and sequence and quality:\n",
    "                reads.append((header, sequence))\n",
    "                count += 1\n",
    "            else:\n",
    "                break  # Incomplete entry\n",
    "    \n",
    "    return reads\n",
    "\n",
    "def write_fasta_file(reads, output_fasta):\n",
    "    \"\"\"Write reads to a FASTA file\"\"\"\n",
    "    with open(output_fasta, 'w') as f:\n",
    "        for header, sequence in reads:\n",
    "            # Extract read ID without @ symbol\n",
    "            read_id = header[1:].split()[0]\n",
    "            f.write(f\">{read_id}\\n{sequence}\\n\")\n",
    "    \n",
    "    return len(reads)\n",
    "\n",
    "# ==================== BLAST Processing ====================\n",
    "\n",
    "def run_blast(query_fasta, db_path, output_file, read_length=100, max_mismatches=1, evalue=1e-5, num_threads=1, max_hits=5):\n",
    "    \"\"\"\n",
    "    Run blastn on the query against the specified database, allowing up to specified mismatches\n",
    "    \n",
    "    Parameters:\n",
    "    - read_length: Average length of reads (used to calculate percent identity)\n",
    "    - max_mismatches: Maximum number of mismatches to allow\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate percent identity threshold based on read length and max mismatches\n",
    "    # For example, allowing 1 mismatch in a 100bp read = 99% identity\n",
    "    percent_identity = 100 - (max_mismatches * 100 / read_length)\n",
    "    \n",
    "    # Use text output format instead of XML for easier parsing\n",
    "    cmd = [\n",
    "        \"blastn\",\n",
    "        \"-query\", query_fasta,\n",
    "        \"-db\", db_path,\n",
    "        \"-out\", output_file,\n",
    "        \"-outfmt\", \"6 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore\",\n",
    "        \"-evalue\", str(evalue),\n",
    "        \"-max_target_seqs\", str(max_hits),\n",
    "        \"-num_threads\", str(num_threads),\n",
    "        \"-perc_identity\", str(percent_identity),\n",
    "        \"-strand\", \"plus\"  # Only search the forward strand\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error running BLAST:\")\n",
    "        print(f\"Command: {' '.join(cmd)}\")\n",
    "        print(f\"Error: {e}\")\n",
    "        print(f\"stdout: {e.stdout.decode() if e.stdout else 'None'}\")\n",
    "        print(f\"stderr: {e.stderr.decode() if e.stderr else 'None'}\")\n",
    "        raise e\n",
    "    \n",
    "    return output_file\n",
    "\n",
    "def process_blast_results(results_file, query_ids, species):\n",
    "    \"\"\"Process BLAST tabular results and extract hits\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Initialize all queries with no hits\n",
    "    for query_id in query_ids:\n",
    "        results[query_id] = []\n",
    "    \n",
    "    # Parse results file\n",
    "    if os.path.getsize(results_file) > 0:  # Check if file is not empty\n",
    "        with open(results_file, 'r') as f:\n",
    "            for line in f:\n",
    "                fields = line.strip().split('\\t')\n",
    "                if len(fields) >= 12:  # Ensure we have all expected fields\n",
    "                    query_id = fields[0]\n",
    "                    mismatches = int(fields[4])  # The mismatch column\n",
    "                    \n",
    "                    # Only add if this is a new hit for this query\n",
    "                    if species not in results[query_id]:\n",
    "                        results[query_id].append(species)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def get_query_ids_from_fasta(fasta_file):\n",
    "    \"\"\"Extract all query IDs from a FASTA file\"\"\"\n",
    "    ids = []\n",
    "    with open(fasta_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('>'):\n",
    "                ids.append(line.strip()[1:])  # Remove '>' and newline\n",
    "    return ids\n",
    "\n",
    "# ==================== Read Classification ====================\n",
    "\n",
    "def classify_reads_with_blast(fastq_files, db_paths, taxonomy, max_reads_per_file=10000, temp_dir=None, num_threads=8, max_mismatches=1):\n",
    "    \"\"\"\n",
    "    Classify reads using BLAST with a specified mismatch tolerance\n",
    "    \n",
    "    Parameters:\n",
    "    - max_mismatches: Maximum number of mismatches to allow in alignments\n",
    "    \"\"\"\n",
    "    if temp_dir is None:\n",
    "        temp_dir = tempfile.mkdtemp()\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for set_name, file_paths in fastq_files.items():\n",
    "        print(f\"\\nProcessing {set_name}:\")\n",
    "        for path in file_paths:\n",
    "            print(f\"  - {path}\")\n",
    "        \n",
    "        start_process = time.time()\n",
    "        \n",
    "        # Read FASTQ files and combine reads\n",
    "        combined_reads = []\n",
    "        for fastq_file in file_paths:\n",
    "            reads = read_fastq_entries(fastq_file, max_reads=max_reads_per_file)\n",
    "            combined_reads.extend(reads)\n",
    "            print(f\"Read {len(reads)} reads from {fastq_file}\")\n",
    "        \n",
    "        print(f\"Total combined reads: {len(combined_reads)}\")\n",
    "        \n",
    "        # Calculate average read length for percent identity threshold\n",
    "        total_length = sum(len(seq) for _, seq in combined_reads)\n",
    "        avg_read_length = total_length / len(combined_reads) if combined_reads else 100\n",
    "        \n",
    "        print(f\"Average read length: {avg_read_length:.2f}bp\")\n",
    "        print(f\"Setting up BLAST to allow maximum {max_mismatches} mismatch(es)\")\n",
    "        percent_identity = 100 - (max_mismatches * 100 / avg_read_length)\n",
    "        print(f\"Using percent identity threshold of {percent_identity:.2f}%\")\n",
    "        \n",
    "        # Write combined reads to a FASTA file\n",
    "        combined_fasta = os.path.join(temp_dir, f\"{set_name}_combined.fasta\")\n",
    "        write_fasta_file(combined_reads, combined_fasta)\n",
    "        \n",
    "        # Get all query IDs\n",
    "        query_ids = get_query_ids_from_fasta(combined_fasta)\n",
    "        \n",
    "        # Initialize results dictionary\n",
    "        classification_results = {query_id: [] for query_id in query_ids}\n",
    "        \n",
    "        # Run BLAST against each reference genome\n",
    "        for species, db_path in db_paths.items():\n",
    "            print(f\"BLASTing against {species}...\")\n",
    "            blast_start = time.time()\n",
    "            \n",
    "            # Run BLAST search\n",
    "            output_file = os.path.join(temp_dir, f\"{set_name}_{re.sub(r'[. ]', '_', species)}_blast.out\")\n",
    "            run_blast(\n",
    "                combined_fasta, \n",
    "                db_path, \n",
    "                output_file, \n",
    "                read_length=avg_read_length,\n",
    "                max_mismatches=max_mismatches,\n",
    "                num_threads=num_threads\n",
    "            )\n",
    "            \n",
    "            # Process BLAST results\n",
    "            species_results = process_blast_results(output_file, query_ids, species)\n",
    "            \n",
    "            # Merge results\n",
    "            for query_id, hits in species_results.items():\n",
    "                classification_results[query_id].extend(hits)\n",
    "            \n",
    "            print(f\"BLAST against {species} completed in {time.time() - blast_start:.2f} seconds\")\n",
    "        \n",
    "        # Convert to the format used by the suffix array implementation\n",
    "        formatted_results = []\n",
    "        for query_id, matching_genomes in classification_results.items():\n",
    "            if not matching_genomes:\n",
    "                formatted_results.append((f\"@{query_id}\", \"Unclassified\", \"Unknown\"))\n",
    "            else:\n",
    "                # Use the original species names for taxonomy lookup\n",
    "                lca = find_lca(taxonomy, matching_genomes)\n",
    "                formatted_results.append((f\"@{query_id}\", matching_genomes, lca))\n",
    "        \n",
    "        all_results[set_name] = formatted_results\n",
    "        \n",
    "        # Calculate processing time\n",
    "        process_time = time.time() - start_process\n",
    "        \n",
    "        # Summarize and print results\n",
    "        summary = summarize_results(formatted_results)\n",
    "        print_summary(summary)\n",
    "        \n",
    "        print(f\"Processing time: {process_time:.2f} seconds\")\n",
    "        print(f\"Reads per second: {len(formatted_results) / process_time:.2f}\")\n",
    "        \n",
    "        print(f\"Memory after processing {set_name}: {track_memory()} MB\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def summarize_results(classification_results):\n",
    "    \"\"\"Summarize classification results\"\"\"\n",
    "    # Count matches per genome\n",
    "    single_matches = Counter()\n",
    "    multi_matches = Counter()\n",
    "    lca_classifications = Counter()\n",
    "    unclassified = 0\n",
    "    \n",
    "    for _, classification, lca in classification_results:\n",
    "        if classification == \"Unclassified\":\n",
    "            unclassified += 1\n",
    "        elif len(classification) == 1:\n",
    "            single_matches[classification[0]] += 1\n",
    "        else:\n",
    "            # For multi-matches, count each genome\n",
    "            for genome in classification:\n",
    "                multi_matches[genome] += 1\n",
    "            # Also count by LCA\n",
    "            lca_classifications[lca] += 1\n",
    "    \n",
    "    # Prepare summary\n",
    "    summary = {\n",
    "        \"total_reads\": len(classification_results),\n",
    "        \"single_matches\": dict(single_matches),\n",
    "        \"multi_matches\": dict(multi_matches),\n",
    "        \"lca_classifications\": dict(lca_classifications),\n",
    "        \"unclassified\": unclassified\n",
    "    }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def print_summary(summary):\n",
    "    \"\"\"Print a formatted summary of classification results\"\"\"\n",
    "    print(\"\\n====================== CLASSIFICATION SUMMARY ======================\")\n",
    "    print(f\"Total reads processed: {summary['total_reads']}\")\n",
    "    \n",
    "    print(\"\\nSingle Genome Matches:\")\n",
    "    for genome, count in summary['single_matches'].items():\n",
    "        percentage = (count / summary['total_reads']) * 100\n",
    "        print(f\"- {genome}: {count} reads ({percentage:.2f}%)\")\n",
    "    \n",
    "    if summary['lca_classifications']:\n",
    "        print(\"\\nMulti-Genome Matches (by LCA):\")\n",
    "        for lca, count in summary['lca_classifications'].items():\n",
    "            percentage = (count / summary['total_reads']) * 100\n",
    "            print(f\"- {lca}: {count} reads ({percentage:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nUnclassified: {summary['unclassified']} reads ({(summary['unclassified'] / summary['total_reads']) * 100:.2f}%)\")\n",
    "    print(\"===================================================================\")\n",
    "\n",
    "# ==================== Main Function ====================\n",
    "\n",
    "def main(num_threads=8, max_reads_per_file=10000, max_mismatches=1):\n",
    "    \"\"\"\n",
    "    Main function to run BLAST-based classification allowing up to the specified number of mismatches\n",
    "    \n",
    "    Parameters:\n",
    "    - num_threads: Number of threads to use for BLAST\n",
    "    - max_reads_per_file: Maximum number of reads to process from each file\n",
    "    - max_mismatches: Maximum number of mismatches to allow in alignments\n",
    "    \"\"\"\n",
    "    # Check if BLAST+ is installed\n",
    "    if not check_blast_installation():\n",
    "        print(\"Please install BLAST+ tools before running this script\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Starting BLAST-based metagenomic classification with {num_threads} threads\")\n",
    "    print(f\"Allowing up to {max_mismatches} mismatch(es) per read\")\n",
    "    \n",
    "    # Define file paths\n",
    "    genome_files = [\n",
    "        \"b_subtilis.fna\",\n",
    "        \"e_coli.fna\",\n",
    "        \"m_tuberculosis.fna\",\n",
    "        \"p_aeruginosa.fna\",\n",
    "        \"s_aureus.fna\"\n",
    "    ]\n",
    "    \n",
    "    # Define combined read file sets\n",
    "    read_file_sets = {\n",
    "        \"error_free_reads\": [\n",
    "            \"simulated_reads_no_errors_10k_R1.fastq\",\n",
    "            \"simulated_reads_no_errors_10k_R2.fastq\"\n",
    "        ],\n",
    "        \"with_errors_reads\": [\n",
    "            \"simulated_reads_miseq_10k_R1.fastq\", \n",
    "            \"simulated_reads_miseq_10k_R2.fastq\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Create a temporary directory\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    print(f\"Using temporary directory: {temp_dir}\")\n",
    "    \n",
    "    try:\n",
    "        # Create BLAST databases\n",
    "        db_paths = create_blast_db(genome_files, temp_dir)\n",
    "        taxonomy = generate_taxonomic_tree()\n",
    "        \n",
    "        print(f\"Memory after creating BLAST databases: {track_memory()} MB\")\n",
    "        \n",
    "        # Classify reads\n",
    "        all_results = classify_reads_with_blast(\n",
    "            read_file_sets, \n",
    "            db_paths, \n",
    "            taxonomy, \n",
    "            max_reads_per_file=max_reads_per_file,\n",
    "            temp_dir=temp_dir,\n",
    "            num_threads=num_threads,\n",
    "            max_mismatches=max_mismatches\n",
    "        )\n",
    "        \n",
    "        # Performance report\n",
    "        total_time = time.time() - start_time\n",
    "        peak_memory = max(memory_usage)\n",
    "        \n",
    "        print(\"\\n====================== PERFORMANCE SUMMARY ======================\")\n",
    "        print(f\"Total execution time: {total_time:.2f} seconds\")\n",
    "        print(f\"Peak memory usage: {peak_memory:.2f} MB\")\n",
    "        print(f\"Maximum mismatches allowed: {max_mismatches}\")\n",
    "        \n",
    "        # Calculate processing statistics\n",
    "        total_reads = sum(len(results) for results in all_results.values())\n",
    "        reads_per_second = total_reads / total_time\n",
    "        \n",
    "        print(f\"\\nTotal reads processed: {total_reads}\")\n",
    "        print(f\"Overall processing speed: {reads_per_second:.2f} reads per second\")\n",
    "        print(\"================================================================\")\n",
    "        \n",
    "    finally:\n",
    "        # Clean up temporary directory\n",
    "        print(f\"Cleaning up temporary directory: {temp_dir}\")\n",
    "        shutil.rmtree(temp_dir)\n",
    "\n",
    "# Run the main function with the specified number of threads\n",
    "if __name__ == \"__main__\":\n",
    "    # Use 8 threads for BLAST (or adjust based on your system)\n",
    "    NUM_THREADS = 8\n",
    "    \n",
    "    # Maximum number of mismatches to allow (Task 1.3)\n",
    "    MAX_MISMATCHES = 0\n",
    "    \n",
    "    main(num_threads=NUM_THREADS, max_mismatches=MAX_MISMATCHES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
