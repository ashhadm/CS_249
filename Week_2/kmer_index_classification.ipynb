{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c971e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "from collections import defaultdict, Counter\n",
    "import psutil\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Performance tracking\n",
    "start_time = time.time()\n",
    "memory_usage = []\n",
    "\n",
    "def track_memory():\n",
    "    \"\"\"Track current memory usage\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_usage.append(process.memory_info().rss / 1024 / 1024)  # MB\n",
    "    return memory_usage[-1]\n",
    "\n",
    "print(f\"Starting with memory: {track_memory()} MB\")\n",
    "\n",
    "# ==================== K-mer Encoding Functions ====================\n",
    "\n",
    "def encode_kmer(kmer):\n",
    "    \"\"\"\n",
    "    Encode a k-mer string into a 64-bit integer using 2-bit encoding\n",
    "    A=00, C=01, G=10, T=11\n",
    "    \"\"\"\n",
    "    encoding = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "    result = 0\n",
    "    for base in kmer:\n",
    "        result = (result << 2) | encoding.get(base, 0)\n",
    "    return result\n",
    "\n",
    "def decode_kmer(encoded, k):\n",
    "    \"\"\"\n",
    "    Decode a 2-bit encoded k-mer back to a string\n",
    "    \"\"\"\n",
    "    decoding = {0: 'A', 1: 'C', 2: 'G', 3: 'T'}\n",
    "    result = []\n",
    "    for i in range(k):\n",
    "        # Extract 2 bits from the end\n",
    "        bits = (encoded >> (2*i)) & 3\n",
    "        result.append(decoding[bits])\n",
    "    return ''.join(reversed(result))\n",
    "\n",
    "# ==================== FASTA/FASTQ Parsing Functions ====================\n",
    "\n",
    "def read_fasta_file(filename):\n",
    "    \"\"\"Read a FASTA file and return the sequence\"\"\"\n",
    "    sequence = \"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        # Skip header line\n",
    "        header = f.readline().strip()\n",
    "        # Read sequence lines\n",
    "        for line in f:\n",
    "            if line.startswith('>'):\n",
    "                continue  # Skip additional headers\n",
    "            sequence += line.strip()\n",
    "    return sequence\n",
    "\n",
    "def read_fastq_file(filename, max_reads=None):\n",
    "    \"\"\"Read a FASTQ file and return reads as a list of (header, sequence) tuples\"\"\"\n",
    "    reads = []\n",
    "    count = 0\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        while True:\n",
    "            if max_reads and count >= max_reads:\n",
    "                break\n",
    "                \n",
    "            header = f.readline().strip()\n",
    "            if not header:\n",
    "                break  # End of file\n",
    "                \n",
    "            sequence = f.readline().strip()\n",
    "            f.readline()  # Skip \"+\" line\n",
    "            quality = f.readline().strip()\n",
    "            \n",
    "            if header and sequence and quality:\n",
    "                reads.append((header, sequence))\n",
    "                count += 1\n",
    "            else:\n",
    "                break  # Incomplete entry\n",
    "    \n",
    "    return reads\n",
    "\n",
    "def read_genome_files(filenames):\n",
    "    \"\"\"Read all genome files and return a dictionary of sequences\"\"\"\n",
    "    print(\"Reading reference genomes...\")\n",
    "    genomes = {}\n",
    "    \n",
    "    for filename in filenames:\n",
    "        base_name = os.path.basename(filename)\n",
    "        # Extract species name from filename\n",
    "        species = base_name.split('.')[0]\n",
    "        \n",
    "        # Use prettier names for the species\n",
    "        if species == \"e_coli\":\n",
    "            species = \"E. coli\"\n",
    "        elif species == \"b_subtilis\":\n",
    "            species = \"B. subtilis\"\n",
    "        elif species == \"p_aeruginosa\":\n",
    "            species = \"P. aeruginosa\"\n",
    "        elif species == \"s_aureus\":\n",
    "            species = \"S. aureus\"\n",
    "        elif species == \"m_tuberculosis\":\n",
    "            species = \"M. tuberculosis\"\n",
    "        \n",
    "        print(f\"Reading genome for {species}...\")\n",
    "        sequence = read_fasta_file(filename)\n",
    "        genomes[species] = sequence\n",
    "        print(f\"Read {len(sequence)} bases for {species}\")\n",
    "    \n",
    "    return genomes\n",
    "\n",
    "def generate_taxonomic_tree():\n",
    "    \"\"\"Generate a taxonomic tree for the 5 reference genomes\"\"\"\n",
    "    taxonomy = {\n",
    "        'E. coli': ['Bacteria', 'Proteobacteria', 'Gammaproteobacteria', 'Enterobacterales', 'Enterobacteriaceae', 'Escherichia', 'E. coli'],\n",
    "        'B. subtilis': ['Bacteria', 'Firmicutes', 'Bacilli', 'Bacillales', 'Bacillaceae', 'Bacillus', 'B. subtilis'],\n",
    "        'P. aeruginosa': ['Bacteria', 'Proteobacteria', 'Gammaproteobacteria', 'Pseudomonadales', 'Pseudomonadaceae', 'Pseudomonas', 'P. aeruginosa'],\n",
    "        'S. aureus': ['Bacteria', 'Firmicutes', 'Bacilli', 'Bacillales', 'Staphylococcaceae', 'Staphylococcus', 'S. aureus'],\n",
    "        'M. tuberculosis': ['Bacteria', 'Actinobacteria', 'Actinomycetia', 'Mycobacteriales', 'Mycobacteriaceae', 'Mycobacterium', 'M. tuberculosis']\n",
    "    }\n",
    "    return taxonomy\n",
    "\n",
    "def find_lca(taxonomy, species_list):\n",
    "    \"\"\"Find the lowest common ancestor of a list of species\"\"\"\n",
    "    if not species_list:\n",
    "        return \"Unknown\"\n",
    "    if len(species_list) == 1:\n",
    "        return species_list[0]  # Return the species if only one match\n",
    "    \n",
    "    # Get taxonomic paths for each species\n",
    "    paths = [taxonomy[species] for species in species_list]\n",
    "    \n",
    "    # Find the common prefix of all paths\n",
    "    common_path = []\n",
    "    for i in range(min(len(path) for path in paths)):\n",
    "        if len(set(path[i] for path in paths)) == 1:\n",
    "            common_path.append(paths[0][i])\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return common_path[-1] if common_path else \"Bacteria\"\n",
    "\n",
    "# ==================== K-mer Index Implementation ====================\n",
    "\n",
    "def extract_kmers_from_chunk(args):\n",
    "    \"\"\"Extract encoded k-mers from a chunk of a genome sequence\"\"\"\n",
    "    chunk, start_idx, k = args\n",
    "    kmers = []\n",
    "    \n",
    "    for i in range(len(chunk) - k + 1):\n",
    "        kmer = chunk[i:i+k]\n",
    "        if 'N' not in kmer:  # Skip k-mers with ambiguous bases\n",
    "            encoded_kmer = encode_kmer(kmer)\n",
    "            kmers.append((encoded_kmer, start_idx + i))\n",
    "            \n",
    "    return kmers\n",
    "\n",
    "def extract_kmers_parallel(sequence, k=31, chunk_size=1000000, num_processes=8):\n",
    "    \"\"\"Extract k-mers from a sequence in parallel\"\"\"\n",
    "    chunks = []\n",
    "    for i in range(0, len(sequence), chunk_size):\n",
    "        chunk = sequence[i:i+chunk_size+k-1]  # Add k-1 to ensure we catch kmers at chunk boundaries\n",
    "        if len(chunk) >= k:\n",
    "            chunks.append((chunk, i, k))\n",
    "    \n",
    "    with mp.Pool(processes=num_processes) as pool:\n",
    "        results = pool.map(extract_kmers_from_chunk, chunks)\n",
    "    \n",
    "    # Combine results from all chunks\n",
    "    all_kmers = []\n",
    "    for result in results:\n",
    "        all_kmers.extend(result)\n",
    "        \n",
    "    return all_kmers\n",
    "\n",
    "def build_kmer_index_for_genome(genome_name, sequence, k=31, num_processes=8):\n",
    "    \"\"\"Build a k-mer index for a single genome\"\"\"\n",
    "    print(f\"Extracting {k}-mers from {genome_name}...\")\n",
    "    start = time.time()\n",
    "    \n",
    "    # Extract k-mers in parallel\n",
    "    kmers = extract_kmers_parallel(sequence, k, num_processes=num_processes)\n",
    "    \n",
    "    # Build index - using standard dict\n",
    "    genome_index = {}\n",
    "    for encoded_kmer, position in kmers:\n",
    "        if encoded_kmer not in genome_index:\n",
    "            genome_index[encoded_kmer] = []\n",
    "        genome_index[encoded_kmer].append(position)\n",
    "    \n",
    "    print(f\"Found {len(kmers)} {k}-mers ({len(genome_index)} unique) in {genome_name}\")\n",
    "    print(f\"K-mer extraction for {genome_name} completed in {time.time() - start:.2f} seconds\")\n",
    "    \n",
    "    return genome_index\n",
    "\n",
    "def merge_kmer_indices(genome_indices):\n",
    "    \"\"\"Merge k-mer indices from multiple genomes using regular dict\"\"\"\n",
    "    # Use a regular dict instead of defaultdict with lambda\n",
    "    combined_index = {}\n",
    "    \n",
    "    for genome_name, genome_index in genome_indices.items():\n",
    "        for encoded_kmer, positions in genome_index.items():\n",
    "            if encoded_kmer not in combined_index:\n",
    "                combined_index[encoded_kmer] = {}\n",
    "            combined_index[encoded_kmer][genome_name] = positions\n",
    "    \n",
    "    return combined_index\n",
    "\n",
    "def build_kmer_index(genomes, k=31, num_processes=8):\n",
    "    \"\"\"Build a k-mer index for all genomes\"\"\"\n",
    "    genome_indices = {}\n",
    "    \n",
    "    print(f\"Building {k}-mer indices for all genomes...\")\n",
    "    for name, sequence in genomes.items():\n",
    "        print(f\"Processing {name}...\")\n",
    "        genome_indices[name] = build_kmer_index_for_genome(name, sequence, k, num_processes)\n",
    "    \n",
    "    # Merge individual genome indices\n",
    "    print(\"Merging k-mer indices...\")\n",
    "    start = time.time()\n",
    "    combined_index = merge_kmer_indices(genome_indices)\n",
    "    print(f\"Merged k-mer index contains {len(combined_index)} unique k-mers\")\n",
    "    print(f\"Index merging completed in {time.time() - start:.2f} seconds\")\n",
    "    \n",
    "    # Calculate some statistics\n",
    "    kmer_stats = analyze_kmer_index(combined_index, k)\n",
    "    \n",
    "    return combined_index, kmer_stats\n",
    "\n",
    "def analyze_kmer_index(index, k=31):\n",
    "    \"\"\"Analyze k-mer index statistics\"\"\"\n",
    "    total_kmers = len(index)\n",
    "    theoretical_kmers = 4**k\n",
    "    \n",
    "    # Count k-mers unique to each genome vs shared\n",
    "    unique_to_genome = {}  # Using regular dict\n",
    "    for genome in ['E. coli', 'B. subtilis', 'P. aeruginosa', 'S. aureus', 'M. tuberculosis']:\n",
    "        unique_to_genome[genome] = 0\n",
    "    \n",
    "    shared_kmers = 0\n",
    "    \n",
    "    for encoded_kmer, genome_dict in index.items():\n",
    "        if len(genome_dict) == 1:\n",
    "            # Unique to one genome\n",
    "            genome = next(iter(genome_dict.keys()))\n",
    "            unique_to_genome[genome] += 1\n",
    "        else:\n",
    "            # Shared between multiple genomes\n",
    "            shared_kmers += 1\n",
    "    \n",
    "    # Build statistics dictionary\n",
    "    stats = {\n",
    "        \"total_unique_kmers\": total_kmers,\n",
    "        \"theoretical_kmers\": theoretical_kmers,\n",
    "        \"coverage_percent\": (total_kmers / theoretical_kmers) * 100,\n",
    "        \"shared_kmers\": shared_kmers,\n",
    "        \"unique_to_genome\": unique_to_genome\n",
    "    }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def print_kmer_stats(stats):\n",
    "    \"\"\"Print k-mer index statistics\"\"\"\n",
    "    print(\"\\n====================== K-MER INDEX STATISTICS ======================\")\n",
    "    print(f\"Total unique k-mers in index: {stats['total_unique_kmers']:,}\")\n",
    "    print(f\"Theoretical k-mer space (4^k): {stats['theoretical_kmers']:,}\")\n",
    "    print(f\"Space coverage: {stats['coverage_percent']:.10f}%\")\n",
    "    \n",
    "    print(\"\\nK-mers unique to each genome:\")\n",
    "    for genome, count in stats['unique_to_genome'].items():\n",
    "        print(f\"- {genome}: {count:,} unique k-mers\")\n",
    "    \n",
    "    print(f\"\\nK-mers shared between genomes: {stats['shared_kmers']:,}\")\n",
    "    print(\"==================================================================\")\n",
    "\n",
    "# ==================== K-mer Based Classification ====================\n",
    "\n",
    "def extract_read_kmers(sequence, k=31):\n",
    "    \"\"\"Extract encoded k-mers from a read sequence\"\"\"\n",
    "    encoded_kmers = []\n",
    "    for i in range(len(sequence) - k + 1):\n",
    "        kmer = sequence[i:i+k]\n",
    "        if 'N' not in kmer:  # Skip k-mers with ambiguous bases\n",
    "            encoded_kmers.append(encode_kmer(kmer))\n",
    "    return encoded_kmers\n",
    "\n",
    "def process_read_chunk_with_kmers(read_chunk, kmer_index, taxonomy, k=31, min_kmer_fraction=0.1):\n",
    "    \"\"\"Process a chunk of reads against the k-mer index\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for header, sequence in read_chunk:\n",
    "        # Skip very short reads\n",
    "        if len(sequence) < k:\n",
    "            results.append((header, \"Unclassified\", \"Unknown\"))\n",
    "            continue\n",
    "        \n",
    "        # Extract k-mers from the read\n",
    "        read_kmers = extract_read_kmers(sequence, k)\n",
    "        if not read_kmers:\n",
    "            results.append((header, \"Unclassified\", \"Unknown\"))\n",
    "            continue\n",
    "        \n",
    "        # Count matches to each genome\n",
    "        genome_matches = {}  # Using regular dict instead of defaultdict\n",
    "        for genome in ['E. coli', 'B. subtilis', 'P. aeruginosa', 'S. aureus', 'M. tuberculosis']:\n",
    "            genome_matches[genome] = 0\n",
    "            \n",
    "        total_kmers = len(read_kmers)\n",
    "        matched_kmers = 0\n",
    "        \n",
    "        for encoded_kmer in read_kmers:\n",
    "            if encoded_kmer in kmer_index:\n",
    "                matched_kmers += 1\n",
    "                for genome in kmer_index[encoded_kmer]:\n",
    "                    genome_matches[genome] += 1\n",
    "        \n",
    "        # Determine best matching genome(s)\n",
    "        max_matches = max(genome_matches.values())\n",
    "        if max_matches == 0:\n",
    "            results.append((header, \"Unclassified\", \"Unknown\"))\n",
    "            continue\n",
    "            \n",
    "        match_fraction = max_matches / total_kmers if total_kmers > 0 else 0\n",
    "        \n",
    "        # Classify based on matches and threshold\n",
    "        if match_fraction < min_kmer_fraction:\n",
    "            results.append((header, \"Unclassified\", \"Unknown\"))\n",
    "        else:\n",
    "            best_genomes = [g for g, c in genome_matches.items() if c == max_matches]\n",
    "            if len(best_genomes) == 1:\n",
    "                results.append((header, best_genomes, best_genomes[0]))\n",
    "            else:\n",
    "                lca = find_lca(taxonomy, best_genomes)\n",
    "                results.append((header, best_genomes, lca))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def distribute_reads(reads, num_chunks):\n",
    "    \"\"\"Distribute reads evenly across chunks\"\"\"\n",
    "    chunk_size = max(1, len(reads) // num_chunks)\n",
    "    return [reads[i:i + chunk_size] for i in range(0, len(reads), chunk_size)]\n",
    "\n",
    "def classify_reads_parallel(reads, kmer_index, taxonomy, k=31, num_processes=8, min_kmer_fraction=0.1):\n",
    "    \"\"\"Classify reads in parallel using multiple processes\"\"\"\n",
    "    # Split reads into chunks for parallelization\n",
    "    read_chunks = distribute_reads(reads, num_processes)\n",
    "    \n",
    "    # Process each chunk in parallel\n",
    "    with mp.Pool(processes=num_processes) as pool:\n",
    "        process_func = partial(process_read_chunk_with_kmers, \n",
    "                               kmer_index=kmer_index, \n",
    "                               taxonomy=taxonomy, \n",
    "                               k=k, \n",
    "                               min_kmer_fraction=min_kmer_fraction)\n",
    "        chunk_results = pool.map(process_func, read_chunks)\n",
    "    \n",
    "    # Combine results from all chunks\n",
    "    all_results = []\n",
    "    for result in chunk_results:\n",
    "        all_results.extend(result)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def summarize_results(classification_results):\n",
    "    \"\"\"Summarize classification results\"\"\"\n",
    "    # Count matches per genome\n",
    "    single_matches = Counter()\n",
    "    multi_matches = Counter()\n",
    "    lca_classifications = Counter()\n",
    "    unclassified = 0\n",
    "    \n",
    "    for _, classification, lca in classification_results:\n",
    "        if classification == \"Unclassified\":\n",
    "            unclassified += 1\n",
    "        elif len(classification) == 1:\n",
    "            single_matches[classification[0]] += 1\n",
    "        else:\n",
    "            # For multi-matches, count each genome\n",
    "            for genome in classification:\n",
    "                multi_matches[genome] += 1\n",
    "            # Also count by LCA\n",
    "            lca_classifications[lca] += 1\n",
    "    \n",
    "    # Prepare summary\n",
    "    summary = {\n",
    "        \"total_reads\": len(classification_results),\n",
    "        \"single_matches\": dict(single_matches),\n",
    "        \"multi_matches\": dict(multi_matches),\n",
    "        \"lca_classifications\": dict(lca_classifications),\n",
    "        \"unclassified\": unclassified\n",
    "    }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def print_summary(summary):\n",
    "    \"\"\"Print a formatted summary of classification results\"\"\"\n",
    "    print(\"\\n====================== CLASSIFICATION SUMMARY ======================\")\n",
    "    print(f\"Total reads processed: {summary['total_reads']}\")\n",
    "    \n",
    "    print(\"\\nSingle Genome Matches:\")\n",
    "    for genome, count in summary['single_matches'].items():\n",
    "        percentage = (count / summary['total_reads']) * 100\n",
    "        print(f\"- {genome}: {count} reads ({percentage:.2f}%)\")\n",
    "    \n",
    "    if summary['lca_classifications']:\n",
    "        print(\"\\nMulti-Genome Matches (by LCA):\")\n",
    "        for lca, count in summary['lca_classifications'].items():\n",
    "            percentage = (count / summary['total_reads']) * 100\n",
    "            print(f\"- {lca}: {count} reads ({percentage:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nUnclassified: {summary['unclassified']} reads ({(summary['unclassified'] / summary['total_reads']) * 100:.2f}%)\")\n",
    "    print(\"===================================================================\")\n",
    "\n",
    "# ==================== Main Function ====================\n",
    "\n",
    "def main(k=31, num_processes=8, max_reads_per_file=10000, min_kmer_fraction=1.0):\n",
    "    \"\"\"Main function to run the entire classification pipeline\"\"\"\n",
    "    print(f\"Starting k-mer based metagenomic classification with k={k} and {num_processes} processes\")\n",
    "    print(f\"Using minimum k-mer match fraction: {min_kmer_fraction}\")\n",
    "    \n",
    "    # Define file paths\n",
    "    genome_files = [\n",
    "        \"b_subtilis.fna\",\n",
    "        \"e_coli.fna\",\n",
    "        \"m_tuberculosis.fna\",\n",
    "        \"p_aeruginosa.fna\",\n",
    "        \"s_aureus.fna\"\n",
    "    ]\n",
    "    \n",
    "    # Define combined read file sets\n",
    "    read_file_sets = {\n",
    "        \"error_free_reads\": [\n",
    "            \"simulated_reads_no_errors_10k_R1.fastq\",\n",
    "            \"simulated_reads_no_errors_10k_R2.fastq\"\n",
    "        ],\n",
    "        \"with_errors_reads\": [\n",
    "            \"simulated_reads_miseq_10k_R1.fastq\", \n",
    "            \"simulated_reads_miseq_10k_R2.fastq\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Load reference genomes\n",
    "    genomes = read_genome_files(genome_files)\n",
    "    taxonomy = generate_taxonomic_tree()\n",
    "    \n",
    "    print(f\"Memory after loading reference genomes: {track_memory()} MB\")\n",
    "    \n",
    "    # Build k-mer index\n",
    "    index_start = time.time()\n",
    "    kmer_index, kmer_stats = build_kmer_index(genomes, k=k, num_processes=num_processes)\n",
    "    index_time = time.time() - index_start\n",
    "    \n",
    "    print(f\"K-mer index built in {index_time:.2f} seconds\")\n",
    "    print(f\"Memory after building k-mer index: {track_memory()} MB\")\n",
    "    \n",
    "    # Print k-mer statistics\n",
    "    print_kmer_stats(kmer_stats)\n",
    "    \n",
    "    # Process each combined read file set\n",
    "    all_results = {}\n",
    "    \n",
    "    for set_name, file_paths in read_file_sets.items():\n",
    "        print(f\"\\nProcessing {set_name}:\")\n",
    "        for path in file_paths:\n",
    "            print(f\"  - {path}\")\n",
    "        \n",
    "        start_process = time.time()\n",
    "        \n",
    "        # Read and combine FASTQ files\n",
    "        combined_reads = []\n",
    "        for file_path in file_paths:\n",
    "            reads = read_fastq_file(file_path, max_reads=max_reads_per_file)\n",
    "            combined_reads.extend(reads)\n",
    "            print(f\"Read {len(reads)} reads from {file_path}\")\n",
    "            \n",
    "        print(f\"Total combined reads: {len(combined_reads)}\")\n",
    "        \n",
    "        # Process reads\n",
    "        results = classify_reads_parallel(\n",
    "            combined_reads, \n",
    "            kmer_index, \n",
    "            taxonomy, \n",
    "            k=k,\n",
    "            num_processes=num_processes,\n",
    "            min_kmer_fraction=min_kmer_fraction\n",
    "        )\n",
    "        \n",
    "        process_time = time.time() - start_process\n",
    "        all_results[set_name] = results\n",
    "        \n",
    "        # Summarize and print results\n",
    "        summary = summarize_results(results)\n",
    "        print_summary(summary)\n",
    "        \n",
    "        print(f\"Processing time: {process_time:.2f} seconds\")\n",
    "        print(f\"Reads per second: {len(results) / process_time:.2f}\")\n",
    "        \n",
    "        print(f\"Memory after processing {set_name}: {track_memory()} MB\")\n",
    "    \n",
    "    # Performance report\n",
    "    total_time = time.time() - start_time\n",
    "    peak_memory = max(memory_usage)\n",
    "    \n",
    "    print(\"\\n====================== PERFORMANCE SUMMARY ======================\")\n",
    "    print(f\"Total execution time: {total_time:.2f} seconds\")\n",
    "    print(f\"K-mer index building time: {index_time:.2f} seconds\")\n",
    "    print(f\"Peak memory usage: {peak_memory:.2f} MB\")\n",
    "    \n",
    "    # Calculate processing statistics\n",
    "    total_reads = sum(len(results) for results in all_results.values())\n",
    "    reads_per_second = total_reads / (total_time - index_time)\n",
    "    \n",
    "    print(f\"\\nTotal reads processed: {total_reads}\")\n",
    "    print(f\"Overall processing speed: {reads_per_second:.2f} reads per second\")\n",
    "    print(\"================================================================\")\n",
    "\n",
    "# Run the main function with the specified parameters\n",
    "if __name__ == \"__main__\":\n",
    "    # Set parameters\n",
    "    K = 31  # k-mer length (as specified in the assignment)\n",
    "    NUM_PROCESSES = 1  # Number of processes to use\n",
    "    MIN_KMER_FRACTION = 1.0  # For exact matching, require 100% of k-mers to match\n",
    "    \n",
    "    main(k=K, num_processes=NUM_PROCESSES, min_kmer_fraction=MIN_KMER_FRACTION)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
