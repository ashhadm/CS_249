{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9d11653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with memory: 272.140625 MB\n",
      "Starting metagenomic classification with 1 processes\n",
      "Reading reference genomes...\n",
      "Reading genome for B. subtilis...\n",
      "Read 4215606 bases for B. subtilis\n",
      "Reading genome for E. coli...\n",
      "Read 4641652 bases for E. coli\n",
      "Reading genome for M. tuberculosis...\n",
      "Read 4411532 bases for M. tuberculosis\n",
      "Reading genome for P. aeruginosa...\n",
      "Read 6264404 bases for P. aeruginosa\n",
      "Reading genome for S. aureus...\n",
      "Read 2821361 bases for S. aureus\n",
      "Memory after loading reference genomes: 272.140625 MB\n",
      "Building suffix arrays for all genomes...\n",
      "Building index for B. subtilis...\n",
      "Index for B. subtilis built in 5.83 seconds\n",
      "Building index for E. coli...\n",
      "Index for E. coli built in 6.70 seconds\n",
      "Building index for M. tuberculosis...\n",
      "Index for M. tuberculosis built in 6.32 seconds\n",
      "Building index for P. aeruginosa...\n",
      "Index for P. aeruginosa built in 9.43 seconds\n",
      "Building index for S. aureus...\n",
      "Index for S. aureus built in 3.79 seconds\n",
      "Memory after building indices: 1567.4140625 MB\n",
      "\n",
      "Processing error_free_reads:\n",
      "  - simulated_reads_no_errors_10k_R1.fastq\n",
      "  - simulated_reads_no_errors_10k_R2.fastq\n",
      "Read 5000 reads from simulated_reads_no_errors_10k_R1.fastq\n",
      "Read 5000 reads from simulated_reads_no_errors_10k_R2.fastq\n",
      "Total combined reads: 10000\n",
      "\n",
      "====================== CLASSIFICATION SUMMARY ======================\n",
      "Total reads processed: 10000\n",
      "\n",
      "Single Genome Matches:\n",
      "- E. coli: 3042 reads (30.42%)\n",
      "- P. aeruginosa: 503 reads (5.03%)\n",
      "- B. subtilis: 505 reads (5.05%)\n",
      "- S. aureus: 503 reads (5.03%)\n",
      "- M. tuberculosis: 506 reads (5.06%)\n",
      "\n",
      "Multi-Genome Matches (by LCA):\n",
      "- Bacteria: 9 reads (0.09%)\n",
      "\n",
      "Unclassified: 4932 reads (49.32%)\n",
      "===================================================================\n",
      "Processing time: 4.36 seconds\n",
      "Reads per second: 2295.19\n",
      "Memory after processing error_free_reads: 1567.23046875 MB\n",
      "\n",
      "Processing with_errors_reads:\n",
      "  - simulated_reads_miseq_10k_R1.fastq\n",
      "  - simulated_reads_miseq_10k_R2.fastq\n",
      "Read 5000 reads from simulated_reads_miseq_10k_R1.fastq\n",
      "Read 5000 reads from simulated_reads_miseq_10k_R2.fastq\n",
      "Total combined reads: 10000\n",
      "\n",
      "====================== CLASSIFICATION SUMMARY ======================\n",
      "Total reads processed: 10000\n",
      "\n",
      "Single Genome Matches:\n",
      "- E. coli: 214 reads (2.14%)\n",
      "- P. aeruginosa: 35 reads (0.35%)\n",
      "- B. subtilis: 48 reads (0.48%)\n",
      "- S. aureus: 36 reads (0.36%)\n",
      "- M. tuberculosis: 42 reads (0.42%)\n",
      "\n",
      "Multi-Genome Matches (by LCA):\n",
      "- Bacteria: 1 reads (0.01%)\n",
      "\n",
      "Unclassified: 9624 reads (96.24%)\n",
      "===================================================================\n",
      "Processing time: 4.51 seconds\n",
      "Reads per second: 2217.50\n",
      "Memory after processing with_errors_reads: 1250.0 MB\n",
      "\n",
      "====================== PERFORMANCE SUMMARY ======================\n",
      "Total execution time: 41.00 seconds\n",
      "Peak memory usage: 1567.41 MB\n",
      "\n",
      "Total reads processed: 20000\n",
      "Overall processing speed: 487.75 reads per second\n",
      "================================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import psutil\n",
    "from functools import partial\n",
    "\n",
    "# Performance tracking\n",
    "start_time = time.time()\n",
    "memory_usage = []\n",
    "\n",
    "def track_memory():\n",
    "    \"\"\"Track current memory usage\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_usage.append(process.memory_info().rss / 1024 / 1024)  # MB\n",
    "    return memory_usage[-1]\n",
    "\n",
    "print(f\"Starting with memory: {track_memory()} MB\")\n",
    "\n",
    "# ==================== FASTA/FASTQ Parsing Functions ====================\n",
    "\n",
    "def read_fasta_file(filename):\n",
    "    \"\"\"Read a FASTA file and return the sequence\"\"\"\n",
    "    sequence = \"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        # Skip header line\n",
    "        header = f.readline().strip()\n",
    "        # Read sequence lines\n",
    "        for line in f:\n",
    "            if line.startswith('>'):\n",
    "                continue  # Skip additional headers\n",
    "            sequence += line.strip()\n",
    "    return sequence\n",
    "\n",
    "def read_fastq_file(filename, max_reads=None):\n",
    "    \"\"\"Read a FASTQ file and return reads as a list of (header, sequence) tuples\"\"\"\n",
    "    reads = []\n",
    "    count = 0\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        while True:\n",
    "            if max_reads and count >= max_reads:\n",
    "                break\n",
    "                \n",
    "            header = f.readline().strip()\n",
    "            if not header:\n",
    "                break  # End of file\n",
    "                \n",
    "            sequence = f.readline().strip()\n",
    "            f.readline()  # Skip \"+\" line\n",
    "            quality = f.readline().strip()\n",
    "            \n",
    "            if header and sequence and quality:\n",
    "                reads.append((header, sequence))\n",
    "                count += 1\n",
    "            else:\n",
    "                break  # Incomplete entry\n",
    "    \n",
    "    return reads\n",
    "\n",
    "def read_genome_files(filenames):\n",
    "    \"\"\"Read all genome files and return a dictionary of sequences\"\"\"\n",
    "    print(\"Reading reference genomes...\")\n",
    "    genomes = {}\n",
    "    \n",
    "    for filename in filenames:\n",
    "        base_name = os.path.basename(filename)\n",
    "        # Extract species name from filename\n",
    "        species = base_name.split('.')[0]\n",
    "        \n",
    "        # Use prettier names for the species\n",
    "        if species == \"e_coli\":\n",
    "            species = \"E. coli\"\n",
    "        elif species == \"b_subtilis\":\n",
    "            species = \"B. subtilis\"\n",
    "        elif species == \"p_aeruginosa\":\n",
    "            species = \"P. aeruginosa\"\n",
    "        elif species == \"s_aureus\":\n",
    "            species = \"S. aureus\"\n",
    "        elif species == \"m_tuberculosis\":\n",
    "            species = \"M. tuberculosis\"\n",
    "        \n",
    "        print(f\"Reading genome for {species}...\")\n",
    "        sequence = read_fasta_file(filename)\n",
    "        genomes[species] = sequence\n",
    "        print(f\"Read {len(sequence)} bases for {species}\")\n",
    "    \n",
    "    return genomes\n",
    "\n",
    "def generate_taxonomic_tree():\n",
    "    \"\"\"Generate a taxonomic tree for the 5 reference genomes\"\"\"\n",
    "    taxonomy = {\n",
    "        'E. coli': ['Bacteria', 'Proteobacteria', 'Gammaproteobacteria', 'Enterobacterales', 'Enterobacteriaceae', 'Escherichia', 'E. coli'],\n",
    "        'B. subtilis': ['Bacteria', 'Firmicutes', 'Bacilli', 'Bacillales', 'Bacillaceae', 'Bacillus', 'B. subtilis'],\n",
    "        'P. aeruginosa': ['Bacteria', 'Proteobacteria', 'Gammaproteobacteria', 'Pseudomonadales', 'Pseudomonadaceae', 'Pseudomonas', 'P. aeruginosa'],\n",
    "        'S. aureus': ['Bacteria', 'Firmicutes', 'Bacilli', 'Bacillales', 'Staphylococcaceae', 'Staphylococcus', 'S. aureus'],\n",
    "        'M. tuberculosis': ['Bacteria', 'Actinobacteria', 'Actinomycetia', 'Mycobacteriales', 'Mycobacteriaceae', 'Mycobacterium', 'M. tuberculosis']\n",
    "    }\n",
    "    return taxonomy\n",
    "\n",
    "def find_lca(taxonomy, species_list):\n",
    "    \"\"\"Find the lowest common ancestor of a list of species\"\"\"\n",
    "    if not species_list:\n",
    "        return \"Unknown\"\n",
    "    if len(species_list) == 1:\n",
    "        return species_list[0]  # Return the species if only one match\n",
    "    \n",
    "    # Get taxonomic paths for each species\n",
    "    paths = [taxonomy[species] for species in species_list]\n",
    "    \n",
    "    # Find the common prefix of all paths\n",
    "    common_path = []\n",
    "    for i in range(min(len(path) for path in paths)):\n",
    "        if len(set(path[i] for path in paths)) == 1:\n",
    "            common_path.append(paths[0][i])\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return common_path[-1] if common_path else \"Bacteria\"\n",
    "\n",
    "# ==================== Suffix Array Implementation (Optimized) ====================\n",
    "\n",
    "def build_suffix_array(text):\n",
    "    \"\"\"Build a suffix array for the given text (optimized for DNA sequences)\"\"\"\n",
    "    n = len(text)\n",
    "    \n",
    "    # For very short texts, use direct approach\n",
    "    if n < 10000:\n",
    "        suffixes = [(text[i:], i) for i in range(n)]\n",
    "        suffixes.sort()\n",
    "        suffix_array = [pos for _, pos in suffixes]\n",
    "        return suffix_array\n",
    "    \n",
    "    # For longer texts, use memory-efficient approach\n",
    "    # Store only indices and compare characters directly during sort\n",
    "    indices = list(range(n))\n",
    "    \n",
    "    # Use Python's built-in sort with our custom comparison function\n",
    "    indices.sort(key=lambda i: (text[i:i+100], i))  # Use first 100 chars as initial sort\n",
    "    \n",
    "    return indices\n",
    "\n",
    "def exact_search(text, pattern, suffix_array):\n",
    "    \"\"\"Fast exact pattern matching using suffix array with direct character comparison\"\"\"\n",
    "    n, m = len(text), len(pattern)\n",
    "    \n",
    "    if m == 0:\n",
    "        return False\n",
    "    \n",
    "    # Binary search for pattern\n",
    "    left, right = 0, n - 1\n",
    "    \n",
    "    while left <= right:\n",
    "        mid = (left + right) // 2\n",
    "        suffix_pos = suffix_array[mid]\n",
    "        \n",
    "        # Compare pattern with current suffix without creating substrings\n",
    "        match = True\n",
    "        comparison = 0\n",
    "        \n",
    "        for i in range(min(m, n - suffix_pos)):\n",
    "            if text[suffix_pos + i] < pattern[i]:\n",
    "                match = False\n",
    "                comparison = -1\n",
    "                break\n",
    "            elif text[suffix_pos + i] > pattern[i]:\n",
    "                match = False\n",
    "                comparison = 1\n",
    "                break\n",
    "        \n",
    "        # If we've compared all characters and they matched\n",
    "        if match:\n",
    "            # If we've reached the end of the pattern, it's a match\n",
    "            if m <= n - suffix_pos:\n",
    "                return True\n",
    "            # If pattern is longer than remaining suffix\n",
    "            comparison = -1\n",
    "        \n",
    "        if comparison < 0:\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            right = mid - 1\n",
    "    \n",
    "    return False\n",
    "\n",
    "def build_genome_indices(genomes):\n",
    "    \"\"\"Build suffix arrays for all genomes\"\"\"\n",
    "    genome_indices = {}\n",
    "    \n",
    "    print(\"Building suffix arrays for all genomes...\")\n",
    "    for name, genome in genomes.items():\n",
    "        print(f\"Building index for {name}...\")\n",
    "        start = time.time()\n",
    "        \n",
    "        # Build suffix array\n",
    "        suffix_array = build_suffix_array(genome)\n",
    "        \n",
    "        genome_indices[name] = {\n",
    "            \"sequence\": genome,\n",
    "            \"suffix_array\": suffix_array\n",
    "        }\n",
    "        \n",
    "        print(f\"Index for {name} built in {time.time() - start:.2f} seconds\")\n",
    "    \n",
    "    return genome_indices\n",
    "\n",
    "# ==================== Read Processing with Direct Parallelization ====================\n",
    "\n",
    "def process_read_chunk(read_chunk, genome_indices, taxonomy):\n",
    "    \"\"\"Process a chunk of reads against all genome indices\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for header, sequence in read_chunk:\n",
    "        # Skip reads with ambiguous bases (N) for exact matching\n",
    "        if 'N' in sequence:\n",
    "            results.append((header, \"Unclassified\", \"Unknown\"))\n",
    "            continue\n",
    "            \n",
    "        # Find exact matches in each genome\n",
    "        matching_genomes = []\n",
    "        \n",
    "        for genome_name, index_data in genome_indices.items():\n",
    "            if exact_search(index_data[\"sequence\"], sequence, index_data[\"suffix_array\"]):\n",
    "                matching_genomes.append(genome_name)\n",
    "        \n",
    "        # Classify based on matches\n",
    "        if not matching_genomes:\n",
    "            classification = \"Unclassified\"\n",
    "            lca = \"Unknown\"\n",
    "        else:\n",
    "            classification = matching_genomes\n",
    "            lca = find_lca(taxonomy, matching_genomes)\n",
    "        \n",
    "        results.append((header, classification, lca))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def distribute_reads(reads, num_chunks):\n",
    "    \"\"\"Distribute reads evenly across chunks\"\"\"\n",
    "    chunk_size = max(1, len(reads) // num_chunks)\n",
    "    return [reads[i:i + chunk_size] for i in range(0, len(reads), chunk_size)]\n",
    "\n",
    "def classify_reads_parallel(reads, genome_indices, taxonomy, num_processes=4):\n",
    "    \"\"\"Classify reads in parallel using multiple processes\"\"\"\n",
    "    # Split reads into chunks for parallelization\n",
    "    read_chunks = distribute_reads(reads, num_processes)\n",
    "    \n",
    "    # Process each chunk in parallel\n",
    "    with mp.Pool(processes=num_processes) as pool:\n",
    "        process_func = partial(process_read_chunk, genome_indices=genome_indices, taxonomy=taxonomy)\n",
    "        chunk_results = pool.map(process_func, read_chunks)\n",
    "    \n",
    "    # Combine results from all chunks\n",
    "    all_results = []\n",
    "    for result in chunk_results:\n",
    "        all_results.extend(result)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def summarize_results(classification_results):\n",
    "    \"\"\"Summarize classification results\"\"\"\n",
    "    # Count matches per genome\n",
    "    single_matches = Counter()\n",
    "    multi_matches = Counter()\n",
    "    lca_classifications = Counter()\n",
    "    unclassified = 0\n",
    "    \n",
    "    for _, classification, lca in classification_results:\n",
    "        if classification == \"Unclassified\":\n",
    "            unclassified += 1\n",
    "        elif len(classification) == 1:\n",
    "            single_matches[classification[0]] += 1\n",
    "        else:\n",
    "            # For multi-matches, count each genome\n",
    "            for genome in classification:\n",
    "                multi_matches[genome] += 1\n",
    "            # Also count by LCA\n",
    "            lca_classifications[lca] += 1\n",
    "    \n",
    "    # Prepare summary\n",
    "    summary = {\n",
    "        \"total_reads\": len(classification_results),\n",
    "        \"single_matches\": dict(single_matches),\n",
    "        \"multi_matches\": dict(multi_matches),\n",
    "        \"lca_classifications\": dict(lca_classifications),\n",
    "        \"unclassified\": unclassified\n",
    "    }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def print_summary(summary):\n",
    "    \"\"\"Print a formatted summary of classification results\"\"\"\n",
    "    print(\"\\n====================== CLASSIFICATION SUMMARY ======================\")\n",
    "    print(f\"Total reads processed: {summary['total_reads']}\")\n",
    "    \n",
    "    print(\"\\nSingle Genome Matches:\")\n",
    "    for genome, count in summary['single_matches'].items():\n",
    "        percentage = (count / summary['total_reads']) * 100\n",
    "        print(f\"- {genome}: {count} reads ({percentage:.2f}%)\")\n",
    "    \n",
    "    if summary['lca_classifications']:\n",
    "        print(\"\\nMulti-Genome Matches (by LCA):\")\n",
    "        total_multi = sum(summary['lca_classifications'].values())\n",
    "        for lca, count in summary['lca_classifications'].items():\n",
    "            percentage = (count / summary['total_reads']) * 100\n",
    "            print(f\"- {lca}: {count} reads ({percentage:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nUnclassified: {summary['unclassified']} reads ({(summary['unclassified'] / summary['total_reads']) * 100:.2f}%)\")\n",
    "    print(\"===================================================================\")\n",
    "\n",
    "# ==================== Main Function ====================\n",
    "\n",
    "def main(num_processes=8, max_reads_per_file=10000):\n",
    "    \"\"\"Main function to run the entire classification pipeline\"\"\"\n",
    "    print(f\"Starting metagenomic classification with {num_processes} processes\")\n",
    "    \n",
    "    # Define file paths\n",
    "    genome_files = [\n",
    "        \"b_subtilis.fna\",\n",
    "        \"e_coli.fna\",\n",
    "        \"m_tuberculosis.fna\",\n",
    "        \"p_aeruginosa.fna\",\n",
    "        \"s_aureus.fna\"\n",
    "    ]\n",
    "    \n",
    "    # Define combined read file sets\n",
    "    read_file_sets = {\n",
    "        \"error_free_reads\": [\n",
    "            \"simulated_reads_no_errors_10k_R1.fastq\",\n",
    "            \"simulated_reads_no_errors_10k_R2.fastq\"\n",
    "        ],\n",
    "        \"with_errors_reads\": [\n",
    "            \"simulated_reads_miseq_10k_R1.fastq\", \n",
    "            \"simulated_reads_miseq_10k_R2.fastq\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Load reference genomes and build indices\n",
    "    genomes = read_genome_files(genome_files)\n",
    "    taxonomy = generate_taxonomic_tree()\n",
    "    \n",
    "    print(f\"Memory after loading reference genomes: {track_memory()} MB\")\n",
    "    \n",
    "    # Build genome indices\n",
    "    genome_indices = build_genome_indices(genomes)\n",
    "    \n",
    "    print(f\"Memory after building indices: {track_memory()} MB\")\n",
    "    \n",
    "    # Process each combined read file set\n",
    "    all_results = {}\n",
    "    \n",
    "    for set_name, file_paths in read_file_sets.items():\n",
    "        print(f\"\\nProcessing {set_name}:\")\n",
    "        for path in file_paths:\n",
    "            print(f\"  - {path}\")\n",
    "        \n",
    "        start_process = time.time()\n",
    "        \n",
    "        # Read and combine FASTQ files\n",
    "        combined_reads = []\n",
    "        for file_path in file_paths:\n",
    "            reads = read_fastq_file(file_path, max_reads=max_reads_per_file)\n",
    "            combined_reads.extend(reads)\n",
    "            print(f\"Read {len(reads)} reads from {file_path}\")\n",
    "            \n",
    "        print(f\"Total combined reads: {len(combined_reads)}\")\n",
    "        \n",
    "        # Process reads\n",
    "        results = classify_reads_parallel(\n",
    "            combined_reads, \n",
    "            genome_indices, \n",
    "            taxonomy, \n",
    "            num_processes=num_processes\n",
    "        )\n",
    "        \n",
    "        process_time = time.time() - start_process\n",
    "        all_results[set_name] = results\n",
    "        \n",
    "        # Summarize and print results\n",
    "        summary = summarize_results(results)\n",
    "        print_summary(summary)\n",
    "        \n",
    "        print(f\"Processing time: {process_time:.2f} seconds\")\n",
    "        print(f\"Reads per second: {len(results) / process_time:.2f}\")\n",
    "        \n",
    "        print(f\"Memory after processing {set_name}: {track_memory()} MB\")\n",
    "    \n",
    "    # Performance report\n",
    "    total_time = time.time() - start_time\n",
    "    peak_memory = max(memory_usage)\n",
    "    \n",
    "    print(\"\\n====================== PERFORMANCE SUMMARY ======================\")\n",
    "    print(f\"Total execution time: {total_time:.2f} seconds\")\n",
    "    print(f\"Peak memory usage: {peak_memory:.2f} MB\")\n",
    "    \n",
    "    # Calculate processing statistics\n",
    "    total_reads = sum(len(results) for results in all_results.values())\n",
    "    reads_per_second = total_reads / total_time\n",
    "    \n",
    "    print(f\"\\nTotal reads processed: {total_reads}\")\n",
    "    print(f\"Overall processing speed: {reads_per_second:.2f} reads per second\")\n",
    "    print(\"================================================================\")\n",
    "\n",
    "# Run the main function with the specified number of processes\n",
    "if __name__ == \"__main__\":\n",
    "    # Set the number of CPU cores to use (adjust as needed)\n",
    "    NUM_PROCESSES = 1  # Using 8 processes for better performance\n",
    "    main(num_processes=NUM_PROCESSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b412ee98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with memory: 255.203125 MB\n",
      "Found blastn at: /usr/bin/blastn\n",
      "Found makeblastdb at: /usr/bin/makeblastdb\n",
      "Starting BLAST-based metagenomic classification with 8 threads\n",
      "Allowing up to 0 mismatch(es) per read\n",
      "Using temporary directory: /tmp/tmp14u7qb18\n",
      "Creating BLAST databases...\n",
      "Creating BLAST database for B. subtilis...\n",
      "Successfully created database for B. subtilis\n",
      "Creating BLAST database for E. coli...\n",
      "Successfully created database for E. coli\n",
      "Creating BLAST database for M. tuberculosis...\n",
      "Successfully created database for M. tuberculosis\n",
      "Creating BLAST database for P. aeruginosa...\n",
      "Successfully created database for P. aeruginosa\n",
      "Creating BLAST database for S. aureus...\n",
      "Successfully created database for S. aureus\n",
      "Memory after creating BLAST databases: 255.203125 MB\n",
      "\n",
      "Processing error_free_reads:\n",
      "  - simulated_reads_no_errors_10k_R1.fastq\n",
      "  - simulated_reads_no_errors_10k_R2.fastq\n",
      "Read 5000 reads from simulated_reads_no_errors_10k_R1.fastq\n",
      "Read 5000 reads from simulated_reads_no_errors_10k_R2.fastq\n",
      "Total combined reads: 10000\n",
      "Average read length: 125.00bp\n",
      "Setting up BLAST to allow maximum 0 mismatch(es)\n",
      "Using percent identity threshold of 100.00%\n",
      "BLASTing against B. subtilis...\n",
      "BLAST against B. subtilis completed in 0.48 seconds\n",
      "BLASTing against E. coli...\n",
      "BLAST against E. coli completed in 0.57 seconds\n",
      "BLASTing against M. tuberculosis...\n",
      "BLAST against M. tuberculosis completed in 0.50 seconds\n",
      "BLASTing against P. aeruginosa...\n",
      "BLAST against P. aeruginosa completed in 0.49 seconds\n",
      "BLASTing against S. aureus...\n",
      "BLAST against S. aureus completed in 0.47 seconds\n",
      "\n",
      "====================== CLASSIFICATION SUMMARY ======================\n",
      "Total reads processed: 10000\n",
      "\n",
      "Single Genome Matches:\n",
      "- E. coli: 3061 reads (30.61%)\n",
      "- P. aeruginosa: 503 reads (5.03%)\n",
      "- B. subtilis: 507 reads (5.07%)\n",
      "- S. aureus: 505 reads (5.05%)\n",
      "- M. tuberculosis: 504 reads (5.04%)\n",
      "\n",
      "Multi-Genome Matches (by LCA):\n",
      "- Bacteria: 9 reads (0.09%)\n",
      "- Gammaproteobacteria: 1 reads (0.01%)\n",
      "\n",
      "Unclassified: 4910 reads (49.10%)\n",
      "===================================================================\n",
      "Processing time: 2.55 seconds\n",
      "Reads per second: 3925.98\n",
      "Memory after processing error_free_reads: 255.203125 MB\n",
      "\n",
      "Processing with_errors_reads:\n",
      "  - simulated_reads_miseq_10k_R1.fastq\n",
      "  - simulated_reads_miseq_10k_R2.fastq\n",
      "Read 5000 reads from simulated_reads_miseq_10k_R1.fastq\n",
      "Read 5000 reads from simulated_reads_miseq_10k_R2.fastq\n",
      "Total combined reads: 10000\n",
      "Average read length: 301.00bp\n",
      "Setting up BLAST to allow maximum 0 mismatch(es)\n",
      "Using percent identity threshold of 100.00%\n",
      "BLASTing against B. subtilis...\n",
      "BLAST against B. subtilis completed in 0.58 seconds\n",
      "BLASTing against E. coli...\n",
      "BLAST against E. coli completed in 0.66 seconds\n",
      "BLASTing against M. tuberculosis...\n",
      "BLAST against M. tuberculosis completed in 0.56 seconds\n",
      "BLASTing against P. aeruginosa...\n",
      "BLAST against P. aeruginosa completed in 0.57 seconds\n",
      "BLASTing against S. aureus...\n",
      "BLAST against S. aureus completed in 0.58 seconds\n",
      "\n",
      "====================== CLASSIFICATION SUMMARY ======================\n",
      "Total reads processed: 10000\n",
      "\n",
      "Single Genome Matches:\n",
      "- E. coli: 331 reads (3.31%)\n",
      "- P. aeruginosa: 50 reads (0.50%)\n",
      "- B. subtilis: 60 reads (0.60%)\n",
      "- S. aureus: 55 reads (0.55%)\n",
      "- M. tuberculosis: 53 reads (0.53%)\n",
      "\n",
      "Multi-Genome Matches (by LCA):\n",
      "- Gammaproteobacteria: 1 reads (0.01%)\n",
      "- Bacteria: 1 reads (0.01%)\n",
      "\n",
      "Unclassified: 9449 reads (94.49%)\n",
      "===================================================================\n",
      "Processing time: 3.00 seconds\n",
      "Reads per second: 3335.89\n",
      "Memory after processing with_errors_reads: 255.203125 MB\n",
      "\n",
      "====================== PERFORMANCE SUMMARY ======================\n",
      "Total execution time: 5.83 seconds\n",
      "Peak memory usage: 255.20 MB\n",
      "Maximum mismatches allowed: 0\n",
      "\n",
      "Total reads processed: 20000\n",
      "Overall processing speed: 3432.53 reads per second\n",
      "================================================================\n",
      "Cleaning up temporary directory: /tmp/tmp14u7qb18\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import subprocess\n",
    "import multiprocessing as mp\n",
    "from collections import Counter, defaultdict\n",
    "import psutil\n",
    "import sys\n",
    "import tempfile\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "# First check if BLAST+ is installed\n",
    "def check_blast_installation():\n",
    "    try:\n",
    "        # Check if blastn is available\n",
    "        result = subprocess.run([\"which\", \"blastn\"], capture_output=True, text=True)\n",
    "        if result.returncode != 0:\n",
    "            print(\"ERROR: BLAST+ tools (blastn, makeblastdb) not found in PATH\")\n",
    "            print(\"\\nPlease install NCBI BLAST+ tools:\")\n",
    "            print(\"For Conda: conda install -c bioconda blast\")\n",
    "            print(\"For Ubuntu/Debian: sudo apt-get install ncbi-blast+\")\n",
    "            return False\n",
    "        \n",
    "        blastn_path = result.stdout.strip()\n",
    "        print(f\"Found blastn at: {blastn_path}\")\n",
    "        \n",
    "        # Check makeblastdb\n",
    "        result = subprocess.run([\"which\", \"makeblastdb\"], capture_output=True, text=True)\n",
    "        if result.returncode != 0:\n",
    "            print(\"ERROR: makeblastdb tool not found in PATH\")\n",
    "            return False\n",
    "            \n",
    "        makeblastdb_path = result.stdout.strip()\n",
    "        print(f\"Found makeblastdb at: {makeblastdb_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking BLAST installation: {e}\")\n",
    "        return False\n",
    "\n",
    "# Performance tracking\n",
    "start_time = time.time()\n",
    "memory_usage = []\n",
    "\n",
    "def track_memory():\n",
    "    \"\"\"Track current memory usage\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_usage.append(process.memory_info().rss / 1024 / 1024)  # MB\n",
    "    return memory_usage[-1]\n",
    "\n",
    "print(f\"Starting with memory: {track_memory()} MB\")\n",
    "\n",
    "# ==================== Taxonomy Functions ====================\n",
    "\n",
    "def generate_taxonomic_tree():\n",
    "    \"\"\"Generate a taxonomic tree for the 5 reference genomes\"\"\"\n",
    "    taxonomy = {\n",
    "        'E. coli': ['Bacteria', 'Proteobacteria', 'Gammaproteobacteria', 'Enterobacterales', 'Enterobacteriaceae', 'Escherichia', 'E. coli'],\n",
    "        'B. subtilis': ['Bacteria', 'Firmicutes', 'Bacilli', 'Bacillales', 'Bacillaceae', 'Bacillus', 'B. subtilis'],\n",
    "        'P. aeruginosa': ['Bacteria', 'Proteobacteria', 'Gammaproteobacteria', 'Pseudomonadales', 'Pseudomonadaceae', 'Pseudomonas', 'P. aeruginosa'],\n",
    "        'S. aureus': ['Bacteria', 'Firmicutes', 'Bacilli', 'Bacillales', 'Staphylococcaceae', 'Staphylococcus', 'S. aureus'],\n",
    "        'M. tuberculosis': ['Bacteria', 'Actinobacteria', 'Actinomycetia', 'Mycobacteriales', 'Mycobacteriaceae', 'Mycobacterium', 'M. tuberculosis'],\n",
    "        # Add variations of names with different separators\n",
    "        'E_coli': ['Bacteria', 'Proteobacteria', 'Gammaproteobacteria', 'Enterobacterales', 'Enterobacteriaceae', 'Escherichia', 'E_coli'],\n",
    "        'B_subtilis': ['Bacteria', 'Firmicutes', 'Bacilli', 'Bacillales', 'Bacillaceae', 'Bacillus', 'B_subtilis'],\n",
    "        'P_aeruginosa': ['Bacteria', 'Proteobacteria', 'Gammaproteobacteria', 'Pseudomonadales', 'Pseudomonadaceae', 'Pseudomonas', 'P_aeruginosa'],\n",
    "        'S_aureus': ['Bacteria', 'Firmicutes', 'Bacilli', 'Bacillales', 'Staphylococcaceae', 'Staphylococcus', 'S_aureus'],\n",
    "        'M_tuberculosis': ['Bacteria', 'Actinobacteria', 'Actinomycetia', 'Mycobacteriales', 'Mycobacteriaceae', 'Mycobacterium', 'M_tuberculosis'],\n",
    "        # Add double underscore versions that might be created by regex\n",
    "        'E__coli': ['Bacteria', 'Proteobacteria', 'Gammaproteobacteria', 'Enterobacterales', 'Enterobacteriaceae', 'Escherichia', 'E__coli'],\n",
    "        'B__subtilis': ['Bacteria', 'Firmicutes', 'Bacilli', 'Bacillales', 'Bacillaceae', 'Bacillus', 'B__subtilis'],\n",
    "        'P__aeruginosa': ['Bacteria', 'Proteobacteria', 'Gammaproteobacteria', 'Pseudomonadales', 'Pseudomonadaceae', 'Pseudomonas', 'P__aeruginosa'],\n",
    "        'S__aureus': ['Bacteria', 'Firmicutes', 'Bacilli', 'Bacillales', 'Staphylococcaceae', 'Staphylococcus', 'S__aureus'],\n",
    "        'M__tuberculosis': ['Bacteria', 'Actinobacteria', 'Actinomycetia', 'Mycobacteriales', 'Mycobacteriaceae', 'Mycobacterium', 'M__tuberculosis']\n",
    "    }\n",
    "    return taxonomy\n",
    "\n",
    "def find_lca(taxonomy, species_list):\n",
    "    \"\"\"Find the lowest common ancestor of a list of species\"\"\"\n",
    "    if not species_list:\n",
    "        return \"Unknown\"\n",
    "    if len(species_list) == 1:\n",
    "        return species_list[0]  # Return the species if only one match\n",
    "    \n",
    "    # Get taxonomic paths for each species\n",
    "    paths = [taxonomy[species] for species in species_list]\n",
    "    \n",
    "    # Find the common prefix of all paths\n",
    "    common_path = []\n",
    "    for i in range(min(len(path) for path in paths)):\n",
    "        if len(set(path[i] for path in paths)) == 1:\n",
    "            common_path.append(paths[0][i])\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return common_path[-1] if common_path else \"Bacteria\"\n",
    "\n",
    "# ==================== BLAST Database Creation ====================\n",
    "\n",
    "def create_blast_db(fasta_files, temp_dir):\n",
    "    \"\"\"Create BLAST databases for each genome\"\"\"\n",
    "    db_paths = {}\n",
    "    \n",
    "    print(\"Creating BLAST databases...\")\n",
    "    for fasta_file in fasta_files:\n",
    "        base_name = os.path.basename(fasta_file)\n",
    "        species = base_name.split('.')[0]\n",
    "        \n",
    "        # Use prettier names for the species but without spaces\n",
    "        if species == \"e_coli\":\n",
    "            species_display = \"E. coli\"\n",
    "            species_id = \"E_coli\"\n",
    "        elif species == \"b_subtilis\":\n",
    "            species_display = \"B. subtilis\"\n",
    "            species_id = \"B_subtilis\"\n",
    "        elif species == \"p_aeruginosa\":\n",
    "            species_display = \"P. aeruginosa\"\n",
    "            species_id = \"P_aeruginosa\"\n",
    "        elif species == \"s_aureus\":\n",
    "            species_display = \"S. aureus\"\n",
    "            species_id = \"S_aureus\"\n",
    "        elif species == \"m_tuberculosis\":\n",
    "            species_display = \"M. tuberculosis\"\n",
    "            species_id = \"M_tuberculosis\"\n",
    "        else:\n",
    "            species_display = species\n",
    "            species_id = species\n",
    "        \n",
    "        # Create a copy of the FASTA file in the temp directory (avoid spaces in filenames)\n",
    "        db_path = os.path.join(temp_dir, species_id)\n",
    "        shutil.copy(fasta_file, f\"{db_path}.fna\")\n",
    "        \n",
    "        # Create the BLAST database\n",
    "        print(f\"Creating BLAST database for {species_display}...\")\n",
    "        cmd = [\"makeblastdb\", \"-in\", f\"{db_path}.fna\", \"-dbtype\", \"nucl\", \"-out\", db_path, \"-title\", species_id]\n",
    "        \n",
    "        try:\n",
    "            subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            print(f\"Successfully created database for {species_display}\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error creating BLAST database for {species_display}:\")\n",
    "            print(f\"Command: {' '.join(cmd)}\")\n",
    "            print(f\"Error: {e}\")\n",
    "            print(f\"stdout: {e.stdout.decode() if e.stdout else 'None'}\")\n",
    "            print(f\"stderr: {e.stderr.decode() if e.stderr else 'None'}\")\n",
    "            raise e\n",
    "        \n",
    "        db_paths[species_display] = db_path\n",
    "    \n",
    "    return db_paths\n",
    "\n",
    "# ==================== FASTQ Parsing Functions ====================\n",
    "\n",
    "def read_fastq_entries(fastq_file, max_reads=None):\n",
    "    \"\"\"Read entries from a FASTQ file and return a list of (header, sequence) tuples\"\"\"\n",
    "    reads = []\n",
    "    count = 0\n",
    "    \n",
    "    with open(fastq_file, 'r') as f:\n",
    "        while True:\n",
    "            if max_reads and count >= max_reads:\n",
    "                break\n",
    "                \n",
    "            # Read FASTQ entry\n",
    "            header = f.readline().strip()\n",
    "            if not header:\n",
    "                break  # End of file\n",
    "                \n",
    "            sequence = f.readline().strip()\n",
    "            f.readline()  # Skip '+' line\n",
    "            quality = f.readline().strip()\n",
    "            \n",
    "            if header and sequence and quality:\n",
    "                reads.append((header, sequence))\n",
    "                count += 1\n",
    "            else:\n",
    "                break  # Incomplete entry\n",
    "    \n",
    "    return reads\n",
    "\n",
    "def write_fasta_file(reads, output_fasta):\n",
    "    \"\"\"Write reads to a FASTA file\"\"\"\n",
    "    with open(output_fasta, 'w') as f:\n",
    "        for header, sequence in reads:\n",
    "            # Extract read ID without @ symbol\n",
    "            read_id = header[1:].split()[0]\n",
    "            f.write(f\">{read_id}\\n{sequence}\\n\")\n",
    "    \n",
    "    return len(reads)\n",
    "\n",
    "# ==================== BLAST Processing ====================\n",
    "\n",
    "def run_blast(query_fasta, db_path, output_file, read_length=100, max_mismatches=1, evalue=1e-5, num_threads=1, max_hits=5):\n",
    "    \"\"\"\n",
    "    Run blastn on the query against the specified database, allowing up to specified mismatches\n",
    "    \n",
    "    Parameters:\n",
    "    - read_length: Average length of reads (used to calculate percent identity)\n",
    "    - max_mismatches: Maximum number of mismatches to allow\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate percent identity threshold based on read length and max mismatches\n",
    "    # For example, allowing 1 mismatch in a 100bp read = 99% identity\n",
    "    percent_identity = 100 - (max_mismatches * 100 / read_length)\n",
    "    \n",
    "    # Use text output format instead of XML for easier parsing\n",
    "    cmd = [\n",
    "        \"blastn\",\n",
    "        \"-query\", query_fasta,\n",
    "        \"-db\", db_path,\n",
    "        \"-out\", output_file,\n",
    "        \"-outfmt\", \"6 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore\",\n",
    "        \"-evalue\", str(evalue),\n",
    "        \"-max_target_seqs\", str(max_hits),\n",
    "        \"-num_threads\", str(num_threads),\n",
    "        \"-perc_identity\", str(percent_identity),\n",
    "        \"-strand\", \"plus\"  # Only search the forward strand\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error running BLAST:\")\n",
    "        print(f\"Command: {' '.join(cmd)}\")\n",
    "        print(f\"Error: {e}\")\n",
    "        print(f\"stdout: {e.stdout.decode() if e.stdout else 'None'}\")\n",
    "        print(f\"stderr: {e.stderr.decode() if e.stderr else 'None'}\")\n",
    "        raise e\n",
    "    \n",
    "    return output_file\n",
    "\n",
    "def process_blast_results(results_file, query_ids, species):\n",
    "    \"\"\"Process BLAST tabular results and extract hits\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Initialize all queries with no hits\n",
    "    for query_id in query_ids:\n",
    "        results[query_id] = []\n",
    "    \n",
    "    # Parse results file\n",
    "    if os.path.getsize(results_file) > 0:  # Check if file is not empty\n",
    "        with open(results_file, 'r') as f:\n",
    "            for line in f:\n",
    "                fields = line.strip().split('\\t')\n",
    "                if len(fields) >= 12:  # Ensure we have all expected fields\n",
    "                    query_id = fields[0]\n",
    "                    mismatches = int(fields[4])  # The mismatch column\n",
    "                    \n",
    "                    # Only add if this is a new hit for this query\n",
    "                    if species not in results[query_id]:\n",
    "                        results[query_id].append(species)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def get_query_ids_from_fasta(fasta_file):\n",
    "    \"\"\"Extract all query IDs from a FASTA file\"\"\"\n",
    "    ids = []\n",
    "    with open(fasta_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('>'):\n",
    "                ids.append(line.strip()[1:])  # Remove '>' and newline\n",
    "    return ids\n",
    "\n",
    "# ==================== Read Classification ====================\n",
    "\n",
    "def classify_reads_with_blast(fastq_files, db_paths, taxonomy, max_reads_per_file=10000, temp_dir=None, num_threads=8, max_mismatches=1):\n",
    "    \"\"\"\n",
    "    Classify reads using BLAST with a specified mismatch tolerance\n",
    "    \n",
    "    Parameters:\n",
    "    - max_mismatches: Maximum number of mismatches to allow in alignments\n",
    "    \"\"\"\n",
    "    if temp_dir is None:\n",
    "        temp_dir = tempfile.mkdtemp()\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for set_name, file_paths in fastq_files.items():\n",
    "        print(f\"\\nProcessing {set_name}:\")\n",
    "        for path in file_paths:\n",
    "            print(f\"  - {path}\")\n",
    "        \n",
    "        start_process = time.time()\n",
    "        \n",
    "        # Read FASTQ files and combine reads\n",
    "        combined_reads = []\n",
    "        for fastq_file in file_paths:\n",
    "            reads = read_fastq_entries(fastq_file, max_reads=max_reads_per_file)\n",
    "            combined_reads.extend(reads)\n",
    "            print(f\"Read {len(reads)} reads from {fastq_file}\")\n",
    "        \n",
    "        print(f\"Total combined reads: {len(combined_reads)}\")\n",
    "        \n",
    "        # Calculate average read length for percent identity threshold\n",
    "        total_length = sum(len(seq) for _, seq in combined_reads)\n",
    "        avg_read_length = total_length / len(combined_reads) if combined_reads else 100\n",
    "        \n",
    "        print(f\"Average read length: {avg_read_length:.2f}bp\")\n",
    "        print(f\"Setting up BLAST to allow maximum {max_mismatches} mismatch(es)\")\n",
    "        percent_identity = 100 - (max_mismatches * 100 / avg_read_length)\n",
    "        print(f\"Using percent identity threshold of {percent_identity:.2f}%\")\n",
    "        \n",
    "        # Write combined reads to a FASTA file\n",
    "        combined_fasta = os.path.join(temp_dir, f\"{set_name}_combined.fasta\")\n",
    "        write_fasta_file(combined_reads, combined_fasta)\n",
    "        \n",
    "        # Get all query IDs\n",
    "        query_ids = get_query_ids_from_fasta(combined_fasta)\n",
    "        \n",
    "        # Initialize results dictionary\n",
    "        classification_results = {query_id: [] for query_id in query_ids}\n",
    "        \n",
    "        # Run BLAST against each reference genome\n",
    "        for species, db_path in db_paths.items():\n",
    "            print(f\"BLASTing against {species}...\")\n",
    "            blast_start = time.time()\n",
    "            \n",
    "            # Run BLAST search\n",
    "            output_file = os.path.join(temp_dir, f\"{set_name}_{re.sub(r'[. ]', '_', species)}_blast.out\")\n",
    "            run_blast(\n",
    "                combined_fasta, \n",
    "                db_path, \n",
    "                output_file, \n",
    "                read_length=avg_read_length,\n",
    "                max_mismatches=max_mismatches,\n",
    "                num_threads=num_threads\n",
    "            )\n",
    "            \n",
    "            # Process BLAST results\n",
    "            species_results = process_blast_results(output_file, query_ids, species)\n",
    "            \n",
    "            # Merge results\n",
    "            for query_id, hits in species_results.items():\n",
    "                classification_results[query_id].extend(hits)\n",
    "            \n",
    "            print(f\"BLAST against {species} completed in {time.time() - blast_start:.2f} seconds\")\n",
    "        \n",
    "        # Convert to the format used by the suffix array implementation\n",
    "        formatted_results = []\n",
    "        for query_id, matching_genomes in classification_results.items():\n",
    "            if not matching_genomes:\n",
    "                formatted_results.append((f\"@{query_id}\", \"Unclassified\", \"Unknown\"))\n",
    "            else:\n",
    "                # Use the original species names for taxonomy lookup\n",
    "                lca = find_lca(taxonomy, matching_genomes)\n",
    "                formatted_results.append((f\"@{query_id}\", matching_genomes, lca))\n",
    "        \n",
    "        all_results[set_name] = formatted_results\n",
    "        \n",
    "        # Calculate processing time\n",
    "        process_time = time.time() - start_process\n",
    "        \n",
    "        # Summarize and print results\n",
    "        summary = summarize_results(formatted_results)\n",
    "        print_summary(summary)\n",
    "        \n",
    "        print(f\"Processing time: {process_time:.2f} seconds\")\n",
    "        print(f\"Reads per second: {len(formatted_results) / process_time:.2f}\")\n",
    "        \n",
    "        print(f\"Memory after processing {set_name}: {track_memory()} MB\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def summarize_results(classification_results):\n",
    "    \"\"\"Summarize classification results\"\"\"\n",
    "    # Count matches per genome\n",
    "    single_matches = Counter()\n",
    "    multi_matches = Counter()\n",
    "    lca_classifications = Counter()\n",
    "    unclassified = 0\n",
    "    \n",
    "    for _, classification, lca in classification_results:\n",
    "        if classification == \"Unclassified\":\n",
    "            unclassified += 1\n",
    "        elif len(classification) == 1:\n",
    "            single_matches[classification[0]] += 1\n",
    "        else:\n",
    "            # For multi-matches, count each genome\n",
    "            for genome in classification:\n",
    "                multi_matches[genome] += 1\n",
    "            # Also count by LCA\n",
    "            lca_classifications[lca] += 1\n",
    "    \n",
    "    # Prepare summary\n",
    "    summary = {\n",
    "        \"total_reads\": len(classification_results),\n",
    "        \"single_matches\": dict(single_matches),\n",
    "        \"multi_matches\": dict(multi_matches),\n",
    "        \"lca_classifications\": dict(lca_classifications),\n",
    "        \"unclassified\": unclassified\n",
    "    }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def print_summary(summary):\n",
    "    \"\"\"Print a formatted summary of classification results\"\"\"\n",
    "    print(\"\\n====================== CLASSIFICATION SUMMARY ======================\")\n",
    "    print(f\"Total reads processed: {summary['total_reads']}\")\n",
    "    \n",
    "    print(\"\\nSingle Genome Matches:\")\n",
    "    for genome, count in summary['single_matches'].items():\n",
    "        percentage = (count / summary['total_reads']) * 100\n",
    "        print(f\"- {genome}: {count} reads ({percentage:.2f}%)\")\n",
    "    \n",
    "    if summary['lca_classifications']:\n",
    "        print(\"\\nMulti-Genome Matches (by LCA):\")\n",
    "        for lca, count in summary['lca_classifications'].items():\n",
    "            percentage = (count / summary['total_reads']) * 100\n",
    "            print(f\"- {lca}: {count} reads ({percentage:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nUnclassified: {summary['unclassified']} reads ({(summary['unclassified'] / summary['total_reads']) * 100:.2f}%)\")\n",
    "    print(\"===================================================================\")\n",
    "\n",
    "# ==================== Main Function ====================\n",
    "\n",
    "def main(num_threads=8, max_reads_per_file=10000, max_mismatches=1):\n",
    "    \"\"\"\n",
    "    Main function to run BLAST-based classification allowing up to the specified number of mismatches\n",
    "    \n",
    "    Parameters:\n",
    "    - num_threads: Number of threads to use for BLAST\n",
    "    - max_reads_per_file: Maximum number of reads to process from each file\n",
    "    - max_mismatches: Maximum number of mismatches to allow in alignments\n",
    "    \"\"\"\n",
    "    # Check if BLAST+ is installed\n",
    "    if not check_blast_installation():\n",
    "        print(\"Please install BLAST+ tools before running this script\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Starting BLAST-based metagenomic classification with {num_threads} threads\")\n",
    "    print(f\"Allowing up to {max_mismatches} mismatch(es) per read\")\n",
    "    \n",
    "    # Define file paths\n",
    "    genome_files = [\n",
    "        \"b_subtilis.fna\",\n",
    "        \"e_coli.fna\",\n",
    "        \"m_tuberculosis.fna\",\n",
    "        \"p_aeruginosa.fna\",\n",
    "        \"s_aureus.fna\"\n",
    "    ]\n",
    "    \n",
    "    # Define combined read file sets\n",
    "    read_file_sets = {\n",
    "        \"error_free_reads\": [\n",
    "            \"simulated_reads_no_errors_10k_R1.fastq\",\n",
    "            \"simulated_reads_no_errors_10k_R2.fastq\"\n",
    "        ],\n",
    "        \"with_errors_reads\": [\n",
    "            \"simulated_reads_miseq_10k_R1.fastq\", \n",
    "            \"simulated_reads_miseq_10k_R2.fastq\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Create a temporary directory\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    print(f\"Using temporary directory: {temp_dir}\")\n",
    "    \n",
    "    try:\n",
    "        # Create BLAST databases\n",
    "        db_paths = create_blast_db(genome_files, temp_dir)\n",
    "        taxonomy = generate_taxonomic_tree()\n",
    "        \n",
    "        print(f\"Memory after creating BLAST databases: {track_memory()} MB\")\n",
    "        \n",
    "        # Classify reads\n",
    "        all_results = classify_reads_with_blast(\n",
    "            read_file_sets, \n",
    "            db_paths, \n",
    "            taxonomy, \n",
    "            max_reads_per_file=max_reads_per_file,\n",
    "            temp_dir=temp_dir,\n",
    "            num_threads=num_threads,\n",
    "            max_mismatches=max_mismatches\n",
    "        )\n",
    "        \n",
    "        # Performance report\n",
    "        total_time = time.time() - start_time\n",
    "        peak_memory = max(memory_usage)\n",
    "        \n",
    "        print(\"\\n====================== PERFORMANCE SUMMARY ======================\")\n",
    "        print(f\"Total execution time: {total_time:.2f} seconds\")\n",
    "        print(f\"Peak memory usage: {peak_memory:.2f} MB\")\n",
    "        print(f\"Maximum mismatches allowed: {max_mismatches}\")\n",
    "        \n",
    "        # Calculate processing statistics\n",
    "        total_reads = sum(len(results) for results in all_results.values())\n",
    "        reads_per_second = total_reads / total_time\n",
    "        \n",
    "        print(f\"\\nTotal reads processed: {total_reads}\")\n",
    "        print(f\"Overall processing speed: {reads_per_second:.2f} reads per second\")\n",
    "        print(\"================================================================\")\n",
    "        \n",
    "    finally:\n",
    "        # Clean up temporary directory\n",
    "        print(f\"Cleaning up temporary directory: {temp_dir}\")\n",
    "        shutil.rmtree(temp_dir)\n",
    "\n",
    "# Run the main function with the specified number of threads\n",
    "if __name__ == \"__main__\":\n",
    "    # Use 8 threads for BLAST (or adjust based on your system)\n",
    "    NUM_THREADS = 8\n",
    "    \n",
    "    # Maximum number of mismatches to allow (Task 1.3)\n",
    "    MAX_MISMATCHES = 0\n",
    "    \n",
    "    main(num_threads=NUM_THREADS, max_mismatches=MAX_MISMATCHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b5ca51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with memory: 99.5 MB\n",
      "Starting k-mer based metagenomic classification with k=31 and 1 processes\n",
      "Using minimum k-mer match fraction: 1.0\n",
      "Reading reference genomes...\n",
      "Reading genome for B. subtilis...\n",
      "Read 4215606 bases for B. subtilis\n",
      "Reading genome for E. coli...\n",
      "Read 4641652 bases for E. coli\n",
      "Reading genome for M. tuberculosis...\n",
      "Read 4411532 bases for M. tuberculosis\n",
      "Reading genome for P. aeruginosa...\n",
      "Read 6264404 bases for P. aeruginosa\n",
      "Reading genome for S. aureus...\n",
      "Read 2821361 bases for S. aureus\n",
      "Memory after loading reference genomes: 121.5 MB\n",
      "Building 31-mer indices for all genomes...\n",
      "Processing B. subtilis...\n",
      "Extracting 31-mers from B. subtilis...\n",
      "Found 4215576 31-mers (4170526 unique) in B. subtilis\n",
      "K-mer extraction for B. subtilis completed in 12.40 seconds\n",
      "Processing E. coli...\n",
      "Extracting 31-mers from E. coli...\n",
      "Found 4641622 31-mers (4570839 unique) in E. coli\n",
      "K-mer extraction for E. coli completed in 12.65 seconds\n",
      "Processing M. tuberculosis...\n",
      "Extracting 31-mers from M. tuberculosis...\n",
      "Found 4411502 31-mers (4358047 unique) in M. tuberculosis\n",
      "K-mer extraction for M. tuberculosis completed in 12.12 seconds\n",
      "Processing P. aeruginosa...\n",
      "Extracting 31-mers from P. aeruginosa...\n",
      "Found 6264374 31-mers (6222731 unique) in P. aeruginosa\n",
      "K-mer extraction for P. aeruginosa completed in 17.31 seconds\n",
      "Processing S. aureus...\n",
      "Extracting 31-mers from S. aureus...\n",
      "Found 2821300 31-mers (2787880 unique) in S. aureus\n",
      "K-mer extraction for S. aureus completed in 7.07 seconds\n",
      "Merging k-mer indices...\n",
      "Merged k-mer index contains 22104664 unique k-mers\n",
      "Index merging completed in 16.94 seconds\n",
      "K-mer index built in 81.68 seconds\n",
      "Memory after building k-mer index: 8620.96875 MB\n",
      "\n",
      "====================== K-MER INDEX STATISTICS ======================\n",
      "Total unique k-mers in index: 22,104,664\n",
      "Theoretical k-mer space (4^k): 4,611,686,018,427,387,904\n",
      "Space coverage: 0.0000000005%\n",
      "\n",
      "K-mers unique to each genome:\n",
      "- E. coli: 4,568,108 unique k-mers\n",
      "- B. subtilis: 4,168,371 unique k-mers\n",
      "- P. aeruginosa: 6,221,034 unique k-mers\n",
      "- S. aureus: 2,784,621 unique k-mers\n",
      "- M. tuberculosis: 4,357,767 unique k-mers\n",
      "\n",
      "K-mers shared between genomes: 4,763\n",
      "==================================================================\n",
      "\n",
      "Processing error_free_reads:\n",
      "  - simulated_reads_no_errors_10k_R1.fastq\n",
      "  - simulated_reads_no_errors_10k_R2.fastq\n",
      "Read 5000 reads from simulated_reads_no_errors_10k_R1.fastq\n",
      "Read 5000 reads from simulated_reads_no_errors_10k_R2.fastq\n",
      "Total combined reads: 10000\n",
      "\n",
      "====================== CLASSIFICATION SUMMARY ======================\n",
      "Total reads processed: 10000\n",
      "\n",
      "Single Genome Matches:\n",
      "- E. coli: 3045 reads (30.45%)\n",
      "- P. aeruginosa: 503 reads (5.03%)\n",
      "- B. subtilis: 505 reads (5.05%)\n",
      "- S. aureus: 503 reads (5.03%)\n",
      "- M. tuberculosis: 506 reads (5.06%)\n",
      "\n",
      "Multi-Genome Matches (by LCA):\n",
      "- Bacteria: 9 reads (0.09%)\n",
      "\n",
      "Unclassified: 4929 reads (49.29%)\n",
      "===================================================================\n",
      "Processing time: 35.68 seconds\n",
      "Reads per second: 280.29\n",
      "Memory after processing error_free_reads: 8662.28125 MB\n",
      "\n",
      "Processing with_errors_reads:\n",
      "  - simulated_reads_miseq_10k_R1.fastq\n",
      "  - simulated_reads_miseq_10k_R2.fastq\n",
      "Read 5000 reads from simulated_reads_miseq_10k_R1.fastq\n",
      "Read 5000 reads from simulated_reads_miseq_10k_R2.fastq\n",
      "Total combined reads: 10000\n",
      "\n",
      "====================== CLASSIFICATION SUMMARY ======================\n",
      "Total reads processed: 10000\n",
      "\n",
      "Single Genome Matches:\n",
      "- E. coli: 215 reads (2.15%)\n",
      "- P. aeruginosa: 35 reads (0.35%)\n",
      "- B. subtilis: 48 reads (0.48%)\n",
      "- S. aureus: 36 reads (0.36%)\n",
      "- M. tuberculosis: 42 reads (0.42%)\n",
      "\n",
      "Multi-Genome Matches (by LCA):\n",
      "- Bacteria: 1 reads (0.01%)\n",
      "\n",
      "Unclassified: 9623 reads (96.23%)\n",
      "===================================================================\n",
      "Processing time: 37.06 seconds\n",
      "Reads per second: 269.81\n",
      "Memory after processing with_errors_reads: 8685.2109375 MB\n",
      "\n",
      "====================== PERFORMANCE SUMMARY ======================\n",
      "Total execution time: 154.50 seconds\n",
      "K-mer index building time: 81.68 seconds\n",
      "Peak memory usage: 8685.21 MB\n",
      "\n",
      "Total reads processed: 20000\n",
      "Overall processing speed: 274.68 reads per second\n",
      "================================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "from collections import defaultdict, Counter\n",
    "import psutil\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Performance tracking\n",
    "start_time = time.time()\n",
    "memory_usage = []\n",
    "\n",
    "def track_memory():\n",
    "    \"\"\"Track current memory usage\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_usage.append(process.memory_info().rss / 1024 / 1024)  # MB\n",
    "    return memory_usage[-1]\n",
    "\n",
    "print(f\"Starting with memory: {track_memory()} MB\")\n",
    "\n",
    "# ==================== K-mer Encoding Functions ====================\n",
    "\n",
    "def encode_kmer(kmer):\n",
    "    \"\"\"\n",
    "    Encode a k-mer string into a 64-bit integer using 2-bit encoding\n",
    "    A=00, C=01, G=10, T=11\n",
    "    \"\"\"\n",
    "    encoding = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "    result = 0\n",
    "    for base in kmer:\n",
    "        result = (result << 2) | encoding.get(base, 0)\n",
    "    return result\n",
    "\n",
    "def decode_kmer(encoded, k):\n",
    "    \"\"\"\n",
    "    Decode a 2-bit encoded k-mer back to a string\n",
    "    \"\"\"\n",
    "    decoding = {0: 'A', 1: 'C', 2: 'G', 3: 'T'}\n",
    "    result = []\n",
    "    for i in range(k):\n",
    "        # Extract 2 bits from the end\n",
    "        bits = (encoded >> (2*i)) & 3\n",
    "        result.append(decoding[bits])\n",
    "    return ''.join(reversed(result))\n",
    "\n",
    "# ==================== FASTA/FASTQ Parsing Functions ====================\n",
    "\n",
    "def read_fasta_file(filename):\n",
    "    \"\"\"Read a FASTA file and return the sequence\"\"\"\n",
    "    sequence = \"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        # Skip header line\n",
    "        header = f.readline().strip()\n",
    "        # Read sequence lines\n",
    "        for line in f:\n",
    "            if line.startswith('>'):\n",
    "                continue  # Skip additional headers\n",
    "            sequence += line.strip()\n",
    "    return sequence\n",
    "\n",
    "def read_fastq_file(filename, max_reads=None):\n",
    "    \"\"\"Read a FASTQ file and return reads as a list of (header, sequence) tuples\"\"\"\n",
    "    reads = []\n",
    "    count = 0\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        while True:\n",
    "            if max_reads and count >= max_reads:\n",
    "                break\n",
    "                \n",
    "            header = f.readline().strip()\n",
    "            if not header:\n",
    "                break  # End of file\n",
    "                \n",
    "            sequence = f.readline().strip()\n",
    "            f.readline()  # Skip \"+\" line\n",
    "            quality = f.readline().strip()\n",
    "            \n",
    "            if header and sequence and quality:\n",
    "                reads.append((header, sequence))\n",
    "                count += 1\n",
    "            else:\n",
    "                break  # Incomplete entry\n",
    "    \n",
    "    return reads\n",
    "\n",
    "def read_genome_files(filenames):\n",
    "    \"\"\"Read all genome files and return a dictionary of sequences\"\"\"\n",
    "    print(\"Reading reference genomes...\")\n",
    "    genomes = {}\n",
    "    \n",
    "    for filename in filenames:\n",
    "        base_name = os.path.basename(filename)\n",
    "        # Extract species name from filename\n",
    "        species = base_name.split('.')[0]\n",
    "        \n",
    "        # Use prettier names for the species\n",
    "        if species == \"e_coli\":\n",
    "            species = \"E. coli\"\n",
    "        elif species == \"b_subtilis\":\n",
    "            species = \"B. subtilis\"\n",
    "        elif species == \"p_aeruginosa\":\n",
    "            species = \"P. aeruginosa\"\n",
    "        elif species == \"s_aureus\":\n",
    "            species = \"S. aureus\"\n",
    "        elif species == \"m_tuberculosis\":\n",
    "            species = \"M. tuberculosis\"\n",
    "        \n",
    "        print(f\"Reading genome for {species}...\")\n",
    "        sequence = read_fasta_file(filename)\n",
    "        genomes[species] = sequence\n",
    "        print(f\"Read {len(sequence)} bases for {species}\")\n",
    "    \n",
    "    return genomes\n",
    "\n",
    "def generate_taxonomic_tree():\n",
    "    \"\"\"Generate a taxonomic tree for the 5 reference genomes\"\"\"\n",
    "    taxonomy = {\n",
    "        'E. coli': ['Bacteria', 'Proteobacteria', 'Gammaproteobacteria', 'Enterobacterales', 'Enterobacteriaceae', 'Escherichia', 'E. coli'],\n",
    "        'B. subtilis': ['Bacteria', 'Firmicutes', 'Bacilli', 'Bacillales', 'Bacillaceae', 'Bacillus', 'B. subtilis'],\n",
    "        'P. aeruginosa': ['Bacteria', 'Proteobacteria', 'Gammaproteobacteria', 'Pseudomonadales', 'Pseudomonadaceae', 'Pseudomonas', 'P. aeruginosa'],\n",
    "        'S. aureus': ['Bacteria', 'Firmicutes', 'Bacilli', 'Bacillales', 'Staphylococcaceae', 'Staphylococcus', 'S. aureus'],\n",
    "        'M. tuberculosis': ['Bacteria', 'Actinobacteria', 'Actinomycetia', 'Mycobacteriales', 'Mycobacteriaceae', 'Mycobacterium', 'M. tuberculosis']\n",
    "    }\n",
    "    return taxonomy\n",
    "\n",
    "def find_lca(taxonomy, species_list):\n",
    "    \"\"\"Find the lowest common ancestor of a list of species\"\"\"\n",
    "    if not species_list:\n",
    "        return \"Unknown\"\n",
    "    if len(species_list) == 1:\n",
    "        return species_list[0]  # Return the species if only one match\n",
    "    \n",
    "    # Get taxonomic paths for each species\n",
    "    paths = [taxonomy[species] for species in species_list]\n",
    "    \n",
    "    # Find the common prefix of all paths\n",
    "    common_path = []\n",
    "    for i in range(min(len(path) for path in paths)):\n",
    "        if len(set(path[i] for path in paths)) == 1:\n",
    "            common_path.append(paths[0][i])\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return common_path[-1] if common_path else \"Bacteria\"\n",
    "\n",
    "# ==================== K-mer Index Implementation ====================\n",
    "\n",
    "def extract_kmers_from_chunk(args):\n",
    "    \"\"\"Extract encoded k-mers from a chunk of a genome sequence\"\"\"\n",
    "    chunk, start_idx, k = args\n",
    "    kmers = []\n",
    "    \n",
    "    for i in range(len(chunk) - k + 1):\n",
    "        kmer = chunk[i:i+k]\n",
    "        if 'N' not in kmer:  # Skip k-mers with ambiguous bases\n",
    "            encoded_kmer = encode_kmer(kmer)\n",
    "            kmers.append((encoded_kmer, start_idx + i))\n",
    "            \n",
    "    return kmers\n",
    "\n",
    "def extract_kmers_parallel(sequence, k=31, chunk_size=1000000, num_processes=8):\n",
    "    \"\"\"Extract k-mers from a sequence in parallel\"\"\"\n",
    "    chunks = []\n",
    "    for i in range(0, len(sequence), chunk_size):\n",
    "        chunk = sequence[i:i+chunk_size+k-1]  # Add k-1 to ensure we catch kmers at chunk boundaries\n",
    "        if len(chunk) >= k:\n",
    "            chunks.append((chunk, i, k))\n",
    "    \n",
    "    with mp.Pool(processes=num_processes) as pool:\n",
    "        results = pool.map(extract_kmers_from_chunk, chunks)\n",
    "    \n",
    "    # Combine results from all chunks\n",
    "    all_kmers = []\n",
    "    for result in results:\n",
    "        all_kmers.extend(result)\n",
    "        \n",
    "    return all_kmers\n",
    "\n",
    "def build_kmer_index_for_genome(genome_name, sequence, k=31, num_processes=8):\n",
    "    \"\"\"Build a k-mer index for a single genome\"\"\"\n",
    "    print(f\"Extracting {k}-mers from {genome_name}...\")\n",
    "    start = time.time()\n",
    "    \n",
    "    # Extract k-mers in parallel\n",
    "    kmers = extract_kmers_parallel(sequence, k, num_processes=num_processes)\n",
    "    \n",
    "    # Build index - using standard dict\n",
    "    genome_index = {}\n",
    "    for encoded_kmer, position in kmers:\n",
    "        if encoded_kmer not in genome_index:\n",
    "            genome_index[encoded_kmer] = []\n",
    "        genome_index[encoded_kmer].append(position)\n",
    "    \n",
    "    print(f\"Found {len(kmers)} {k}-mers ({len(genome_index)} unique) in {genome_name}\")\n",
    "    print(f\"K-mer extraction for {genome_name} completed in {time.time() - start:.2f} seconds\")\n",
    "    \n",
    "    return genome_index\n",
    "\n",
    "def merge_kmer_indices(genome_indices):\n",
    "    \"\"\"Merge k-mer indices from multiple genomes using regular dict\"\"\"\n",
    "    # Use a regular dict instead of defaultdict with lambda\n",
    "    combined_index = {}\n",
    "    \n",
    "    for genome_name, genome_index in genome_indices.items():\n",
    "        for encoded_kmer, positions in genome_index.items():\n",
    "            if encoded_kmer not in combined_index:\n",
    "                combined_index[encoded_kmer] = {}\n",
    "            combined_index[encoded_kmer][genome_name] = positions\n",
    "    \n",
    "    return combined_index\n",
    "\n",
    "def build_kmer_index(genomes, k=31, num_processes=8):\n",
    "    \"\"\"Build a k-mer index for all genomes\"\"\"\n",
    "    genome_indices = {}\n",
    "    \n",
    "    print(f\"Building {k}-mer indices for all genomes...\")\n",
    "    for name, sequence in genomes.items():\n",
    "        print(f\"Processing {name}...\")\n",
    "        genome_indices[name] = build_kmer_index_for_genome(name, sequence, k, num_processes)\n",
    "    \n",
    "    # Merge individual genome indices\n",
    "    print(\"Merging k-mer indices...\")\n",
    "    start = time.time()\n",
    "    combined_index = merge_kmer_indices(genome_indices)\n",
    "    print(f\"Merged k-mer index contains {len(combined_index)} unique k-mers\")\n",
    "    print(f\"Index merging completed in {time.time() - start:.2f} seconds\")\n",
    "    \n",
    "    # Calculate some statistics\n",
    "    kmer_stats = analyze_kmer_index(combined_index, k)\n",
    "    \n",
    "    return combined_index, kmer_stats\n",
    "\n",
    "def analyze_kmer_index(index, k=31):\n",
    "    \"\"\"Analyze k-mer index statistics\"\"\"\n",
    "    total_kmers = len(index)\n",
    "    theoretical_kmers = 4**k\n",
    "    \n",
    "    # Count k-mers unique to each genome vs shared\n",
    "    unique_to_genome = {}  # Using regular dict\n",
    "    for genome in ['E. coli', 'B. subtilis', 'P. aeruginosa', 'S. aureus', 'M. tuberculosis']:\n",
    "        unique_to_genome[genome] = 0\n",
    "    \n",
    "    shared_kmers = 0\n",
    "    \n",
    "    for encoded_kmer, genome_dict in index.items():\n",
    "        if len(genome_dict) == 1:\n",
    "            # Unique to one genome\n",
    "            genome = next(iter(genome_dict.keys()))\n",
    "            unique_to_genome[genome] += 1\n",
    "        else:\n",
    "            # Shared between multiple genomes\n",
    "            shared_kmers += 1\n",
    "    \n",
    "    # Build statistics dictionary\n",
    "    stats = {\n",
    "        \"total_unique_kmers\": total_kmers,\n",
    "        \"theoretical_kmers\": theoretical_kmers,\n",
    "        \"coverage_percent\": (total_kmers / theoretical_kmers) * 100,\n",
    "        \"shared_kmers\": shared_kmers,\n",
    "        \"unique_to_genome\": unique_to_genome\n",
    "    }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def print_kmer_stats(stats):\n",
    "    \"\"\"Print k-mer index statistics\"\"\"\n",
    "    print(\"\\n====================== K-MER INDEX STATISTICS ======================\")\n",
    "    print(f\"Total unique k-mers in index: {stats['total_unique_kmers']:,}\")\n",
    "    print(f\"Theoretical k-mer space (4^k): {stats['theoretical_kmers']:,}\")\n",
    "    print(f\"Space coverage: {stats['coverage_percent']:.10f}%\")\n",
    "    \n",
    "    print(\"\\nK-mers unique to each genome:\")\n",
    "    for genome, count in stats['unique_to_genome'].items():\n",
    "        print(f\"- {genome}: {count:,} unique k-mers\")\n",
    "    \n",
    "    print(f\"\\nK-mers shared between genomes: {stats['shared_kmers']:,}\")\n",
    "    print(\"==================================================================\")\n",
    "\n",
    "# ==================== K-mer Based Classification ====================\n",
    "\n",
    "def extract_read_kmers(sequence, k=31):\n",
    "    \"\"\"Extract encoded k-mers from a read sequence\"\"\"\n",
    "    encoded_kmers = []\n",
    "    for i in range(len(sequence) - k + 1):\n",
    "        kmer = sequence[i:i+k]\n",
    "        if 'N' not in kmer:  # Skip k-mers with ambiguous bases\n",
    "            encoded_kmers.append(encode_kmer(kmer))\n",
    "    return encoded_kmers\n",
    "\n",
    "def process_read_chunk_with_kmers(read_chunk, kmer_index, taxonomy, k=31, min_kmer_fraction=0.1):\n",
    "    \"\"\"Process a chunk of reads against the k-mer index\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for header, sequence in read_chunk:\n",
    "        # Skip very short reads\n",
    "        if len(sequence) < k:\n",
    "            results.append((header, \"Unclassified\", \"Unknown\"))\n",
    "            continue\n",
    "        \n",
    "        # Extract k-mers from the read\n",
    "        read_kmers = extract_read_kmers(sequence, k)\n",
    "        if not read_kmers:\n",
    "            results.append((header, \"Unclassified\", \"Unknown\"))\n",
    "            continue\n",
    "        \n",
    "        # Count matches to each genome\n",
    "        genome_matches = {}  # Using regular dict instead of defaultdict\n",
    "        for genome in ['E. coli', 'B. subtilis', 'P. aeruginosa', 'S. aureus', 'M. tuberculosis']:\n",
    "            genome_matches[genome] = 0\n",
    "            \n",
    "        total_kmers = len(read_kmers)\n",
    "        matched_kmers = 0\n",
    "        \n",
    "        for encoded_kmer in read_kmers:\n",
    "            if encoded_kmer in kmer_index:\n",
    "                matched_kmers += 1\n",
    "                for genome in kmer_index[encoded_kmer]:\n",
    "                    genome_matches[genome] += 1\n",
    "        \n",
    "        # Determine best matching genome(s)\n",
    "        max_matches = max(genome_matches.values())\n",
    "        if max_matches == 0:\n",
    "            results.append((header, \"Unclassified\", \"Unknown\"))\n",
    "            continue\n",
    "            \n",
    "        match_fraction = max_matches / total_kmers if total_kmers > 0 else 0\n",
    "        \n",
    "        # Classify based on matches and threshold\n",
    "        if match_fraction < min_kmer_fraction:\n",
    "            results.append((header, \"Unclassified\", \"Unknown\"))\n",
    "        else:\n",
    "            best_genomes = [g for g, c in genome_matches.items() if c == max_matches]\n",
    "            if len(best_genomes) == 1:\n",
    "                results.append((header, best_genomes, best_genomes[0]))\n",
    "            else:\n",
    "                lca = find_lca(taxonomy, best_genomes)\n",
    "                results.append((header, best_genomes, lca))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def distribute_reads(reads, num_chunks):\n",
    "    \"\"\"Distribute reads evenly across chunks\"\"\"\n",
    "    chunk_size = max(1, len(reads) // num_chunks)\n",
    "    return [reads[i:i + chunk_size] for i in range(0, len(reads), chunk_size)]\n",
    "\n",
    "def classify_reads_parallel(reads, kmer_index, taxonomy, k=31, num_processes=8, min_kmer_fraction=0.1):\n",
    "    \"\"\"Classify reads in parallel using multiple processes\"\"\"\n",
    "    # Split reads into chunks for parallelization\n",
    "    read_chunks = distribute_reads(reads, num_processes)\n",
    "    \n",
    "    # Process each chunk in parallel\n",
    "    with mp.Pool(processes=num_processes) as pool:\n",
    "        process_func = partial(process_read_chunk_with_kmers, \n",
    "                               kmer_index=kmer_index, \n",
    "                               taxonomy=taxonomy, \n",
    "                               k=k, \n",
    "                               min_kmer_fraction=min_kmer_fraction)\n",
    "        chunk_results = pool.map(process_func, read_chunks)\n",
    "    \n",
    "    # Combine results from all chunks\n",
    "    all_results = []\n",
    "    for result in chunk_results:\n",
    "        all_results.extend(result)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def summarize_results(classification_results):\n",
    "    \"\"\"Summarize classification results\"\"\"\n",
    "    # Count matches per genome\n",
    "    single_matches = Counter()\n",
    "    multi_matches = Counter()\n",
    "    lca_classifications = Counter()\n",
    "    unclassified = 0\n",
    "    \n",
    "    for _, classification, lca in classification_results:\n",
    "        if classification == \"Unclassified\":\n",
    "            unclassified += 1\n",
    "        elif len(classification) == 1:\n",
    "            single_matches[classification[0]] += 1\n",
    "        else:\n",
    "            # For multi-matches, count each genome\n",
    "            for genome in classification:\n",
    "                multi_matches[genome] += 1\n",
    "            # Also count by LCA\n",
    "            lca_classifications[lca] += 1\n",
    "    \n",
    "    # Prepare summary\n",
    "    summary = {\n",
    "        \"total_reads\": len(classification_results),\n",
    "        \"single_matches\": dict(single_matches),\n",
    "        \"multi_matches\": dict(multi_matches),\n",
    "        \"lca_classifications\": dict(lca_classifications),\n",
    "        \"unclassified\": unclassified\n",
    "    }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def print_summary(summary):\n",
    "    \"\"\"Print a formatted summary of classification results\"\"\"\n",
    "    print(\"\\n====================== CLASSIFICATION SUMMARY ======================\")\n",
    "    print(f\"Total reads processed: {summary['total_reads']}\")\n",
    "    \n",
    "    print(\"\\nSingle Genome Matches:\")\n",
    "    for genome, count in summary['single_matches'].items():\n",
    "        percentage = (count / summary['total_reads']) * 100\n",
    "        print(f\"- {genome}: {count} reads ({percentage:.2f}%)\")\n",
    "    \n",
    "    if summary['lca_classifications']:\n",
    "        print(\"\\nMulti-Genome Matches (by LCA):\")\n",
    "        for lca, count in summary['lca_classifications'].items():\n",
    "            percentage = (count / summary['total_reads']) * 100\n",
    "            print(f\"- {lca}: {count} reads ({percentage:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nUnclassified: {summary['unclassified']} reads ({(summary['unclassified'] / summary['total_reads']) * 100:.2f}%)\")\n",
    "    print(\"===================================================================\")\n",
    "\n",
    "# ==================== Main Function ====================\n",
    "\n",
    "def main(k=31, num_processes=8, max_reads_per_file=10000, min_kmer_fraction=1.0):\n",
    "    \"\"\"Main function to run the entire classification pipeline\"\"\"\n",
    "    print(f\"Starting k-mer based metagenomic classification with k={k} and {num_processes} processes\")\n",
    "    print(f\"Using minimum k-mer match fraction: {min_kmer_fraction}\")\n",
    "    \n",
    "    # Define file paths\n",
    "    genome_files = [\n",
    "        \"b_subtilis.fna\",\n",
    "        \"e_coli.fna\",\n",
    "        \"m_tuberculosis.fna\",\n",
    "        \"p_aeruginosa.fna\",\n",
    "        \"s_aureus.fna\"\n",
    "    ]\n",
    "    \n",
    "    # Define combined read file sets\n",
    "    read_file_sets = {\n",
    "        \"error_free_reads\": [\n",
    "            \"simulated_reads_no_errors_10k_R1.fastq\",\n",
    "            \"simulated_reads_no_errors_10k_R2.fastq\"\n",
    "        ],\n",
    "        \"with_errors_reads\": [\n",
    "            \"simulated_reads_miseq_10k_R1.fastq\", \n",
    "            \"simulated_reads_miseq_10k_R2.fastq\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Load reference genomes\n",
    "    genomes = read_genome_files(genome_files)\n",
    "    taxonomy = generate_taxonomic_tree()\n",
    "    \n",
    "    print(f\"Memory after loading reference genomes: {track_memory()} MB\")\n",
    "    \n",
    "    # Build k-mer index\n",
    "    index_start = time.time()\n",
    "    kmer_index, kmer_stats = build_kmer_index(genomes, k=k, num_processes=num_processes)\n",
    "    index_time = time.time() - index_start\n",
    "    \n",
    "    print(f\"K-mer index built in {index_time:.2f} seconds\")\n",
    "    print(f\"Memory after building k-mer index: {track_memory()} MB\")\n",
    "    \n",
    "    # Print k-mer statistics\n",
    "    print_kmer_stats(kmer_stats)\n",
    "    \n",
    "    # Process each combined read file set\n",
    "    all_results = {}\n",
    "    \n",
    "    for set_name, file_paths in read_file_sets.items():\n",
    "        print(f\"\\nProcessing {set_name}:\")\n",
    "        for path in file_paths:\n",
    "            print(f\"  - {path}\")\n",
    "        \n",
    "        start_process = time.time()\n",
    "        \n",
    "        # Read and combine FASTQ files\n",
    "        combined_reads = []\n",
    "        for file_path in file_paths:\n",
    "            reads = read_fastq_file(file_path, max_reads=max_reads_per_file)\n",
    "            combined_reads.extend(reads)\n",
    "            print(f\"Read {len(reads)} reads from {file_path}\")\n",
    "            \n",
    "        print(f\"Total combined reads: {len(combined_reads)}\")\n",
    "        \n",
    "        # Process reads\n",
    "        results = classify_reads_parallel(\n",
    "            combined_reads, \n",
    "            kmer_index, \n",
    "            taxonomy, \n",
    "            k=k,\n",
    "            num_processes=num_processes,\n",
    "            min_kmer_fraction=min_kmer_fraction\n",
    "        )\n",
    "        \n",
    "        process_time = time.time() - start_process\n",
    "        all_results[set_name] = results\n",
    "        \n",
    "        # Summarize and print results\n",
    "        summary = summarize_results(results)\n",
    "        print_summary(summary)\n",
    "        \n",
    "        print(f\"Processing time: {process_time:.2f} seconds\")\n",
    "        print(f\"Reads per second: {len(results) / process_time:.2f}\")\n",
    "        \n",
    "        print(f\"Memory after processing {set_name}: {track_memory()} MB\")\n",
    "    \n",
    "    # Performance report\n",
    "    total_time = time.time() - start_time\n",
    "    peak_memory = max(memory_usage)\n",
    "    \n",
    "    print(\"\\n====================== PERFORMANCE SUMMARY ======================\")\n",
    "    print(f\"Total execution time: {total_time:.2f} seconds\")\n",
    "    print(f\"K-mer index building time: {index_time:.2f} seconds\")\n",
    "    print(f\"Peak memory usage: {peak_memory:.2f} MB\")\n",
    "    \n",
    "    # Calculate processing statistics\n",
    "    total_reads = sum(len(results) for results in all_results.values())\n",
    "    reads_per_second = total_reads / (total_time - index_time)\n",
    "    \n",
    "    print(f\"\\nTotal reads processed: {total_reads}\")\n",
    "    print(f\"Overall processing speed: {reads_per_second:.2f} reads per second\")\n",
    "    print(\"================================================================\")\n",
    "\n",
    "# Run the main function with the specified parameters\n",
    "if __name__ == \"__main__\":\n",
    "    # Set parameters\n",
    "    K = 31  # k-mer length (as specified in the assignment)\n",
    "    NUM_PROCESSES = 1  # Number of processes to use\n",
    "    MIN_KMER_FRACTION = 1.0  # For exact matching, require 100% of k-mers to match\n",
    "    \n",
    "    main(k=K, num_processes=NUM_PROCESSES, min_kmer_fraction=MIN_KMER_FRACTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aba0c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with memory: 99.0 MB\n",
      "Starting minimizer-based metagenomic classification with k=31, w=10, and 1 processes\n",
      "Using minimum minimizer match fraction: 1.0\n",
      "Reading reference genomes...\n",
      "Reading genome for B. subtilis...\n",
      "Read 4215606 bases for B. subtilis\n",
      "Reading genome for E. coli...\n",
      "Read 4641652 bases for E. coli\n",
      "Reading genome for M. tuberculosis...\n",
      "Read 4411532 bases for M. tuberculosis\n",
      "Reading genome for P. aeruginosa...\n",
      "Read 6264404 bases for P. aeruginosa\n",
      "Reading genome for S. aureus...\n",
      "Read 2821361 bases for S. aureus\n",
      "Memory after loading reference genomes: 121.0 MB\n",
      "Building minimizer index (k=31, w=10) for all genomes...\n",
      "Processing B. subtilis...\n",
      "Extracting minimizers (k=31, w=10) from B. subtilis...\n",
      "Found 890,678 unique minimizers out of 4,215,576 possible k-mers\n",
      "Reduction ratio: 4.73x\n",
      "Minimizer extraction for B. subtilis completed in 13.88 seconds\n",
      "Processing E. coli...\n",
      "Extracting minimizers (k=31, w=10) from E. coli...\n",
      "Found 943,297 unique minimizers out of 4,641,622 possible k-mers\n",
      "Reduction ratio: 4.92x\n",
      "Minimizer extraction for E. coli completed in 15.36 seconds\n",
      "Processing M. tuberculosis...\n",
      "Extracting minimizers (k=31, w=10) from M. tuberculosis...\n",
      "Found 870,064 unique minimizers out of 4,411,502 possible k-mers\n",
      "Reduction ratio: 5.07x\n",
      "Minimizer extraction for M. tuberculosis completed in 14.87 seconds\n",
      "Processing P. aeruginosa...\n",
      "Extracting minimizers (k=31, w=10) from P. aeruginosa...\n",
      "Found 1,238,426 unique minimizers out of 6,264,374 possible k-mers\n",
      "Reduction ratio: 5.06x\n",
      "Minimizer extraction for P. aeruginosa completed in 21.27 seconds\n",
      "Processing S. aureus...\n",
      "Extracting minimizers (k=31, w=10) from S. aureus...\n",
      "Found 574,335 unique minimizers out of 2,821,331 possible k-mers\n",
      "Reduction ratio: 4.91x\n",
      "Minimizer extraction for S. aureus completed in 9.89 seconds\n",
      "Merging minimizer indices...\n",
      "Merged minimizer index contains 4,515,766 unique minimizers\n",
      "Index merging completed in 3.20 seconds\n",
      "Overall reduction ratio: 4.95x (from 22,354,405 k-mers to 4,515,766 minimizers)\n",
      "Minimizer index built in 79.12 seconds\n",
      "Memory after building minimizer index: 1900.0234375 MB\n",
      "\n",
      "================ MINIMIZER INDEX STATISTICS ================\n",
      "Parameters: k=31, w=10\n",
      "Total unique minimizers in index: 4,515,766\n",
      "Total k-mers in reference genomes: 22,354,405\n",
      "Theoretical k-mer space (4^k): 4,611,686,018,427,387,904\n",
      "Space coverage: 0.0000000001%\n",
      "Minimizer reduction ratio: 4.95x\n",
      "\n",
      "Minimizers unique to each genome:\n",
      "- E. coli: 942,770 unique minimizers\n",
      "- B. subtilis: 890,253 unique minimizers\n",
      "- P. aeruginosa: 1,238,109 unique minimizers\n",
      "- S. aureus: 573,699 unique minimizers\n",
      "- M. tuberculosis: 870,015 unique minimizers\n",
      "\n",
      "Minimizers shared between genomes: 920\n",
      "===========================================================\n",
      "\n",
      "Processing error_free_reads:\n",
      "  - simulated_reads_no_errors_10k_R1.fastq\n",
      "  - simulated_reads_no_errors_10k_R2.fastq\n",
      "Read 5000 reads from simulated_reads_no_errors_10k_R1.fastq\n",
      "Read 5000 reads from simulated_reads_no_errors_10k_R2.fastq\n",
      "Total combined reads: 10000\n",
      "\n",
      "====================== CLASSIFICATION SUMMARY ======================\n",
      "Total reads processed: 10000\n",
      "\n",
      "Single Genome Matches:\n",
      "- E. coli: 3045 reads (30.45%)\n",
      "- P. aeruginosa: 503 reads (5.03%)\n",
      "- B. subtilis: 505 reads (5.05%)\n",
      "- S. aureus: 503 reads (5.03%)\n",
      "- M. tuberculosis: 506 reads (5.06%)\n",
      "\n",
      "Multi-Genome Matches (by LCA):\n",
      "- Bacteria: 9 reads (0.09%)\n",
      "\n",
      "Unclassified: 4929 reads (49.29%)\n",
      "===================================================================\n",
      "Processing time: 10.16 seconds\n",
      "Reads per second: 984.17\n",
      "Memory after processing error_free_reads: 1910.98046875 MB\n",
      "\n",
      "Processing with_errors_reads:\n",
      "  - simulated_reads_miseq_10k_R1.fastq\n",
      "  - simulated_reads_miseq_10k_R2.fastq\n",
      "Read 5000 reads from simulated_reads_miseq_10k_R1.fastq\n",
      "Read 5000 reads from simulated_reads_miseq_10k_R2.fastq\n",
      "Total combined reads: 10000\n",
      "\n",
      "====================== CLASSIFICATION SUMMARY ======================\n",
      "Total reads processed: 10000\n",
      "\n",
      "Single Genome Matches:\n",
      "- E. coli: 291 reads (2.91%)\n",
      "- P. aeruginosa: 56 reads (0.56%)\n",
      "- B. subtilis: 59 reads (0.59%)\n",
      "- S. aureus: 49 reads (0.49%)\n",
      "- M. tuberculosis: 51 reads (0.51%)\n",
      "\n",
      "Multi-Genome Matches (by LCA):\n",
      "- Bacteria: 1 reads (0.01%)\n",
      "\n",
      "Unclassified: 9493 reads (94.93%)\n",
      "===================================================================\n",
      "Processing time: 14.43 seconds\n",
      "Reads per second: 693.19\n",
      "Memory after processing with_errors_reads: 1917.80859375 MB\n",
      "\n",
      "====================== PERFORMANCE SUMMARY ======================\n",
      "Total execution time: 103.78 seconds\n",
      "Minimizer index building time: 79.12 seconds\n",
      "Peak memory usage: 1917.81 MB\n",
      "Memory reduction compared to full k-mer index: 4.95x\n",
      "\n",
      "Total reads processed: 20000\n",
      "Overall processing speed: 810.97 reads per second\n",
      "================================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "from collections import defaultdict, Counter, deque\n",
    "import psutil\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Performance tracking\n",
    "start_time = time.time()\n",
    "memory_usage = []\n",
    "\n",
    "def track_memory():\n",
    "    \"\"\"Track current memory usage\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_usage.append(process.memory_info().rss / 1024 / 1024)  # MB\n",
    "    return memory_usage[-1]\n",
    "\n",
    "print(f\"Starting with memory: {track_memory()} MB\")\n",
    "\n",
    "# ==================== K-mer Encoding Functions ====================\n",
    "\n",
    "def encode_kmer(kmer):\n",
    "    \"\"\"\n",
    "    Encode a k-mer string into a 64-bit integer using 2-bit encoding\n",
    "    A=00, C=01, G=10, T=11\n",
    "    \"\"\"\n",
    "    encoding = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "    result = 0\n",
    "    for base in kmer:\n",
    "        result = (result << 2) | encoding.get(base, 0)\n",
    "    return result\n",
    "\n",
    "def decode_kmer(encoded, k):\n",
    "    \"\"\"\n",
    "    Decode a 2-bit encoded k-mer back to a string\n",
    "    \"\"\"\n",
    "    decoding = {0: 'A', 1: 'C', 2: 'G', 3: 'T'}\n",
    "    result = []\n",
    "    for i in range(k):\n",
    "        # Extract 2 bits from the end\n",
    "        bits = (encoded >> (2*i)) & 3\n",
    "        result.append(decoding[bits])\n",
    "    return ''.join(reversed(result))\n",
    "\n",
    "# ==================== FASTA/FASTQ Parsing Functions ====================\n",
    "\n",
    "def read_fasta_file(filename):\n",
    "    \"\"\"Read a FASTA file and return the sequence\"\"\"\n",
    "    sequence = \"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        # Skip header line\n",
    "        header = f.readline().strip()\n",
    "        # Read sequence lines\n",
    "        for line in f:\n",
    "            if line.startswith('>'):\n",
    "                continue  # Skip additional headers\n",
    "            sequence += line.strip()\n",
    "    return sequence\n",
    "\n",
    "def read_fastq_file(filename, max_reads=None):\n",
    "    \"\"\"Read a FASTQ file and return reads as a list of (header, sequence) tuples\"\"\"\n",
    "    reads = []\n",
    "    count = 0\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        while True:\n",
    "            if max_reads and count >= max_reads:\n",
    "                break\n",
    "                \n",
    "            header = f.readline().strip()\n",
    "            if not header:\n",
    "                break  # End of file\n",
    "                \n",
    "            sequence = f.readline().strip()\n",
    "            f.readline()  # Skip \"+\" line\n",
    "            quality = f.readline().strip()\n",
    "            \n",
    "            if header and sequence and quality:\n",
    "                reads.append((header, sequence))\n",
    "                count += 1\n",
    "            else:\n",
    "                break  # Incomplete entry\n",
    "    \n",
    "    return reads\n",
    "\n",
    "def read_genome_files(filenames):\n",
    "    \"\"\"Read all genome files and return a dictionary of sequences\"\"\"\n",
    "    print(\"Reading reference genomes...\")\n",
    "    genomes = {}\n",
    "    \n",
    "    for filename in filenames:\n",
    "        base_name = os.path.basename(filename)\n",
    "        # Extract species name from filename\n",
    "        species = base_name.split('.')[0]\n",
    "        \n",
    "        # Use prettier names for the species\n",
    "        if species == \"e_coli\":\n",
    "            species = \"E. coli\"\n",
    "        elif species == \"b_subtilis\":\n",
    "            species = \"B. subtilis\"\n",
    "        elif species == \"p_aeruginosa\":\n",
    "            species = \"P. aeruginosa\"\n",
    "        elif species == \"s_aureus\":\n",
    "            species = \"S. aureus\"\n",
    "        elif species == \"m_tuberculosis\":\n",
    "            species = \"M. tuberculosis\"\n",
    "        \n",
    "        print(f\"Reading genome for {species}...\")\n",
    "        sequence = read_fasta_file(filename)\n",
    "        genomes[species] = sequence\n",
    "        print(f\"Read {len(sequence)} bases for {species}\")\n",
    "    \n",
    "    return genomes\n",
    "\n",
    "def generate_taxonomic_tree():\n",
    "    \"\"\"Generate a taxonomic tree for the 5 reference genomes\"\"\"\n",
    "    taxonomy = {\n",
    "        'E. coli': ['Bacteria', 'Proteobacteria', 'Gammaproteobacteria', 'Enterobacterales', 'Enterobacteriaceae', 'Escherichia', 'E. coli'],\n",
    "        'B. subtilis': ['Bacteria', 'Firmicutes', 'Bacilli', 'Bacillales', 'Bacillaceae', 'Bacillus', 'B. subtilis'],\n",
    "        'P. aeruginosa': ['Bacteria', 'Proteobacteria', 'Gammaproteobacteria', 'Pseudomonadales', 'Pseudomonadaceae', 'Pseudomonas', 'P. aeruginosa'],\n",
    "        'S. aureus': ['Bacteria', 'Firmicutes', 'Bacilli', 'Bacillales', 'Staphylococcaceae', 'Staphylococcus', 'S. aureus'],\n",
    "        'M. tuberculosis': ['Bacteria', 'Actinobacteria', 'Actinomycetia', 'Mycobacteriales', 'Mycobacteriaceae', 'Mycobacterium', 'M. tuberculosis']\n",
    "    }\n",
    "    return taxonomy\n",
    "\n",
    "def find_lca(taxonomy, species_list):\n",
    "    \"\"\"Find the lowest common ancestor of a list of species\"\"\"\n",
    "    if not species_list:\n",
    "        return \"Unknown\"\n",
    "    if len(species_list) == 1:\n",
    "        return species_list[0]  # Return the species if only one match\n",
    "    \n",
    "    # Get taxonomic paths for each species\n",
    "    paths = [taxonomy[species] for species in species_list]\n",
    "    \n",
    "    # Find the common prefix of all paths\n",
    "    common_path = []\n",
    "    for i in range(min(len(path) for path in paths)):\n",
    "        if len(set(path[i] for path in paths)) == 1:\n",
    "            common_path.append(paths[0][i])\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return common_path[-1] if common_path else \"Bacteria\"\n",
    "\n",
    "# ==================== Minimizer Implementation ====================\n",
    "\n",
    "def extract_minimizers_from_chunk(args):\n",
    "    \"\"\"Extract minimizers from a chunk of a genome sequence using a sliding window approach\"\"\"\n",
    "    chunk, start_idx, k, w = args\n",
    "    \n",
    "    # Skip if chunk is too short\n",
    "    if len(chunk) < k:\n",
    "        return []\n",
    "    \n",
    "    # First, extract all k-mers with their positions\n",
    "    kmers = []\n",
    "    for i in range(len(chunk) - k + 1):\n",
    "        kmer = chunk[i:i+k]\n",
    "        if 'N' not in kmer:  # Skip k-mers with ambiguous bases\n",
    "            encoded_kmer = encode_kmer(kmer)\n",
    "            kmers.append((encoded_kmer, start_idx + i))\n",
    "    \n",
    "    # Now find minimizers in each window of w consecutive k-mers\n",
    "    minimizers = {}\n",
    "    \n",
    "    for i in range(len(kmers) - w + 1):\n",
    "        window = kmers[i:i+w]\n",
    "        # Find the lexicographically smallest k-mer in the window\n",
    "        min_kmer, min_pos = min(window, key=lambda x: x[0])\n",
    "        \n",
    "        if min_kmer not in minimizers:\n",
    "            minimizers[min_kmer] = []\n",
    "        if min_pos not in minimizers[min_kmer]:\n",
    "            minimizers[min_kmer].append(min_pos)\n",
    "    \n",
    "    return list(minimizers.items())\n",
    "\n",
    "def extract_minimizers_parallel(sequence, k=31, w=10, chunk_size=1000000, num_processes=8):\n",
    "    \"\"\"Extract minimizers from a sequence in parallel\"\"\"\n",
    "    chunks = []\n",
    "    for i in range(0, len(sequence), chunk_size):\n",
    "        # Add k+w-1 to ensure we don't miss any minimizers at chunk boundaries\n",
    "        chunk = sequence[i:i+chunk_size+k+w-1]\n",
    "        if len(chunk) >= k:\n",
    "            chunks.append((chunk, i, k, w))\n",
    "    \n",
    "    with mp.Pool(processes=num_processes) as pool:\n",
    "        results = pool.map(extract_minimizers_from_chunk, chunks)\n",
    "    \n",
    "    # Combine results from all chunks\n",
    "    all_minimizers = {}\n",
    "    for result in results:\n",
    "        for encoded_kmer, positions in result:\n",
    "            if encoded_kmer not in all_minimizers:\n",
    "                all_minimizers[encoded_kmer] = []\n",
    "            all_minimizers[encoded_kmer].extend(positions)\n",
    "    \n",
    "    return all_minimizers\n",
    "\n",
    "def build_minimizer_index_for_genome(genome_name, sequence, k=31, w=10, num_processes=8):\n",
    "    \"\"\"Build a minimizer-based index for a single genome\"\"\"\n",
    "    print(f\"Extracting minimizers (k={k}, w={w}) from {genome_name}...\")\n",
    "    start = time.time()\n",
    "    \n",
    "    # Extract minimizers in parallel\n",
    "    minimizers = extract_minimizers_parallel(sequence, k, w, num_processes=num_processes)\n",
    "    \n",
    "    # Count the total number of k-mers\n",
    "    total_kmers = len(sequence) - k + 1\n",
    "    total_minimizers = len(minimizers)\n",
    "    reduction_ratio = total_kmers / total_minimizers if total_minimizers > 0 else float('inf')\n",
    "    \n",
    "    print(f\"Found {total_minimizers:,} unique minimizers out of {total_kmers:,} possible k-mers\")\n",
    "    print(f\"Reduction ratio: {reduction_ratio:.2f}x\")\n",
    "    print(f\"Minimizer extraction for {genome_name} completed in {time.time() - start:.2f} seconds\")\n",
    "    \n",
    "    return minimizers, total_kmers, total_minimizers\n",
    "\n",
    "def merge_minimizer_indices(genome_indices):\n",
    "    \"\"\"Merge minimizer indices from multiple genomes using regular dict\"\"\"\n",
    "    combined_index = {}\n",
    "    \n",
    "    for genome_name, genome_index in genome_indices.items():\n",
    "        for encoded_kmer, positions in genome_index.items():\n",
    "            if encoded_kmer not in combined_index:\n",
    "                combined_index[encoded_kmer] = {}\n",
    "            combined_index[encoded_kmer][genome_name] = positions\n",
    "    \n",
    "    return combined_index\n",
    "\n",
    "def build_minimizer_index(genomes, k=31, w=10, num_processes=8):\n",
    "    \"\"\"Build a minimizer-based index for all genomes\"\"\"\n",
    "    genome_indices = {}\n",
    "    total_genome_kmers = 0\n",
    "    total_genome_minimizers = 0\n",
    "    \n",
    "    print(f\"Building minimizer index (k={k}, w={w}) for all genomes...\")\n",
    "    for name, sequence in genomes.items():\n",
    "        print(f\"Processing {name}...\")\n",
    "        genome_index, genome_kmers, genome_minimizers = build_minimizer_index_for_genome(\n",
    "            name, sequence, k, w, num_processes\n",
    "        )\n",
    "        genome_indices[name] = genome_index\n",
    "        total_genome_kmers += genome_kmers\n",
    "        total_genome_minimizers += genome_minimizers\n",
    "    \n",
    "    # Merge individual genome indices\n",
    "    print(\"Merging minimizer indices...\")\n",
    "    start = time.time()\n",
    "    combined_index = merge_minimizer_indices(genome_indices)\n",
    "    print(f\"Merged minimizer index contains {len(combined_index):,} unique minimizers\")\n",
    "    print(f\"Index merging completed in {time.time() - start:.2f} seconds\")\n",
    "    \n",
    "    # Calculate overall reduction ratio\n",
    "    overall_reduction = total_genome_kmers / len(combined_index) if len(combined_index) > 0 else float('inf')\n",
    "    print(f\"Overall reduction ratio: {overall_reduction:.2f}x (from {total_genome_kmers:,} k-mers to {len(combined_index):,} minimizers)\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    minimizer_stats = analyze_minimizer_index(combined_index, k, w, total_genome_kmers, total_genome_minimizers)\n",
    "    \n",
    "    return combined_index, minimizer_stats\n",
    "\n",
    "def analyze_minimizer_index(index, k, w, total_kmers, total_minimizers):\n",
    "    \"\"\"Analyze minimizer index statistics\"\"\"\n",
    "    # Count minimizers unique to each genome vs shared\n",
    "    unique_to_genome = {}\n",
    "    for genome in ['E. coli', 'B. subtilis', 'P. aeruginosa', 'S. aureus', 'M. tuberculosis']:\n",
    "        unique_to_genome[genome] = 0\n",
    "    \n",
    "    shared_minimizers = 0\n",
    "    \n",
    "    for encoded_kmer, genome_dict in index.items():\n",
    "        if len(genome_dict) == 1:\n",
    "            # Unique to one genome\n",
    "            genome = next(iter(genome_dict.keys()))\n",
    "            unique_to_genome[genome] += 1\n",
    "        else:\n",
    "            # Shared between multiple genomes\n",
    "            shared_minimizers += 1\n",
    "    \n",
    "    # Theoretical k-mer space\n",
    "    theoretical_kmers = 4**k\n",
    "    \n",
    "    # Build statistics dictionary\n",
    "    stats = {\n",
    "        \"k\": k,\n",
    "        \"w\": w,\n",
    "        \"total_kmers\": total_kmers,\n",
    "        \"total_minimizers\": total_minimizers,\n",
    "        \"total_unique_minimizers\": len(index),\n",
    "        \"theoretical_kmers\": theoretical_kmers,\n",
    "        \"coverage_percent\": (len(index) / theoretical_kmers) * 100,\n",
    "        \"reduction_ratio\": total_kmers / len(index) if len(index) > 0 else float('inf'),\n",
    "        \"shared_minimizers\": shared_minimizers,\n",
    "        \"unique_to_genome\": unique_to_genome\n",
    "    }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def print_minimizer_stats(stats):\n",
    "    \"\"\"Print minimizer index statistics\"\"\"\n",
    "    print(\"\\n================ MINIMIZER INDEX STATISTICS ================\")\n",
    "    print(f\"Parameters: k={stats['k']}, w={stats['w']}\")\n",
    "    print(f\"Total unique minimizers in index: {stats['total_unique_minimizers']:,}\")\n",
    "    print(f\"Total k-mers in reference genomes: {stats['total_kmers']:,}\")\n",
    "    print(f\"Theoretical k-mer space (4^k): {stats['theoretical_kmers']:,}\")\n",
    "    print(f\"Space coverage: {stats['coverage_percent']:.10f}%\")\n",
    "    print(f\"Minimizer reduction ratio: {stats['reduction_ratio']:.2f}x\")\n",
    "    \n",
    "    print(\"\\nMinimizers unique to each genome:\")\n",
    "    for genome, count in stats['unique_to_genome'].items():\n",
    "        print(f\"- {genome}: {count:,} unique minimizers\")\n",
    "    \n",
    "    print(f\"\\nMinimizers shared between genomes: {stats['shared_minimizers']:,}\")\n",
    "    print(\"===========================================================\")\n",
    "\n",
    "# ==================== Minimizer Based Classification ====================\n",
    "\n",
    "def extract_read_minimizers(sequence, k=31, w=10):\n",
    "    \"\"\"Extract minimizers from a read sequence using a sliding window approach\"\"\"\n",
    "    # Skip if read is too short\n",
    "    if len(sequence) < k:\n",
    "        return []\n",
    "    \n",
    "    # Extract all k-mers from the read\n",
    "    kmers = []\n",
    "    for i in range(len(sequence) - k + 1):\n",
    "        kmer = sequence[i:i+k]\n",
    "        if 'N' not in kmer:  # Skip k-mers with ambiguous bases\n",
    "            kmers.append((encode_kmer(kmer), i))\n",
    "    \n",
    "    # Find minimizers in each window\n",
    "    minimizers = set()\n",
    "    for i in range(len(kmers) - w + 1):\n",
    "        window = kmers[i:i+w]\n",
    "        min_kmer, _ = min(window, key=lambda x: x[0])\n",
    "        minimizers.add(min_kmer)\n",
    "    \n",
    "    return minimizers\n",
    "\n",
    "def process_read_chunk_with_minimizers(read_chunk, minimizer_index, taxonomy, k=31, w=10, min_minimizer_fraction=0.1):\n",
    "    \"\"\"Process a chunk of reads against the minimizer index\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for header, sequence in read_chunk:\n",
    "        # Skip very short reads\n",
    "        if len(sequence) < k:\n",
    "            results.append((header, \"Unclassified\", \"Unknown\"))\n",
    "            continue\n",
    "        \n",
    "        # Extract minimizers from the read\n",
    "        read_minimizers = extract_read_minimizers(sequence, k, w)\n",
    "        if not read_minimizers:\n",
    "            results.append((header, \"Unclassified\", \"Unknown\"))\n",
    "            continue\n",
    "        \n",
    "        # Count matches to each genome\n",
    "        genome_matches = {}\n",
    "        for genome in ['E. coli', 'B. subtilis', 'P. aeruginosa', 'S. aureus', 'M. tuberculosis']:\n",
    "            genome_matches[genome] = 0\n",
    "            \n",
    "        total_minimizers = len(read_minimizers)\n",
    "        matched_minimizers = 0\n",
    "        \n",
    "        for encoded_kmer in read_minimizers:\n",
    "            if encoded_kmer in minimizer_index:\n",
    "                matched_minimizers += 1\n",
    "                for genome in minimizer_index[encoded_kmer]:\n",
    "                    genome_matches[genome] += 1\n",
    "        \n",
    "        # Determine best matching genome(s)\n",
    "        max_matches = max(genome_matches.values())\n",
    "        if max_matches == 0:\n",
    "            results.append((header, \"Unclassified\", \"Unknown\"))\n",
    "            continue\n",
    "            \n",
    "        match_fraction = max_matches / total_minimizers if total_minimizers > 0 else 0\n",
    "        \n",
    "        # Classify based on matches and threshold\n",
    "        if match_fraction < min_minimizer_fraction:\n",
    "            results.append((header, \"Unclassified\", \"Unknown\"))\n",
    "        else:\n",
    "            best_genomes = [g for g, c in genome_matches.items() if c == max_matches]\n",
    "            if len(best_genomes) == 1:\n",
    "                results.append((header, best_genomes, best_genomes[0]))\n",
    "            else:\n",
    "                lca = find_lca(taxonomy, best_genomes)\n",
    "                results.append((header, best_genomes, lca))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def distribute_reads(reads, num_chunks):\n",
    "    \"\"\"Distribute reads evenly across chunks\"\"\"\n",
    "    chunk_size = max(1, len(reads) // num_chunks)\n",
    "    return [reads[i:i + chunk_size] for i in range(0, len(reads), chunk_size)]\n",
    "\n",
    "def classify_reads_parallel(reads, minimizer_index, taxonomy, k=31, w=10, num_processes=8, min_minimizer_fraction=0.1):\n",
    "    \"\"\"Classify reads in parallel using multiple processes\"\"\"\n",
    "    # Split reads into chunks for parallelization\n",
    "    read_chunks = distribute_reads(reads, num_processes)\n",
    "    \n",
    "    # Process each chunk in parallel\n",
    "    with mp.Pool(processes=num_processes) as pool:\n",
    "        process_func = partial(process_read_chunk_with_minimizers, \n",
    "                               minimizer_index=minimizer_index, \n",
    "                               taxonomy=taxonomy, \n",
    "                               k=k,\n",
    "                               w=w,\n",
    "                               min_minimizer_fraction=min_minimizer_fraction)\n",
    "        chunk_results = pool.map(process_func, read_chunks)\n",
    "    \n",
    "    # Combine results from all chunks\n",
    "    all_results = []\n",
    "    for result in chunk_results:\n",
    "        all_results.extend(result)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def summarize_results(classification_results):\n",
    "    \"\"\"Summarize classification results\"\"\"\n",
    "    # Count matches per genome\n",
    "    single_matches = Counter()\n",
    "    multi_matches = Counter()\n",
    "    lca_classifications = Counter()\n",
    "    unclassified = 0\n",
    "    \n",
    "    for _, classification, lca in classification_results:\n",
    "        if classification == \"Unclassified\":\n",
    "            unclassified += 1\n",
    "        elif len(classification) == 1:\n",
    "            single_matches[classification[0]] += 1\n",
    "        else:\n",
    "            # For multi-matches, count each genome\n",
    "            for genome in classification:\n",
    "                multi_matches[genome] += 1\n",
    "            # Also count by LCA\n",
    "            lca_classifications[lca] += 1\n",
    "    \n",
    "    # Prepare summary\n",
    "    summary = {\n",
    "        \"total_reads\": len(classification_results),\n",
    "        \"single_matches\": dict(single_matches),\n",
    "        \"multi_matches\": dict(multi_matches),\n",
    "        \"lca_classifications\": dict(lca_classifications),\n",
    "        \"unclassified\": unclassified\n",
    "    }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def print_summary(summary):\n",
    "    \"\"\"Print a formatted summary of classification results\"\"\"\n",
    "    print(\"\\n====================== CLASSIFICATION SUMMARY ======================\")\n",
    "    print(f\"Total reads processed: {summary['total_reads']}\")\n",
    "    \n",
    "    print(\"\\nSingle Genome Matches:\")\n",
    "    for genome, count in summary['single_matches'].items():\n",
    "        percentage = (count / summary['total_reads']) * 100\n",
    "        print(f\"- {genome}: {count} reads ({percentage:.2f}%)\")\n",
    "    \n",
    "    if summary['lca_classifications']:\n",
    "        print(\"\\nMulti-Genome Matches (by LCA):\")\n",
    "        for lca, count in summary['lca_classifications'].items():\n",
    "            percentage = (count / summary['total_reads']) * 100\n",
    "            print(f\"- {lca}: {count} reads ({percentage:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nUnclassified: {summary['unclassified']} reads ({(summary['unclassified'] / summary['total_reads']) * 100:.2f}%)\")\n",
    "    print(\"===================================================================\")\n",
    "\n",
    "# ==================== Main Function ====================\n",
    "\n",
    "def main(k=31, w=10, num_processes=8, max_reads_per_file=10000, min_minimizer_fraction=1.0):\n",
    "    \"\"\"Main function to run the entire classification pipeline\"\"\"\n",
    "    print(f\"Starting minimizer-based metagenomic classification with k={k}, w={w}, and {num_processes} processes\")\n",
    "    print(f\"Using minimum minimizer match fraction: {min_minimizer_fraction}\")\n",
    "    \n",
    "    # Define file paths\n",
    "    genome_files = [\n",
    "        \"b_subtilis.fna\",\n",
    "        \"e_coli.fna\",\n",
    "        \"m_tuberculosis.fna\",\n",
    "        \"p_aeruginosa.fna\",\n",
    "        \"s_aureus.fna\"\n",
    "    ]\n",
    "    \n",
    "    # Define combined read file sets\n",
    "    read_file_sets = {\n",
    "        \"error_free_reads\": [\n",
    "            \"simulated_reads_no_errors_10k_R1.fastq\",\n",
    "            \"simulated_reads_no_errors_10k_R2.fastq\"\n",
    "        ],\n",
    "        \"with_errors_reads\": [\n",
    "            \"simulated_reads_miseq_10k_R1.fastq\", \n",
    "            \"simulated_reads_miseq_10k_R2.fastq\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Load reference genomes\n",
    "    genomes = read_genome_files(genome_files)\n",
    "    taxonomy = generate_taxonomic_tree()\n",
    "    \n",
    "    print(f\"Memory after loading reference genomes: {track_memory()} MB\")\n",
    "    \n",
    "    # Build minimizer index\n",
    "    index_start = time.time()\n",
    "    minimizer_index, minimizer_stats = build_minimizer_index(\n",
    "        genomes, k=k, w=w, num_processes=num_processes\n",
    "    )\n",
    "    index_time = time.time() - index_start\n",
    "    \n",
    "    print(f\"Minimizer index built in {index_time:.2f} seconds\")\n",
    "    print(f\"Memory after building minimizer index: {track_memory()} MB\")\n",
    "    \n",
    "    # Print minimizer statistics\n",
    "    print_minimizer_stats(minimizer_stats)\n",
    "    \n",
    "    # Process each combined read file set\n",
    "    all_results = {}\n",
    "    \n",
    "    for set_name, file_paths in read_file_sets.items():\n",
    "        print(f\"\\nProcessing {set_name}:\")\n",
    "        for path in file_paths:\n",
    "            print(f\"  - {path}\")\n",
    "        \n",
    "        start_process = time.time()\n",
    "        \n",
    "        # Read and combine FASTQ files\n",
    "        combined_reads = []\n",
    "        for file_path in file_paths:\n",
    "            reads = read_fastq_file(file_path, max_reads=max_reads_per_file)\n",
    "            combined_reads.extend(reads)\n",
    "            print(f\"Read {len(reads)} reads from {file_path}\")\n",
    "            \n",
    "        print(f\"Total combined reads: {len(combined_reads)}\")\n",
    "        \n",
    "        # Process reads\n",
    "        results = classify_reads_parallel(\n",
    "            combined_reads, \n",
    "            minimizer_index, \n",
    "            taxonomy, \n",
    "            k=k,\n",
    "            w=w,\n",
    "            num_processes=num_processes,\n",
    "            min_minimizer_fraction=min_minimizer_fraction\n",
    "        )\n",
    "        \n",
    "        process_time = time.time() - start_process\n",
    "        all_results[set_name] = results\n",
    "        \n",
    "        # Summarize and print results\n",
    "        summary = summarize_results(results)\n",
    "        print_summary(summary)\n",
    "        \n",
    "        print(f\"Processing time: {process_time:.2f} seconds\")\n",
    "        print(f\"Reads per second: {len(results) / process_time:.2f}\")\n",
    "        \n",
    "        print(f\"Memory after processing {set_name}: {track_memory()} MB\")\n",
    "    \n",
    "    # Performance report\n",
    "    total_time = time.time() - start_time\n",
    "    peak_memory = max(memory_usage)\n",
    "    \n",
    "    print(\"\\n====================== PERFORMANCE SUMMARY ======================\")\n",
    "    print(f\"Total execution time: {total_time:.2f} seconds\")\n",
    "    print(f\"Minimizer index building time: {index_time:.2f} seconds\")\n",
    "    print(f\"Peak memory usage: {peak_memory:.2f} MB\")\n",
    "    print(f\"Memory reduction compared to full k-mer index: {minimizer_stats['reduction_ratio']:.2f}x\")\n",
    "    \n",
    "    # Calculate processing statistics\n",
    "    total_reads = sum(len(results) for results in all_results.values())\n",
    "    reads_per_second = total_reads / (total_time - index_time)\n",
    "    \n",
    "    print(f\"\\nTotal reads processed: {total_reads}\")\n",
    "    print(f\"Overall processing speed: {reads_per_second:.2f} reads per second\")\n",
    "    print(\"================================================================\")\n",
    "\n",
    "# Run the main function with the specified parameters\n",
    "if __name__ == \"__main__\":\n",
    "    # Set parameters\n",
    "    K = 31  # k-mer length (as specified in the assignment)\n",
    "    W = 10  # window size for minimizers\n",
    "    NUM_PROCESSES = 1 # Number of processes to use\n",
    "    MIN_MINIMIZER_FRACTION = 1.0  # For exact matching, require 100% of minimizers to match\n",
    "    \n",
    "    main(k=K, w=W, num_processes=NUM_PROCESSES, min_minimizer_fraction=MIN_MINIMIZER_FRACTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5fa30c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
