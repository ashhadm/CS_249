{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115f3a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "from collections import defaultdict, Counter, deque\n",
    "import psutil\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Performance tracking\n",
    "start_time = time.time()\n",
    "memory_usage = []\n",
    "\n",
    "def track_memory():\n",
    "    \"\"\"Track current memory usage\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_usage.append(process.memory_info().rss / 1024 / 1024)  # MB\n",
    "    return memory_usage[-1]\n",
    "\n",
    "print(f\"Starting with memory: {track_memory()} MB\")\n",
    "\n",
    "# ==================== K-mer Encoding Functions ====================\n",
    "\n",
    "def encode_kmer(kmer):\n",
    "    \"\"\"\n",
    "    Encode a k-mer string into a 64-bit integer using 2-bit encoding\n",
    "    A=00, C=01, G=10, T=11\n",
    "    \"\"\"\n",
    "    encoding = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "    result = 0\n",
    "    for base in kmer:\n",
    "        result = (result << 2) | encoding.get(base, 0)\n",
    "    return result\n",
    "\n",
    "def decode_kmer(encoded, k):\n",
    "    \"\"\"\n",
    "    Decode a 2-bit encoded k-mer back to a string\n",
    "    \"\"\"\n",
    "    decoding = {0: 'A', 1: 'C', 2: 'G', 3: 'T'}\n",
    "    result = []\n",
    "    for i in range(k):\n",
    "        # Extract 2 bits from the end\n",
    "        bits = (encoded >> (2*i)) & 3\n",
    "        result.append(decoding[bits])\n",
    "    return ''.join(reversed(result))\n",
    "\n",
    "# ==================== FASTA/FASTQ Parsing Functions ====================\n",
    "\n",
    "def read_fasta_file(filename):\n",
    "    \"\"\"Read a FASTA file and return the sequence\"\"\"\n",
    "    sequence = \"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        # Skip header line\n",
    "        header = f.readline().strip()\n",
    "        # Read sequence lines\n",
    "        for line in f:\n",
    "            if line.startswith('>'):\n",
    "                continue  # Skip additional headers\n",
    "            sequence += line.strip()\n",
    "    return sequence\n",
    "\n",
    "def read_fastq_file(filename, max_reads=None):\n",
    "    \"\"\"Read a FASTQ file and return reads as a list of (header, sequence) tuples\"\"\"\n",
    "    reads = []\n",
    "    count = 0\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        while True:\n",
    "            if max_reads and count >= max_reads:\n",
    "                break\n",
    "                \n",
    "            header = f.readline().strip()\n",
    "            if not header:\n",
    "                break  # End of file\n",
    "                \n",
    "            sequence = f.readline().strip()\n",
    "            f.readline()  # Skip \"+\" line\n",
    "            quality = f.readline().strip()\n",
    "            \n",
    "            if header and sequence and quality:\n",
    "                reads.append((header, sequence))\n",
    "                count += 1\n",
    "            else:\n",
    "                break  # Incomplete entry\n",
    "    \n",
    "    return reads\n",
    "\n",
    "def read_genome_files(filenames):\n",
    "    \"\"\"Read all genome files and return a dictionary of sequences\"\"\"\n",
    "    print(\"Reading reference genomes...\")\n",
    "    genomes = {}\n",
    "    \n",
    "    for filename in filenames:\n",
    "        base_name = os.path.basename(filename)\n",
    "        # Extract species name from filename\n",
    "        species = base_name.split('.')[0]\n",
    "        \n",
    "        # Use prettier names for the species\n",
    "        if species == \"e_coli\":\n",
    "            species = \"E. coli\"\n",
    "        elif species == \"b_subtilis\":\n",
    "            species = \"B. subtilis\"\n",
    "        elif species == \"p_aeruginosa\":\n",
    "            species = \"P. aeruginosa\"\n",
    "        elif species == \"s_aureus\":\n",
    "            species = \"S. aureus\"\n",
    "        elif species == \"m_tuberculosis\":\n",
    "            species = \"M. tuberculosis\"\n",
    "        \n",
    "        print(f\"Reading genome for {species}...\")\n",
    "        sequence = read_fasta_file(filename)\n",
    "        genomes[species] = sequence\n",
    "        print(f\"Read {len(sequence)} bases for {species}\")\n",
    "    \n",
    "    return genomes\n",
    "\n",
    "def generate_taxonomic_tree():\n",
    "    \"\"\"Generate a taxonomic tree for the 5 reference genomes\"\"\"\n",
    "    taxonomy = {\n",
    "        'E. coli': ['Bacteria', 'Proteobacteria', 'Gammaproteobacteria', 'Enterobacterales', 'Enterobacteriaceae', 'Escherichia', 'E. coli'],\n",
    "        'B. subtilis': ['Bacteria', 'Firmicutes', 'Bacilli', 'Bacillales', 'Bacillaceae', 'Bacillus', 'B. subtilis'],\n",
    "        'P. aeruginosa': ['Bacteria', 'Proteobacteria', 'Gammaproteobacteria', 'Pseudomonadales', 'Pseudomonadaceae', 'Pseudomonas', 'P. aeruginosa'],\n",
    "        'S. aureus': ['Bacteria', 'Firmicutes', 'Bacilli', 'Bacillales', 'Staphylococcaceae', 'Staphylococcus', 'S. aureus'],\n",
    "        'M. tuberculosis': ['Bacteria', 'Actinobacteria', 'Actinomycetia', 'Mycobacteriales', 'Mycobacteriaceae', 'Mycobacterium', 'M. tuberculosis']\n",
    "    }\n",
    "    return taxonomy\n",
    "\n",
    "def find_lca(taxonomy, species_list):\n",
    "    \"\"\"Find the lowest common ancestor of a list of species\"\"\"\n",
    "    if not species_list:\n",
    "        return \"Unknown\"\n",
    "    if len(species_list) == 1:\n",
    "        return species_list[0]  # Return the species if only one match\n",
    "    \n",
    "    # Get taxonomic paths for each species\n",
    "    paths = [taxonomy[species] for species in species_list]\n",
    "    \n",
    "    # Find the common prefix of all paths\n",
    "    common_path = []\n",
    "    for i in range(min(len(path) for path in paths)):\n",
    "        if len(set(path[i] for path in paths)) == 1:\n",
    "            common_path.append(paths[0][i])\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return common_path[-1] if common_path else \"Bacteria\"\n",
    "\n",
    "# ==================== Minimizer Implementation ====================\n",
    "\n",
    "def extract_minimizers_from_chunk(args):\n",
    "    \"\"\"Extract minimizers from a chunk of a genome sequence using a sliding window approach\"\"\"\n",
    "    chunk, start_idx, k, w = args\n",
    "    \n",
    "    # Skip if chunk is too short\n",
    "    if len(chunk) < k:\n",
    "        return []\n",
    "    \n",
    "    # First, extract all k-mers with their positions\n",
    "    kmers = []\n",
    "    for i in range(len(chunk) - k + 1):\n",
    "        kmer = chunk[i:i+k]\n",
    "        if 'N' not in kmer:  # Skip k-mers with ambiguous bases\n",
    "            encoded_kmer = encode_kmer(kmer)\n",
    "            kmers.append((encoded_kmer, start_idx + i))\n",
    "    \n",
    "    # Now find minimizers in each window of w consecutive k-mers\n",
    "    minimizers = {}\n",
    "    \n",
    "    for i in range(len(kmers) - w + 1):\n",
    "        window = kmers[i:i+w]\n",
    "        # Find the lexicographically smallest k-mer in the window\n",
    "        min_kmer, min_pos = min(window, key=lambda x: x[0])\n",
    "        \n",
    "        if min_kmer not in minimizers:\n",
    "            minimizers[min_kmer] = []\n",
    "        if min_pos not in minimizers[min_kmer]:\n",
    "            minimizers[min_kmer].append(min_pos)\n",
    "    \n",
    "    return list(minimizers.items())\n",
    "\n",
    "def extract_minimizers_parallel(sequence, k=31, w=10, chunk_size=1000000, num_processes=8):\n",
    "    \"\"\"Extract minimizers from a sequence in parallel\"\"\"\n",
    "    chunks = []\n",
    "    for i in range(0, len(sequence), chunk_size):\n",
    "        # Add k+w-1 to ensure we don't miss any minimizers at chunk boundaries\n",
    "        chunk = sequence[i:i+chunk_size+k+w-1]\n",
    "        if len(chunk) >= k:\n",
    "            chunks.append((chunk, i, k, w))\n",
    "    \n",
    "    with mp.Pool(processes=num_processes) as pool:\n",
    "        results = pool.map(extract_minimizers_from_chunk, chunks)\n",
    "    \n",
    "    # Combine results from all chunks\n",
    "    all_minimizers = {}\n",
    "    for result in results:\n",
    "        for encoded_kmer, positions in result:\n",
    "            if encoded_kmer not in all_minimizers:\n",
    "                all_minimizers[encoded_kmer] = []\n",
    "            all_minimizers[encoded_kmer].extend(positions)\n",
    "    \n",
    "    return all_minimizers\n",
    "\n",
    "def build_minimizer_index_for_genome(genome_name, sequence, k=31, w=10, num_processes=8):\n",
    "    \"\"\"Build a minimizer-based index for a single genome\"\"\"\n",
    "    print(f\"Extracting minimizers (k={k}, w={w}) from {genome_name}...\")\n",
    "    start = time.time()\n",
    "    \n",
    "    # Extract minimizers in parallel\n",
    "    minimizers = extract_minimizers_parallel(sequence, k, w, num_processes=num_processes)\n",
    "    \n",
    "    # Count the total number of k-mers\n",
    "    total_kmers = len(sequence) - k + 1\n",
    "    total_minimizers = len(minimizers)\n",
    "    reduction_ratio = total_kmers / total_minimizers if total_minimizers > 0 else float('inf')\n",
    "    \n",
    "    print(f\"Found {total_minimizers:,} unique minimizers out of {total_kmers:,} possible k-mers\")\n",
    "    print(f\"Reduction ratio: {reduction_ratio:.2f}x\")\n",
    "    print(f\"Minimizer extraction for {genome_name} completed in {time.time() - start:.2f} seconds\")\n",
    "    \n",
    "    return minimizers, total_kmers, total_minimizers\n",
    "\n",
    "def merge_minimizer_indices(genome_indices):\n",
    "    \"\"\"Merge minimizer indices from multiple genomes using regular dict\"\"\"\n",
    "    combined_index = {}\n",
    "    \n",
    "    for genome_name, genome_index in genome_indices.items():\n",
    "        for encoded_kmer, positions in genome_index.items():\n",
    "            if encoded_kmer not in combined_index:\n",
    "                combined_index[encoded_kmer] = {}\n",
    "            combined_index[encoded_kmer][genome_name] = positions\n",
    "    \n",
    "    return combined_index\n",
    "\n",
    "def build_minimizer_index(genomes, k=31, w=10, num_processes=8):\n",
    "    \"\"\"Build a minimizer-based index for all genomes\"\"\"\n",
    "    genome_indices = {}\n",
    "    total_genome_kmers = 0\n",
    "    total_genome_minimizers = 0\n",
    "    \n",
    "    print(f\"Building minimizer index (k={k}, w={w}) for all genomes...\")\n",
    "    for name, sequence in genomes.items():\n",
    "        print(f\"Processing {name}...\")\n",
    "        genome_index, genome_kmers, genome_minimizers = build_minimizer_index_for_genome(\n",
    "            name, sequence, k, w, num_processes\n",
    "        )\n",
    "        genome_indices[name] = genome_index\n",
    "        total_genome_kmers += genome_kmers\n",
    "        total_genome_minimizers += genome_minimizers\n",
    "    \n",
    "    # Merge individual genome indices\n",
    "    print(\"Merging minimizer indices...\")\n",
    "    start = time.time()\n",
    "    combined_index = merge_minimizer_indices(genome_indices)\n",
    "    print(f\"Merged minimizer index contains {len(combined_index):,} unique minimizers\")\n",
    "    print(f\"Index merging completed in {time.time() - start:.2f} seconds\")\n",
    "    \n",
    "    # Calculate overall reduction ratio\n",
    "    overall_reduction = total_genome_kmers / len(combined_index) if len(combined_index) > 0 else float('inf')\n",
    "    print(f\"Overall reduction ratio: {overall_reduction:.2f}x (from {total_genome_kmers:,} k-mers to {len(combined_index):,} minimizers)\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    minimizer_stats = analyze_minimizer_index(combined_index, k, w, total_genome_kmers, total_genome_minimizers)\n",
    "    \n",
    "    return combined_index, minimizer_stats\n",
    "\n",
    "def analyze_minimizer_index(index, k, w, total_kmers, total_minimizers):\n",
    "    \"\"\"Analyze minimizer index statistics\"\"\"\n",
    "    # Count minimizers unique to each genome vs shared\n",
    "    unique_to_genome = {}\n",
    "    for genome in ['E. coli', 'B. subtilis', 'P. aeruginosa', 'S. aureus', 'M. tuberculosis']:\n",
    "        unique_to_genome[genome] = 0\n",
    "    \n",
    "    shared_minimizers = 0\n",
    "    \n",
    "    for encoded_kmer, genome_dict in index.items():\n",
    "        if len(genome_dict) == 1:\n",
    "            # Unique to one genome\n",
    "            genome = next(iter(genome_dict.keys()))\n",
    "            unique_to_genome[genome] += 1\n",
    "        else:\n",
    "            # Shared between multiple genomes\n",
    "            shared_minimizers += 1\n",
    "    \n",
    "    # Theoretical k-mer space\n",
    "    theoretical_kmers = 4**k\n",
    "    \n",
    "    # Build statistics dictionary\n",
    "    stats = {\n",
    "        \"k\": k,\n",
    "        \"w\": w,\n",
    "        \"total_kmers\": total_kmers,\n",
    "        \"total_minimizers\": total_minimizers,\n",
    "        \"total_unique_minimizers\": len(index),\n",
    "        \"theoretical_kmers\": theoretical_kmers,\n",
    "        \"coverage_percent\": (len(index) / theoretical_kmers) * 100,\n",
    "        \"reduction_ratio\": total_kmers / len(index) if len(index) > 0 else float('inf'),\n",
    "        \"shared_minimizers\": shared_minimizers,\n",
    "        \"unique_to_genome\": unique_to_genome\n",
    "    }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def print_minimizer_stats(stats):\n",
    "    \"\"\"Print minimizer index statistics\"\"\"\n",
    "    print(\"\\n================ MINIMIZER INDEX STATISTICS ================\")\n",
    "    print(f\"Parameters: k={stats['k']}, w={stats['w']}\")\n",
    "    print(f\"Total unique minimizers in index: {stats['total_unique_minimizers']:,}\")\n",
    "    print(f\"Total k-mers in reference genomes: {stats['total_kmers']:,}\")\n",
    "    print(f\"Theoretical k-mer space (4^k): {stats['theoretical_kmers']:,}\")\n",
    "    print(f\"Space coverage: {stats['coverage_percent']:.10f}%\")\n",
    "    print(f\"Minimizer reduction ratio: {stats['reduction_ratio']:.2f}x\")\n",
    "    \n",
    "    print(\"\\nMinimizers unique to each genome:\")\n",
    "    for genome, count in stats['unique_to_genome'].items():\n",
    "        print(f\"- {genome}: {count:,} unique minimizers\")\n",
    "    \n",
    "    print(f\"\\nMinimizers shared between genomes: {stats['shared_minimizers']:,}\")\n",
    "    print(\"===========================================================\")\n",
    "\n",
    "# ==================== Minimizer Based Classification ====================\n",
    "\n",
    "def extract_read_minimizers(sequence, k=31, w=10):\n",
    "    \"\"\"Extract minimizers from a read sequence using a sliding window approach\"\"\"\n",
    "    # Skip if read is too short\n",
    "    if len(sequence) < k:\n",
    "        return []\n",
    "    \n",
    "    # Extract all k-mers from the read\n",
    "    kmers = []\n",
    "    for i in range(len(sequence) - k + 1):\n",
    "        kmer = sequence[i:i+k]\n",
    "        if 'N' not in kmer:  # Skip k-mers with ambiguous bases\n",
    "            kmers.append((encode_kmer(kmer), i))\n",
    "    \n",
    "    # Find minimizers in each window\n",
    "    minimizers = set()\n",
    "    for i in range(len(kmers) - w + 1):\n",
    "        window = kmers[i:i+w]\n",
    "        min_kmer, _ = min(window, key=lambda x: x[0])\n",
    "        minimizers.add(min_kmer)\n",
    "    \n",
    "    return minimizers\n",
    "\n",
    "def process_read_chunk_with_minimizers(read_chunk, minimizer_index, taxonomy, k=31, w=10, min_minimizer_fraction=0.1):\n",
    "    \"\"\"Process a chunk of reads against the minimizer index\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for header, sequence in read_chunk:\n",
    "        # Skip very short reads\n",
    "        if len(sequence) < k:\n",
    "            results.append((header, \"Unclassified\", \"Unknown\"))\n",
    "            continue\n",
    "        \n",
    "        # Extract minimizers from the read\n",
    "        read_minimizers = extract_read_minimizers(sequence, k, w)\n",
    "        if not read_minimizers:\n",
    "            results.append((header, \"Unclassified\", \"Unknown\"))\n",
    "            continue\n",
    "        \n",
    "        # Count matches to each genome\n",
    "        genome_matches = {}\n",
    "        for genome in ['E. coli', 'B. subtilis', 'P. aeruginosa', 'S. aureus', 'M. tuberculosis']:\n",
    "            genome_matches[genome] = 0\n",
    "            \n",
    "        total_minimizers = len(read_minimizers)\n",
    "        matched_minimizers = 0\n",
    "        \n",
    "        for encoded_kmer in read_minimizers:\n",
    "            if encoded_kmer in minimizer_index:\n",
    "                matched_minimizers += 1\n",
    "                for genome in minimizer_index[encoded_kmer]:\n",
    "                    genome_matches[genome] += 1\n",
    "        \n",
    "        # Determine best matching genome(s)\n",
    "        max_matches = max(genome_matches.values())\n",
    "        if max_matches == 0:\n",
    "            results.append((header, \"Unclassified\", \"Unknown\"))\n",
    "            continue\n",
    "            \n",
    "        match_fraction = max_matches / total_minimizers if total_minimizers > 0 else 0\n",
    "        \n",
    "        # Classify based on matches and threshold\n",
    "        if match_fraction < min_minimizer_fraction:\n",
    "            results.append((header, \"Unclassified\", \"Unknown\"))\n",
    "        else:\n",
    "            best_genomes = [g for g, c in genome_matches.items() if c == max_matches]\n",
    "            if len(best_genomes) == 1:\n",
    "                results.append((header, best_genomes, best_genomes[0]))\n",
    "            else:\n",
    "                lca = find_lca(taxonomy, best_genomes)\n",
    "                results.append((header, best_genomes, lca))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def distribute_reads(reads, num_chunks):\n",
    "    \"\"\"Distribute reads evenly across chunks\"\"\"\n",
    "    chunk_size = max(1, len(reads) // num_chunks)\n",
    "    return [reads[i:i + chunk_size] for i in range(0, len(reads), chunk_size)]\n",
    "\n",
    "def classify_reads_parallel(reads, minimizer_index, taxonomy, k=31, w=10, num_processes=8, min_minimizer_fraction=0.1):\n",
    "    \"\"\"Classify reads in parallel using multiple processes\"\"\"\n",
    "    # Split reads into chunks for parallelization\n",
    "    read_chunks = distribute_reads(reads, num_processes)\n",
    "    \n",
    "    # Process each chunk in parallel\n",
    "    with mp.Pool(processes=num_processes) as pool:\n",
    "        process_func = partial(process_read_chunk_with_minimizers, \n",
    "                               minimizer_index=minimizer_index, \n",
    "                               taxonomy=taxonomy, \n",
    "                               k=k,\n",
    "                               w=w,\n",
    "                               min_minimizer_fraction=min_minimizer_fraction)\n",
    "        chunk_results = pool.map(process_func, read_chunks)\n",
    "    \n",
    "    # Combine results from all chunks\n",
    "    all_results = []\n",
    "    for result in chunk_results:\n",
    "        all_results.extend(result)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def summarize_results(classification_results):\n",
    "    \"\"\"Summarize classification results\"\"\"\n",
    "    # Count matches per genome\n",
    "    single_matches = Counter()\n",
    "    multi_matches = Counter()\n",
    "    lca_classifications = Counter()\n",
    "    unclassified = 0\n",
    "    \n",
    "    for _, classification, lca in classification_results:\n",
    "        if classification == \"Unclassified\":\n",
    "            unclassified += 1\n",
    "        elif len(classification) == 1:\n",
    "            single_matches[classification[0]] += 1\n",
    "        else:\n",
    "            # For multi-matches, count each genome\n",
    "            for genome in classification:\n",
    "                multi_matches[genome] += 1\n",
    "            # Also count by LCA\n",
    "            lca_classifications[lca] += 1\n",
    "    \n",
    "    # Prepare summary\n",
    "    summary = {\n",
    "        \"total_reads\": len(classification_results),\n",
    "        \"single_matches\": dict(single_matches),\n",
    "        \"multi_matches\": dict(multi_matches),\n",
    "        \"lca_classifications\": dict(lca_classifications),\n",
    "        \"unclassified\": unclassified\n",
    "    }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def print_summary(summary):\n",
    "    \"\"\"Print a formatted summary of classification results\"\"\"\n",
    "    print(\"\\n====================== CLASSIFICATION SUMMARY ======================\")\n",
    "    print(f\"Total reads processed: {summary['total_reads']}\")\n",
    "    \n",
    "    print(\"\\nSingle Genome Matches:\")\n",
    "    for genome, count in summary['single_matches'].items():\n",
    "        percentage = (count / summary['total_reads']) * 100\n",
    "        print(f\"- {genome}: {count} reads ({percentage:.2f}%)\")\n",
    "    \n",
    "    if summary['lca_classifications']:\n",
    "        print(\"\\nMulti-Genome Matches (by LCA):\")\n",
    "        for lca, count in summary['lca_classifications'].items():\n",
    "            percentage = (count / summary['total_reads']) * 100\n",
    "            print(f\"- {lca}: {count} reads ({percentage:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nUnclassified: {summary['unclassified']} reads ({(summary['unclassified'] / summary['total_reads']) * 100:.2f}%)\")\n",
    "    print(\"===================================================================\")\n",
    "\n",
    "# ==================== Main Function ====================\n",
    "\n",
    "def main(k=31, w=10, num_processes=8, max_reads_per_file=10000, min_minimizer_fraction=1.0):\n",
    "    \"\"\"Main function to run the entire classification pipeline\"\"\"\n",
    "    print(f\"Starting minimizer-based metagenomic classification with k={k}, w={w}, and {num_processes} processes\")\n",
    "    print(f\"Using minimum minimizer match fraction: {min_minimizer_fraction}\")\n",
    "    \n",
    "    # Define file paths\n",
    "    genome_files = [\n",
    "        \"b_subtilis.fna\",\n",
    "        \"e_coli.fna\",\n",
    "        \"m_tuberculosis.fna\",\n",
    "        \"p_aeruginosa.fna\",\n",
    "        \"s_aureus.fna\"\n",
    "    ]\n",
    "    \n",
    "    # Define combined read file sets\n",
    "    read_file_sets = {\n",
    "        \"error_free_reads\": [\n",
    "            \"simulated_reads_no_errors_10k_R1.fastq\",\n",
    "            \"simulated_reads_no_errors_10k_R2.fastq\"\n",
    "        ],\n",
    "        \"with_errors_reads\": [\n",
    "            \"simulated_reads_miseq_10k_R1.fastq\", \n",
    "            \"simulated_reads_miseq_10k_R2.fastq\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Load reference genomes\n",
    "    genomes = read_genome_files(genome_files)\n",
    "    taxonomy = generate_taxonomic_tree()\n",
    "    \n",
    "    print(f\"Memory after loading reference genomes: {track_memory()} MB\")\n",
    "    \n",
    "    # Build minimizer index\n",
    "    index_start = time.time()\n",
    "    minimizer_index, minimizer_stats = build_minimizer_index(\n",
    "        genomes, k=k, w=w, num_processes=num_processes\n",
    "    )\n",
    "    index_time = time.time() - index_start\n",
    "    \n",
    "    print(f\"Minimizer index built in {index_time:.2f} seconds\")\n",
    "    print(f\"Memory after building minimizer index: {track_memory()} MB\")\n",
    "    \n",
    "    # Print minimizer statistics\n",
    "    print_minimizer_stats(minimizer_stats)\n",
    "    \n",
    "    # Process each combined read file set\n",
    "    all_results = {}\n",
    "    \n",
    "    for set_name, file_paths in read_file_sets.items():\n",
    "        print(f\"\\nProcessing {set_name}:\")\n",
    "        for path in file_paths:\n",
    "            print(f\"  - {path}\")\n",
    "        \n",
    "        start_process = time.time()\n",
    "        \n",
    "        # Read and combine FASTQ files\n",
    "        combined_reads = []\n",
    "        for file_path in file_paths:\n",
    "            reads = read_fastq_file(file_path, max_reads=max_reads_per_file)\n",
    "            combined_reads.extend(reads)\n",
    "            print(f\"Read {len(reads)} reads from {file_path}\")\n",
    "            \n",
    "        print(f\"Total combined reads: {len(combined_reads)}\")\n",
    "        \n",
    "        # Process reads\n",
    "        results = classify_reads_parallel(\n",
    "            combined_reads, \n",
    "            minimizer_index, \n",
    "            taxonomy, \n",
    "            k=k,\n",
    "            w=w,\n",
    "            num_processes=num_processes,\n",
    "            min_minimizer_fraction=min_minimizer_fraction\n",
    "        )\n",
    "        \n",
    "        process_time = time.time() - start_process\n",
    "        all_results[set_name] = results\n",
    "        \n",
    "        # Summarize and print results\n",
    "        summary = summarize_results(results)\n",
    "        print_summary(summary)\n",
    "        \n",
    "        print(f\"Processing time: {process_time:.2f} seconds\")\n",
    "        print(f\"Reads per second: {len(results) / process_time:.2f}\")\n",
    "        \n",
    "        print(f\"Memory after processing {set_name}: {track_memory()} MB\")\n",
    "    \n",
    "    # Performance report\n",
    "    total_time = time.time() - start_time\n",
    "    peak_memory = max(memory_usage)\n",
    "    \n",
    "    print(\"\\n====================== PERFORMANCE SUMMARY ======================\")\n",
    "    print(f\"Total execution time: {total_time:.2f} seconds\")\n",
    "    print(f\"Minimizer index building time: {index_time:.2f} seconds\")\n",
    "    print(f\"Peak memory usage: {peak_memory:.2f} MB\")\n",
    "    print(f\"Memory reduction compared to full k-mer index: {minimizer_stats['reduction_ratio']:.2f}x\")\n",
    "    \n",
    "    # Calculate processing statistics\n",
    "    total_reads = sum(len(results) for results in all_results.values())\n",
    "    reads_per_second = total_reads / (total_time - index_time)\n",
    "    \n",
    "    print(f\"\\nTotal reads processed: {total_reads}\")\n",
    "    print(f\"Overall processing speed: {reads_per_second:.2f} reads per second\")\n",
    "    print(\"================================================================\")\n",
    "\n",
    "# Run the main function with the specified parameters\n",
    "if __name__ == \"__main__\":\n",
    "    # Set parameters\n",
    "    K = 31  # k-mer length (as specified in the assignment)\n",
    "    W = 10  # window size for minimizers\n",
    "    NUM_PROCESSES = 1 # Number of processes to use\n",
    "    MIN_MINIMIZER_FRACTION = 1.0  # For exact matching, require 100% of minimizers to match\n",
    "    \n",
    "    main(k=K, w=W, num_processes=NUM_PROCESSES, min_minimizer_fraction=MIN_MINIMIZER_FRACTION)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
