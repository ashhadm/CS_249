{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c89bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import psutil\n",
    "from functools import partial\n",
    "\n",
    "# Performance tracking\n",
    "start_time = time.time()\n",
    "memory_usage = []\n",
    "\n",
    "def track_memory():\n",
    "    \"\"\"Track current memory usage\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_usage.append(process.memory_info().rss / 1024 / 1024)  # MB\n",
    "    return memory_usage[-1]\n",
    "\n",
    "print(f\"Starting with memory: {track_memory()} MB\")\n",
    "\n",
    "# ==================== FASTA/FASTQ Parsing Functions ====================\n",
    "\n",
    "def read_fasta_file(filename):\n",
    "    \"\"\"Read a FASTA file and return the sequence\"\"\"\n",
    "    sequence = \"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        # Skip header line\n",
    "        header = f.readline().strip()\n",
    "        # Read sequence lines\n",
    "        for line in f:\n",
    "            if line.startswith('>'):\n",
    "                continue  # Skip additional headers\n",
    "            sequence += line.strip()\n",
    "    return sequence\n",
    "\n",
    "def read_fastq_file(filename, max_reads=None):\n",
    "    \"\"\"Read a FASTQ file and return reads as a list of (header, sequence) tuples\"\"\"\n",
    "    reads = []\n",
    "    count = 0\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        while True:\n",
    "            if max_reads and count >= max_reads:\n",
    "                break\n",
    "                \n",
    "            header = f.readline().strip()\n",
    "            if not header:\n",
    "                break  # End of file\n",
    "                \n",
    "            sequence = f.readline().strip()\n",
    "            f.readline()  # Skip \"+\" line\n",
    "            quality = f.readline().strip()\n",
    "            \n",
    "            if header and sequence and quality:\n",
    "                reads.append((header, sequence))\n",
    "                count += 1\n",
    "            else:\n",
    "                break  # Incomplete entry\n",
    "    \n",
    "    return reads\n",
    "\n",
    "def read_genome_files(filenames):\n",
    "    \"\"\"Read all genome files and return a dictionary of sequences\"\"\"\n",
    "    print(\"Reading reference genomes...\")\n",
    "    genomes = {}\n",
    "    \n",
    "    for filename in filenames:\n",
    "        base_name = os.path.basename(filename)\n",
    "        # Extract species name from filename\n",
    "        species = base_name.split('.')[0]\n",
    "        \n",
    "        # Use prettier names for the species\n",
    "        if species == \"e_coli\":\n",
    "            species = \"E. coli\"\n",
    "        elif species == \"b_subtilis\":\n",
    "            species = \"B. subtilis\"\n",
    "        elif species == \"p_aeruginosa\":\n",
    "            species = \"P. aeruginosa\"\n",
    "        elif species == \"s_aureus\":\n",
    "            species = \"S. aureus\"\n",
    "        elif species == \"m_tuberculosis\":\n",
    "            species = \"M. tuberculosis\"\n",
    "        \n",
    "        print(f\"Reading genome for {species}...\")\n",
    "        sequence = read_fasta_file(filename)\n",
    "        genomes[species] = sequence\n",
    "        print(f\"Read {len(sequence)} bases for {species}\")\n",
    "    \n",
    "    return genomes\n",
    "\n",
    "def generate_taxonomic_tree():\n",
    "    \"\"\"Generate a taxonomic tree for the 5 reference genomes\"\"\"\n",
    "    taxonomy = {\n",
    "        'E. coli': ['Bacteria', 'Proteobacteria', 'Gammaproteobacteria', 'Enterobacterales', 'Enterobacteriaceae', 'Escherichia', 'E. coli'],\n",
    "        'B. subtilis': ['Bacteria', 'Firmicutes', 'Bacilli', 'Bacillales', 'Bacillaceae', 'Bacillus', 'B. subtilis'],\n",
    "        'P. aeruginosa': ['Bacteria', 'Proteobacteria', 'Gammaproteobacteria', 'Pseudomonadales', 'Pseudomonadaceae', 'Pseudomonas', 'P. aeruginosa'],\n",
    "        'S. aureus': ['Bacteria', 'Firmicutes', 'Bacilli', 'Bacillales', 'Staphylococcaceae', 'Staphylococcus', 'S. aureus'],\n",
    "        'M. tuberculosis': ['Bacteria', 'Actinobacteria', 'Actinomycetia', 'Mycobacteriales', 'Mycobacteriaceae', 'Mycobacterium', 'M. tuberculosis']\n",
    "    }\n",
    "    return taxonomy\n",
    "\n",
    "def find_lca(taxonomy, species_list):\n",
    "    \"\"\"Find the lowest common ancestor of a list of species\"\"\"\n",
    "    if not species_list:\n",
    "        return \"Unknown\"\n",
    "    if len(species_list) == 1:\n",
    "        return species_list[0]  # Return the species if only one match\n",
    "    \n",
    "    # Get taxonomic paths for each species\n",
    "    paths = [taxonomy[species] for species in species_list]\n",
    "    \n",
    "    # Find the common prefix of all paths\n",
    "    common_path = []\n",
    "    for i in range(min(len(path) for path in paths)):\n",
    "        if len(set(path[i] for path in paths)) == 1:\n",
    "            common_path.append(paths[0][i])\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return common_path[-1] if common_path else \"Bacteria\"\n",
    "\n",
    "# ==================== Suffix Array Implementation (Optimized) ====================\n",
    "\n",
    "def build_suffix_array(text):\n",
    "    \"\"\"Build a suffix array for the given text (optimized for DNA sequences)\"\"\"\n",
    "    n = len(text)\n",
    "    \n",
    "    # For very short texts, use direct approach\n",
    "    if n < 10000:\n",
    "        suffixes = [(text[i:], i) for i in range(n)]\n",
    "        suffixes.sort()\n",
    "        suffix_array = [pos for _, pos in suffixes]\n",
    "        return suffix_array\n",
    "    \n",
    "    # For longer texts, use memory-efficient approach\n",
    "    # Store only indices and compare characters directly during sort\n",
    "    indices = list(range(n))\n",
    "    \n",
    "    # Use Python's built-in sort with our custom comparison function\n",
    "    indices.sort(key=lambda i: (text[i:i+100], i))  # Use first 100 chars as initial sort\n",
    "    \n",
    "    return indices\n",
    "\n",
    "def exact_search(text, pattern, suffix_array):\n",
    "    \"\"\"Fast exact pattern matching using suffix array with direct character comparison\"\"\"\n",
    "    n, m = len(text), len(pattern)\n",
    "    \n",
    "    if m == 0:\n",
    "        return False\n",
    "    \n",
    "    # Binary search for pattern\n",
    "    left, right = 0, n - 1\n",
    "    \n",
    "    while left <= right:\n",
    "        mid = (left + right) // 2\n",
    "        suffix_pos = suffix_array[mid]\n",
    "        \n",
    "        # Compare pattern with current suffix without creating substrings\n",
    "        match = True\n",
    "        comparison = 0\n",
    "        \n",
    "        for i in range(min(m, n - suffix_pos)):\n",
    "            if text[suffix_pos + i] < pattern[i]:\n",
    "                match = False\n",
    "                comparison = -1\n",
    "                break\n",
    "            elif text[suffix_pos + i] > pattern[i]:\n",
    "                match = False\n",
    "                comparison = 1\n",
    "                break\n",
    "        \n",
    "        # If we've compared all characters and they matched\n",
    "        if match:\n",
    "            # If we've reached the end of the pattern, it's a match\n",
    "            if m <= n - suffix_pos:\n",
    "                return True\n",
    "            # If pattern is longer than remaining suffix\n",
    "            comparison = -1\n",
    "        \n",
    "        if comparison < 0:\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            right = mid - 1\n",
    "    \n",
    "    return False\n",
    "\n",
    "def build_genome_indices(genomes):\n",
    "    \"\"\"Build suffix arrays for all genomes\"\"\"\n",
    "    genome_indices = {}\n",
    "    \n",
    "    print(\"Building suffix arrays for all genomes...\")\n",
    "    for name, genome in genomes.items():\n",
    "        print(f\"Building index for {name}...\")\n",
    "        start = time.time()\n",
    "        \n",
    "        # Build suffix array\n",
    "        suffix_array = build_suffix_array(genome)\n",
    "        \n",
    "        genome_indices[name] = {\n",
    "            \"sequence\": genome,\n",
    "            \"suffix_array\": suffix_array\n",
    "        }\n",
    "        \n",
    "        print(f\"Index for {name} built in {time.time() - start:.2f} seconds\")\n",
    "    \n",
    "    return genome_indices\n",
    "\n",
    "# ==================== Read Processing with Direct Parallelization ====================\n",
    "\n",
    "def process_read_chunk(read_chunk, genome_indices, taxonomy):\n",
    "    \"\"\"Process a chunk of reads against all genome indices\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for header, sequence in read_chunk:\n",
    "        # Skip reads with ambiguous bases (N) for exact matching\n",
    "        if 'N' in sequence:\n",
    "            results.append((header, \"Unclassified\", \"Unknown\"))\n",
    "            continue\n",
    "            \n",
    "        # Find exact matches in each genome\n",
    "        matching_genomes = []\n",
    "        \n",
    "        for genome_name, index_data in genome_indices.items():\n",
    "            if exact_search(index_data[\"sequence\"], sequence, index_data[\"suffix_array\"]):\n",
    "                matching_genomes.append(genome_name)\n",
    "        \n",
    "        # Classify based on matches\n",
    "        if not matching_genomes:\n",
    "            classification = \"Unclassified\"\n",
    "            lca = \"Unknown\"\n",
    "        else:\n",
    "            classification = matching_genomes\n",
    "            lca = find_lca(taxonomy, matching_genomes)\n",
    "        \n",
    "        results.append((header, classification, lca))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def distribute_reads(reads, num_chunks):\n",
    "    \"\"\"Distribute reads evenly across chunks\"\"\"\n",
    "    chunk_size = max(1, len(reads) // num_chunks)\n",
    "    return [reads[i:i + chunk_size] for i in range(0, len(reads), chunk_size)]\n",
    "\n",
    "def classify_reads_parallel(reads, genome_indices, taxonomy, num_processes=4):\n",
    "    \"\"\"Classify reads in parallel using multiple processes\"\"\"\n",
    "    # Split reads into chunks for parallelization\n",
    "    read_chunks = distribute_reads(reads, num_processes)\n",
    "    \n",
    "    # Process each chunk in parallel\n",
    "    with mp.Pool(processes=num_processes) as pool:\n",
    "        process_func = partial(process_read_chunk, genome_indices=genome_indices, taxonomy=taxonomy)\n",
    "        chunk_results = pool.map(process_func, read_chunks)\n",
    "    \n",
    "    # Combine results from all chunks\n",
    "    all_results = []\n",
    "    for result in chunk_results:\n",
    "        all_results.extend(result)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def summarize_results(classification_results):\n",
    "    \"\"\"Summarize classification results\"\"\"\n",
    "    # Count matches per genome\n",
    "    single_matches = Counter()\n",
    "    multi_matches = Counter()\n",
    "    lca_classifications = Counter()\n",
    "    unclassified = 0\n",
    "    \n",
    "    for _, classification, lca in classification_results:\n",
    "        if classification == \"Unclassified\":\n",
    "            unclassified += 1\n",
    "        elif len(classification) == 1:\n",
    "            single_matches[classification[0]] += 1\n",
    "        else:\n",
    "            # For multi-matches, count each genome\n",
    "            for genome in classification:\n",
    "                multi_matches[genome] += 1\n",
    "            # Also count by LCA\n",
    "            lca_classifications[lca] += 1\n",
    "    \n",
    "    # Prepare summary\n",
    "    summary = {\n",
    "        \"total_reads\": len(classification_results),\n",
    "        \"single_matches\": dict(single_matches),\n",
    "        \"multi_matches\": dict(multi_matches),\n",
    "        \"lca_classifications\": dict(lca_classifications),\n",
    "        \"unclassified\": unclassified\n",
    "    }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def print_summary(summary):\n",
    "    \"\"\"Print a formatted summary of classification results\"\"\"\n",
    "    print(\"\\n====================== CLASSIFICATION SUMMARY ======================\")\n",
    "    print(f\"Total reads processed: {summary['total_reads']}\")\n",
    "    \n",
    "    print(\"\\nSingle Genome Matches:\")\n",
    "    for genome, count in summary['single_matches'].items():\n",
    "        percentage = (count / summary['total_reads']) * 100\n",
    "        print(f\"- {genome}: {count} reads ({percentage:.2f}%)\")\n",
    "    \n",
    "    if summary['lca_classifications']:\n",
    "        print(\"\\nMulti-Genome Matches (by LCA):\")\n",
    "        total_multi = sum(summary['lca_classifications'].values())\n",
    "        for lca, count in summary['lca_classifications'].items():\n",
    "            percentage = (count / summary['total_reads']) * 100\n",
    "            print(f\"- {lca}: {count} reads ({percentage:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nUnclassified: {summary['unclassified']} reads ({(summary['unclassified'] / summary['total_reads']) * 100:.2f}%)\")\n",
    "    print(\"===================================================================\")\n",
    "\n",
    "# ==================== Main Function ====================\n",
    "\n",
    "def main(num_processes=8, max_reads_per_file=10000):\n",
    "    \"\"\"Main function to run the entire classification pipeline\"\"\"\n",
    "    print(f\"Starting metagenomic classification with {num_processes} processes\")\n",
    "    \n",
    "    # Define file paths\n",
    "    genome_files = [\n",
    "        \"b_subtilis.fna\",\n",
    "        \"e_coli.fna\",\n",
    "        \"m_tuberculosis.fna\",\n",
    "        \"p_aeruginosa.fna\",\n",
    "        \"s_aureus.fna\"\n",
    "    ]\n",
    "    \n",
    "    # Define combined read file sets\n",
    "    read_file_sets = {\n",
    "        \"error_free_reads\": [\n",
    "            \"simulated_reads_no_errors_10k_R1.fastq\",\n",
    "            \"simulated_reads_no_errors_10k_R2.fastq\"\n",
    "        ],\n",
    "        \"with_errors_reads\": [\n",
    "            \"simulated_reads_miseq_10k_R1.fastq\", \n",
    "            \"simulated_reads_miseq_10k_R2.fastq\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Load reference genomes and build indices\n",
    "    genomes = read_genome_files(genome_files)\n",
    "    taxonomy = generate_taxonomic_tree()\n",
    "    \n",
    "    print(f\"Memory after loading reference genomes: {track_memory()} MB\")\n",
    "    \n",
    "    # Build genome indices\n",
    "    genome_indices = build_genome_indices(genomes)\n",
    "    \n",
    "    print(f\"Memory after building indices: {track_memory()} MB\")\n",
    "    \n",
    "    # Process each combined read file set\n",
    "    all_results = {}\n",
    "    \n",
    "    for set_name, file_paths in read_file_sets.items():\n",
    "        print(f\"\\nProcessing {set_name}:\")\n",
    "        for path in file_paths:\n",
    "            print(f\"  - {path}\")\n",
    "        \n",
    "        start_process = time.time()\n",
    "        \n",
    "        # Read and combine FASTQ files\n",
    "        combined_reads = []\n",
    "        for file_path in file_paths:\n",
    "            reads = read_fastq_file(file_path, max_reads=max_reads_per_file)\n",
    "            combined_reads.extend(reads)\n",
    "            print(f\"Read {len(reads)} reads from {file_path}\")\n",
    "            \n",
    "        print(f\"Total combined reads: {len(combined_reads)}\")\n",
    "        \n",
    "        # Process reads\n",
    "        results = classify_reads_parallel(\n",
    "            combined_reads, \n",
    "            genome_indices, \n",
    "            taxonomy, \n",
    "            num_processes=num_processes\n",
    "        )\n",
    "        \n",
    "        process_time = time.time() - start_process\n",
    "        all_results[set_name] = results\n",
    "        \n",
    "        # Summarize and print results\n",
    "        summary = summarize_results(results)\n",
    "        print_summary(summary)\n",
    "        \n",
    "        print(f\"Processing time: {process_time:.2f} seconds\")\n",
    "        print(f\"Reads per second: {len(results) / process_time:.2f}\")\n",
    "        \n",
    "        print(f\"Memory after processing {set_name}: {track_memory()} MB\")\n",
    "    \n",
    "    # Performance report\n",
    "    total_time = time.time() - start_time\n",
    "    peak_memory = max(memory_usage)\n",
    "    \n",
    "    print(\"\\n====================== PERFORMANCE SUMMARY ======================\")\n",
    "    print(f\"Total execution time: {total_time:.2f} seconds\")\n",
    "    print(f\"Peak memory usage: {peak_memory:.2f} MB\")\n",
    "    \n",
    "    # Calculate processing statistics\n",
    "    total_reads = sum(len(results) for results in all_results.values())\n",
    "    reads_per_second = total_reads / total_time\n",
    "    \n",
    "    print(f\"\\nTotal reads processed: {total_reads}\")\n",
    "    print(f\"Overall processing speed: {reads_per_second:.2f} reads per second\")\n",
    "    print(\"================================================================\")\n",
    "\n",
    "# Run the main function with the specified number of processes\n",
    "if __name__ == \"__main__\":\n",
    "    # Set the number of CPU cores to use (adjust as needed)\n",
    "    NUM_PROCESSES = 1  # Using 8 processes for better performance\n",
    "    main(num_processes=NUM_PROCESSES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
